{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "jewish-concept",
   "metadata": {},
   "source": [
    "/*************************************************************************************************************************\n",
    "                                                     ##Disclaimer##\n",
    "    \n",
    "-> We are providing this code block strictly for learning and researching, this is not a production ready code. \n",
    "-> We have no liability on this particular code under any circumstances; users should use this code on their own risk. \n",
    "\n",
    "-> All software, hardware and othr products that are referenced in these materials belong to the respective vendor who developed or who owns this product.\n",
    "\n",
    "/*************************************************************************************************************************\n",
    "                                                  @DeepSphere.AI ,Inc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-march",
   "metadata": {},
   "source": [
    "# Step 0: Downloading Deepspeech Pretrained Model to Convert Voice to Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abroad-theology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  deepspeech_file.zip\n",
      "   creating: deepspeech file/\n",
      "  inflating: deepspeech file/deepspeech-0.8.2-models.pbmm  \n",
      "  inflating: deepspeech file/deepspeech-0.8.2-models.scorer  \n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Initialise a client\n",
    "storage_client = storage.Client(\"medicaltranscript \")\n",
    "# Create a bucket object for our bucket\n",
    "bucket = storage_client.get_bucket('medicaltranscript')\n",
    "# Create a blob object from the filepath\n",
    "blob = bucket.blob(\"deepspeech_file.zip\")\n",
    "# Download the file to a destination\n",
    "blob.download_to_filename('deepspeech_file.zip')\n",
    "\n",
    "!unzip deepspeech_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-gibson",
   "metadata": {},
   "source": [
    "# Step 1: Set up .YAML Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lesbian-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "# Importing the yaml library\n",
    "\n",
    "import yaml\n",
    "# Importing the OS library\n",
    "\n",
    "import os\n",
    "\n",
    "vAR_yaml_file_path = '/home/jupyter/DeepsphereProjects/Healthcare/VoiceToTextClassification/4_ModelImplementation_SourceCode/config.yaml'\n",
    "    \n",
    "with open (vAR_yaml_file_path) as vAR_file:\n",
    "    \n",
    "    \n",
    "           config = yaml.safe_load(vAR_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "quality-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "vAR_Text_Path=config.get(\"text_file\")\n",
    "vAR_Symptom_Path=config.get(\"symptoms_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-serial",
   "metadata": {},
   "source": [
    "# Step 2: Importing the Required Run Time Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "crude-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import cufflinks as cf\n",
    "import plotly.offline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deepspeech import Model \n",
    "import numpy as np\n",
    "import wave\n",
    "import git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-company",
   "metadata": {},
   "source": [
    "# Step 3: Importing DeepSpeech Model for Converting Audio to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ready-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():  \n",
    "    Voice_to_text()\n",
    "    Clone_CliNER()\n",
    "    Predict_symptoms()\n",
    "    Text_Classification_and_Visualization()\n",
    "    \n",
    "def Voice_to_text():\n",
    "    vAR_beam_width=1000\n",
    "    vAR_lm_alpha=0.93 \n",
    "    vAR_lm_beta=1.18\n",
    "\n",
    "    model=Model(\"deepspeech file/deepspeech-0.8.2-models.pbmm\")\n",
    "    model.enableExternalScorer(\"deepspeech file/deepspeech-0.8.2-models.scorer\")\n",
    "\n",
    "    model.setScorerAlphaBeta(vAR_lm_alpha,vAR_lm_beta)\n",
    "    model.setBeamWidth(vAR_beam_width)\n",
    "    \n",
    "# Conversion of audio data to 16khz\n",
    "    !ffmpeg -i Audio_Conversation.wav -vn -ar 16000 -ac 1 hospital1.wav\n",
    "\n",
    "    def read_wav_file(filename):\n",
    "        with wave.open(filename, 'rb') as vAR_temp:       # \"rb\" mode opens the file in binary format for reading\n",
    "            rate=vAR_temp.getframerate()\n",
    "            frames=vAR_temp.getnframes()\n",
    "            buffer=vAR_temp.readframes(frames)\n",
    "        return buffer, rate\n",
    "\n",
    "    def transcribe(file):\n",
    "        buffer,rate=read_wav_file(file)\n",
    "        data16=np.frombuffer(buffer,dtype=np.int16)\n",
    "        return model.stt(data16)\n",
    "    \n",
    "# Storing the output in a .txt file    \n",
    "    f= open(vAR_Text_Path,\"w+\")\n",
    "\n",
    "    f.write(transcribe('hospital1.wav'))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-iceland",
   "metadata": {},
   "source": [
    "# Step 4: Training and Extracting Symptoms from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "matched-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clone_CliNER():\n",
    "    git.Git(\"/home/jupyter/DeepsphereProjects/Healthcare/VoiceToTextClassification/4_ModelImplementation_SourceCode\").clone(\"https://github.com/text-machine-lab/CliNER.git\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "final-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_symptoms():\n",
    "    %cd CliNER/\n",
    "    !python cliner predict --txt /home/jupyter/DeepsphereProjects/Healthcare/VoiceToTextClassification/4_ModelImplementation_SourceCode/CliNER/data/examples/hospital111.txt --out data/predictions --model models/silver.crf --format i2b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-vacation",
   "metadata": {},
   "source": [
    "# Step 5: Text classification and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-racing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.3.1 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/opt/conda --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1609680890771/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1609680890771/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : stereo\n",
      "\u001b[0mInput #0, wav, from 'Audio_Conversation.wav':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.45.100\n",
      "  Duration: 00:03:12.42, bitrate: 1411 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "File 'hospital1.wav' already exists. Overwrite? [y/N] ^C\n"
     ]
    }
   ],
   "source": [
    "def Text_Classification_and_Visualization():\n",
    "    with open(vAR_Text_Path,'r') as file:\n",
    "        read_file = file.read()\n",
    "# Text analytics (summarized content)\n",
    "    tokenized_text=sent_tokenize(read_file)\n",
    "    tokenized_word=word_tokenize(read_file)\n",
    "\n",
    "    fdist = FreqDist(tokenized_word)\n",
    "# removing all the stop words\n",
    "    vAR_stop_words=set(stopwords.words(\"english\"))\n",
    "    filtered_sent=[]\n",
    "    for vAR_iter in tokenized_word:\n",
    "        if vAR_iter not in vAR_stop_words:\n",
    "            filtered_sent.append(vAR_iter)\n",
    "    with open(vAR_Symptom_Path, \"r\") as vAR:\n",
    "        lines = vAR.readlines()\n",
    "    Outputlist = []\n",
    "# matched symptoms from filtered tokenized words from text file\n",
    "    for vAR_val in filtered_sent:\n",
    "        with open(vAR_Symptom_Path,\"r\") as vAR:\n",
    "            lines = vAR.readlines()\n",
    "        for line in lines:\n",
    "            vAR_out = line.strip()\n",
    "            if vAR_val == vAR_out:\n",
    "                Outputlist.append(vAR_val)\n",
    "# Plotting a Line Chart                \n",
    "    plt.plot(Outputlist)\n",
    "    plt.ylabel('Symptoms')\n",
    "    plt.xlabel('numbers')\n",
    "    plt.show()\n",
    "    symptoms = list(set(Outputlist))\n",
    "    print(symptoms)\n",
    "    vAR_df = DataFrame (Outputlist,columns=['Symptoms'])\n",
    "    vAR_table=vAR_df['Symptoms'].value_counts()\n",
    "    \n",
    "# visualization using Pie chart\n",
    "    vAR_table.plot.pie(y=vAR_df.index,shadow=False, explode=None, startangle=90, autopct='%1.1f%%')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    cf.go_offline()\n",
    "    cf.set_config_file(offline=False, world_readable=True)\n",
    "# visualization using cufflinks and plotly libraries\n",
    "    vAR_df.iplot(kind='hist',bins=50,xTitle='symptoms',linecolor='black',yTitle='frequency',title='Symptoms frequency distribution')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-flight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-digest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.mnightly-2021-02-02-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:mnightly-2021-02-02-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
