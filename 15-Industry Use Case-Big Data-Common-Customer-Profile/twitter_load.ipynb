{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /**********************************************************************************************************\n",
    "\n",
    "# File Name\t\t: \ttwitter_load.ipynb\n",
    "# Purpose\t\t\t:   Parsing tweets and loading into bigquery table\n",
    "# Author\t\t\t:   DeepSphere.AI, Inc.\n",
    "# Date and Time \t: \t03/16/2021 10:30 hrs\n",
    "# Version\t\t\t: \t1.0 \n",
    "\n",
    "# /************************************************************************************************************\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    vAR_client = bigquery.Client()\n",
    "    vAR_sql = \"SELECT tweet FROM `vast-verve-292018.ds_ai.ds_ai_stream_tweets` limit 1000\"\n",
    "    vAR_df = vAR_client.query(vAR_sql).to_dataframe()\n",
    "    vAR_lang_np = []\n",
    "    vAR_text_np = []\n",
    "    vAR_cust_np = []\n",
    "    vAR_loc_np = []\n",
    "    vAR_profile_image = []\n",
    "    vAR_age_np = []\n",
    "    vAR_gender_np = []\n",
    "    vAR_summarize_np = []\n",
    "    vAR_data_frame = pd.DataFrame()\n",
    "    for lCount_idx in vAR_df.index:\n",
    "        vAR_x = vAR_df['tweet'][lCount_idx]\n",
    "        vAR_data = json.loads(vAR_x)\n",
    "        vAR_cust_np.append(vAR_data['user']['screen_name'])\n",
    "        vAR_lang_np.append(vAR_data['lang'])\n",
    "        vAR_text_np.append(vAR_data['text'])\n",
    "        vAR_loc_np.append(vAR_data['user']['location'])\n",
    "        vAR_profile_image.append(vAR_data['user']['profile_image_url_https'])\n",
    "        vAR_summarize_np.append(summarize_text(vAR_data['text']))\n",
    "        vAR_download_image_path = \"/home/dsailabusr1/PRAKASH/DE-Framework/DE-Framework-Impl/Framework/test.jpg\"\n",
    "        download_image(vAR_data['user']['profile_image_url_https'],vAR_download_image_path)\n",
    "        vAR_age_,vAR_gender_ = age_and_gender(vAR_download_image_path)\n",
    "        if len(vAR_age_)>0:\n",
    "            vAR_age_ = vAR_age_[1:-1]\n",
    "            vAR_age_mean = vAR_age_.split(\"-\")\n",
    "            vAR_val = (int(vAR_age_mean[0])+int(vAR_age_mean[1]))/2\n",
    "            vAR_age_np.append(str(int(vAR_val)))\n",
    "        else:\n",
    "            vAR_age_np.append(vAR_age_)\n",
    "        vAR_gender_np.append(vAR_gender_)\n",
    "    vAR_data_frame['customer_id'] = vAR_cust_np\n",
    "    vAR_data_frame['language'] = vAR_lang_np\n",
    "    vAR_data_frame['text_content'] = vAR_text_np\n",
    "    vAR_data_frame['location'] = vAR_loc_np\n",
    "    vAR_data_frame['profile_image_url'] = vAR_profile_image\n",
    "    vAR_data_frame['age'] = vAR_age_np\n",
    "    vAR_data_frame['gender'] = vAR_gender_np\n",
    "    vAR_data_frame['summary_or_insights'] = vAR_summarize_np\n",
    "    \n",
    "    vAR_data_frame = vAR_data_frame[vAR_data_frame['location'].notnull()]\n",
    "    return vAR_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import heapq\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chmod -R 777 '/home/dsailabusr1/PRAKASH/DE-Framework/DE-Framework-Impl/Framework/test.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download profile image\n",
    "def download_image(url,path):\n",
    "    import requests\n",
    "    vAR_r = requests.get(url)\n",
    "    with open(path, \"wb\") as vAR_f:\n",
    "        vAR_f.write(vAR_r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is for age and gender detecion\n",
    "# # Downloading pretrained data and unzipping it\n",
    "# !gdown https://drive.google.com/uc?id=1_aDScOvBeBLCn_iv0oxSO8X1ySQpSbIS\n",
    "# # https://drive.google.com/uc?id=1_aDScOvBeBLCn_iv0oxSO8X1ySQpSbIS\n",
    "# !unzip /home/dsailabusr1/PRAKASH/DE-Framework/DE-Framework-Impl/Framework/modelNweight.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class age_detect:\n",
    "    # Import required modules\n",
    "    import cv2 as cv\n",
    "    import math\n",
    "    import time\n",
    "    # from google.colab.patches import cv2_imshow\n",
    "    vAR_faceProto = \"modelNweight/opencv_face_detector.pbtxt\"\n",
    "    vAR_faceModel = \"modelNweight/opencv_face_detector_uint8.pb\"\n",
    "\n",
    "    vAR_ageProto = \"modelNweight/age_deploy.prototxt\"\n",
    "    vAR_ageModel = \"modelNweight/age_net.caffemodel\"\n",
    "\n",
    "    vAR_genderProto = \"modelNweight/gender_deploy.prototxt\"\n",
    "    vAR_genderModel = \"modelNweight/gender_net.caffemodel\"\n",
    "\n",
    "    vAR_MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "    vAR_ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "    vAR_vAR_genderList = ['Male', 'Female']\n",
    "\n",
    "    # Load network\n",
    "    vAR_ageNet = cv.dnn.readNet(vAR_ageModel, vAR_ageProto)\n",
    "    vAR_genderNet = cv.dnn.readNet(vAR_genderModel, vAR_genderProto)\n",
    "    vAR_faceNet = cv.dnn.readNet(vAR_faceModel, vAR_faceProto)\n",
    "\n",
    "    vAR_padding = 20\n",
    "\n",
    "    def getFaceBox(self,net, frame, conf_threshold=0.7):\n",
    "        import cv2 as cv\n",
    "        vAR_frameOpencvDnn = frame.copy()\n",
    "        vAR_frameHeight = vAR_frameOpencvDnn.shape[0]\n",
    "        vAR_frameWidth = vAR_frameOpencvDnn.shape[1]\n",
    "        vAR_blob = cv.dnn.blobFromImage(vAR_frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "        net.setInput(vAR_blob)\n",
    "        vAR_detections = net.forward()\n",
    "        vAR_bboxes = []\n",
    "        for i in range(vAR_detections.shape[2]):\n",
    "            vAR_confidence = vAR_detections[0, 0, i, 2]\n",
    "            if vAR_confidence > conf_threshold:\n",
    "                vAR_x1 = int(vAR_detections[0, 0, i, 3] * vAR_frameWidth)\n",
    "                vAR_y1 = int(vAR_detections[0, 0, i, 4] * vAR_frameHeight)\n",
    "                vAR_x2 = int(vAR_detections[0, 0, i, 5] * vAR_frameWidth)\n",
    "                vAR_y2 = int(vAR_detections[0, 0, i, 6] * vAR_frameHeight)\n",
    "                vAR_bboxes.append([vAR_x1, vAR_y1, vAR_x2, vAR_y2])\n",
    "                cv.rectangle(vAR_frameOpencvDnn, (vAR_x1, vAR_y1), (vAR_x2, vAR_y2), (0, 255, 0), int(round(vAR_frameHeight/150)), 8)\n",
    "        return vAR_frameOpencvDnn, vAR_bboxes\n",
    "\n",
    "    def age_gender_detector(self,frame):\n",
    "        # Read frame\n",
    "        import time\n",
    "        import cv2 as cv\n",
    "        t = time.time()\n",
    "        vAR_frameFace, vAR_bboxes = self.getFaceBox(self.vAR_faceNet, frame)\n",
    "        vAR_age=''\n",
    "        vAR_gender = ''\n",
    "        for bbox in vAR_bboxes:\n",
    "            # print(bbox)\n",
    "            face = frame[max(0,bbox[1]-self.vAR_padding):min(bbox[3]+self.vAR_padding,frame.shape[0]-1),max(0,bbox[0]-self.vAR_padding):min(bbox[2]+self.vAR_padding, frame.shape[1]-1)]\n",
    "            vAR_blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), self.vAR_MODEL_MEAN_VALUES, swapRB=False)\n",
    "            self.vAR_genderNet.setInput(vAR_blob)\n",
    "            vAR_genderPreds = self.vAR_genderNet.forward()\n",
    "            vAR_gender = self.vAR_vAR_genderList[vAR_genderPreds[0].argmax()]\n",
    "            self.vAR_ageNet.setInput(vAR_blob)\n",
    "            vAR_agePreds = self.vAR_ageNet.forward()\n",
    "            vAR_age = self.vAR_ageList[vAR_agePreds[0].argmax()]\n",
    "\n",
    "    #         label = \"{},{}\".format(vAR_gender, vAR_age)\n",
    "    #         print(label)\n",
    "    #         cv.putText(vAR_frameFace, label, (bbox[0], bbox[1]-10), cv.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv.LINE_AA)\n",
    "        return vAR_age,vAR_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_and_gender(path):\n",
    "    import cv2 as cv\n",
    "    vAR_age_ = \"\"\n",
    "    vAR_gender_ = \"\"\n",
    "    try:\n",
    "        input = cv.imread(path)\n",
    "        vAR_resize = cv.resize(input, (227,227))\n",
    "        rName_Obj = age_detect()\n",
    "        vAR_age_,vAR_gender_ = rName_Obj.age_gender_detector(vAR_resize)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return vAR_age_,vAR_gender_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_load():\n",
    "    from google.cloud import bigquery\n",
    "    import datetime\n",
    "    import time\n",
    "    vAR_client = bigquery.Client()\n",
    "    vAR_df = read_data()\n",
    "    vAR_topic = 'covid'\n",
    "    vAR_df['platform_source'] = 'twitter'\n",
    "    vAR_df['topic_or_hashtag'] = vAR_topic\n",
    "    vAR_df['age'] = vAR_df.age.astype(str)\n",
    "    ts = time.time()\n",
    "    st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d')\n",
    "    vAR_df['created_time'] = st\n",
    "    insert_flag = insert_bigquery(bigquery,vAR_client,vAR_df)\n",
    "    return insert_flag\n",
    "#     update_flag = update_twitter_summary(client,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_bigquery(bigquery,client,df):\n",
    "\n",
    "    import datetime\n",
    "    import pandas\n",
    "    import pytz\n",
    "    # TODO(developer): Set table_id to the ID of the table to create.\n",
    "    vAR_table_id = \"vast-verve-292018.ds_ai.ds_ai_twitter_data\"\n",
    "    vAR_job_config = bigquery.LoadJobConfig(\n",
    "        # Specify a (partial) schema. All columns are always written to the\n",
    "        # table. The schema is used to assist in data type definitions.\n",
    "        # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "        # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "        # disposition it replaces the table with the loaded data.\n",
    "        write_disposition=\"WRITE_APPEND\",\n",
    "    )\n",
    "\n",
    "    vAR_job = client.load_table_from_dataframe(\n",
    "        df, vAR_table_id, job_config=vAR_job_config\n",
    "    )  # Make an API request.\n",
    "    vAR_job.result()  # Wait for the job to complete.\n",
    "\n",
    "    vAR_table = client.get_table(vAR_table_id)  # Make an API request.\n",
    "    print(\n",
    "        \"Loaded {} rows and {} columns to {}\".format(\n",
    "            vAR_table.num_rows, len(vAR_table.schema), vAR_table_id\n",
    "        )\n",
    "    )\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/text-summarization-with-nltk-in-python/\n",
    "def summarize_text(article_text):\n",
    "    import nltk\n",
    "    import heapq\n",
    "    import re\n",
    "    # Removing Square Brackets and Extra Spaces\n",
    "    article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "    article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "    # Removing special characters and digits\n",
    "    vAR_formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "    vAR_formatted_article_text = re.sub(r'\\s+', ' ', vAR_formatted_article_text)\n",
    "    # Converting Text To Sentences\n",
    "    vAR_sentence_list = nltk.sent_tokenize(article_text)\n",
    "    vAR_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    # Find Weighted Frequency of Occurrence\n",
    "    vAR_word_frequencies = {}\n",
    "    for lCount_word in nltk.word_tokenize(vAR_formatted_article_text):\n",
    "        if lCount_word not in vAR_stopwords:\n",
    "            if lCount_word not in vAR_word_frequencies.keys():\n",
    "                vAR_word_frequencies[lCount_word] = 1\n",
    "            else:\n",
    "                vAR_word_frequencies[lCount_word] += 1\n",
    "    vAR_maximum_frequncy = max(vAR_word_frequencies.values())\n",
    "\n",
    "    for lCount_word in vAR_word_frequencies.keys():\n",
    "        vAR_word_frequencies[lCount_word] = (vAR_word_frequencies[lCount_word]/vAR_maximum_frequncy)\n",
    "    # Calculating Sentence Scores\n",
    "    vAR_sentence_scores = {}\n",
    "    for sent in vAR_sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in vAR_word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in vAR_sentence_scores.keys():\n",
    "                        vAR_sentence_scores[sent] = vAR_word_frequencies[word]\n",
    "                    else:\n",
    "                        vAR_sentence_scores[sent] += vAR_word_frequencies[word]\n",
    "    vAR_summary_sentences = heapq.nlargest(4, vAR_sentence_scores, key=vAR_sentence_scores.get)\n",
    "    vAR_summary = ' '.join(vAR_summary_sentences)\n",
    "    return vAR_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /**********************************************************************************************************\n",
    "# Disclaimer.\n",
    "\n",
    "# We are providing this code block strictly for learning and researching, this is not a production ready code.\n",
    "# We have no liability on this particular code under any circumstances; users should\n",
    "# use this code on their own risk. All software,hardware and other products that are\n",
    "# referenced in these materials belong to the respective vendor who developed or who owns \n",
    "# this product.\n",
    "# /**********************************************************************************************************\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
