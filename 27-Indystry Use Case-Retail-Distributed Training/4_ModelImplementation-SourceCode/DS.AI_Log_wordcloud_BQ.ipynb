{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"autoscroll":"auto"},"outputs":[{"data":{"text/plain":["Intitializing Scala interpreter ..."]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Spark Web UI available at http://cluster2-m:8088/proxy/application_1621161459144_0001\n","SparkContext available as 'sc' (version = 3.1.1, master = yarn, app id = application_1621161459144_0001)\n","SparkSession available as 'spark'\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["import org.apache.spark._\n","import org.apache.spark.sql.SparkSession\n","import org.apache.spark.sql._\n","import org.apache.spark.sql.Row\n","import org.apache.spark.sql.types._\n"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["/*\n","     Step #1 :  Import Libraries\n"," */\n","import org.apache.spark._\n","import org.apache.spark.sql.SparkSession\n","import org.apache.spark.sql._\n","import org.apache.spark.sql.Row\n","import org.apache.spark.sql.types._"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["vAR_spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1e3d0318\n"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["val vAR_spark = SparkSession.builder\n","  .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.19.1\")\n","  .getOrCreate()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["vAR_bucket: String = dataproc-staging-us-central1-363181475522-it3j8dl5\n"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["val vAR_bucket = \"dataproc-staging-us-central1-363181475522-it3j8dl5\"\n","vAR_spark.conf.set(\"temporaryGcsBucket\", vAR_bucket)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["data: Seq[String] = List(divine-course-311407.Errors.hadoop_hdfs_namenode_20210509, divine-course-311407.Errors.hadoop_hdfs_datanode_20210509, divine-course-311407.Errors.hadoop_hdfs_secondarynamenode_20210509, divine-course-311407.Errors.hadoop_mapred_historyserver_20210509, divine-course-311407.Errors.hadoop_yarn_nodemanager_20210509, divine-course-311407.Errors.hadoop_yarn_resourcemanager_20210509, divine-course-311407.Errors.hadoop_yarn_timelineserver_20210509, divine-course-311407.Errors.yarn_userlogs_20210509)\n"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["val data = Seq((\"divine-course-311407.Errors.hadoop_hdfs_namenode_20210509\"),\n","                (\"divine-course-311407.Errors.hadoop_hdfs_datanode_20210509\"),\n","              (\"divine-course-311407.Errors.hadoop_hdfs_secondarynamenode_20210509\"),\n","              (\"divine-course-311407.Errors.hadoop_mapred_historyserver_20210509\"),\n","              (\"divine-course-311407.Errors.hadoop_yarn_nodemanager_20210509\"),\n","              (\"divine-course-311407.Errors.hadoop_yarn_resourcemanager_20210509\"),\n","              (\"divine-course-311407.Errors.hadoop_yarn_timelineserver_20210509\"),\n","              (\"divine-course-311407.Errors.yarn_userlogs_20210509\"))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["for( a <- 0 to 7 ){ \n","    val LogErrs = vAR_spark.read.format(\"bigquery\")\n","    .load(data(a))\n","    .select(\"jsonPayload.message\",\"timestamp\")\n","    .cache()\n","    \n","    if (a==0){LogErrs.write \n","  .format(\"bigquery\") \n","  .option(\"table\", \"divine-course-311407:Errors.ErrorMsg\") \n","  .option(\"temporaryGcsBucket\",vAR_bucket)\n","  .mode(\"overwrite\") \n","  .save()}\n","    \n","    else{LogErrs.write \n","  .format(\"bigquery\") \n","  .option(\"table\", \"divine-course-311407:Errors.ErrorMsg\") \n","  .option(\"temporaryGcsBucket\",vAR_bucket)\n","  .mode(\"append\") \n","  .save()}\n","    \n","      }  "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["Messages: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [message: string, timestamp: timestamp]\n"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["val Messages = vAR_spark.read.format(\"bigquery\")\n","    .load(\"divine-course-311407:Errors.ErrorMsg\")\n","    .cache()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------------+-----------------------+\n","|message                    |timestamp              |\n","+---------------------------+-----------------------+\n","|RECEIVED SIGNAL 1: SIGHUP  |2021-05-09 10:24:14.943|\n","|RECEIVED SIGNAL 1: SIGHUP  |2021-05-09 05:32:33.531|\n","|RECEIVED SIGNAL 1: SIGHUP  |2021-05-09 13:19:12.254|\n","|RECEIVED SIGNAL 15: SIGTERM|2021-05-09 10:24:14.998|\n","|RECEIVED SIGNAL 15: SIGTERM|2021-05-09 05:32:33.494|\n","|RECEIVED SIGNAL 15: SIGTERM|2021-05-09 13:19:12.212|\n","|RECEIVED SIGNAL 1: SIGHUP  |2021-05-09 05:34:19.536|\n","|RECEIVED SIGNAL 1: SIGHUP  |2021-05-09 10:26:01.848|\n","|RECEIVED SIGNAL 1: SIGHUP  |2021-05-09 05:34:19.484|\n","|RECEIVED SIGNAL 1: SIGHUP  |2021-05-09 10:26:01.861|\n","|RECEIVED SIGNAL 15: SIGTERM|2021-05-09 05:34:19.5  |\n","|RECEIVED SIGNAL 15: SIGTERM|2021-05-09 10:26:01.811|\n","|RECEIVED SIGNAL 15: SIGTERM|2021-05-09 05:34:19.488|\n","|RECEIVED SIGNAL 15: SIGTERM|2021-05-09 10:26:01.856|\n","|RECEIVED SIGNAL 1: SIGHUP  |2021-05-09 05:32:33.003|\n","|RECEIVED SIGNAL 1: SIGHUP  |2021-05-09 10:24:14.942|\n","|RECEIVED SIGNAL 1: SIGHUP  |2021-05-09 13:19:12.303|\n","|RECEIVED SIGNAL 15: SIGTERM|2021-05-09 05:32:32.996|\n","|RECEIVED SIGNAL 15: SIGTERM|2021-05-09 10:24:14.963|\n","|RECEIVED SIGNAL 15: SIGTERM|2021-05-09 13:19:12.281|\n","+---------------------------+-----------------------+\n","only showing top 20 rows\n","\n"]}],"source":["Messages.show(false)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["splitMsg: org.apache.spark.sql.DataFrame = [Description: string, ErrorName: string ... 2 more fields]\n"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["var splitMsg = Messages.withColumn(\"Description\",split(col(\"message\"),\":\").getItem(0))\n","    .withColumn(\"ErrorName\",split(col(\"message\"),\":\").getItem(1))\n","    .withColumn(\"ErrorName\",ltrim(col(\"ErrorName\")))\n","    .withColumn(\"Date\",split(col(\"timestamp\"),\" \").getItem(0))\n","    .withColumn(\"Time\",split(col(\"timestamp\"),\" \").getItem(1))\n","    .drop(\"message\",\"timestamp\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------+---------+----------+------------+\n","|       Description|ErrorName|      Date|        Time|\n","+------------------+---------+----------+------------+\n","| RECEIVED SIGNAL 1|   SIGHUP|2021-05-09|10:24:14.943|\n","| RECEIVED SIGNAL 1|   SIGHUP|2021-05-09|05:32:33.531|\n","| RECEIVED SIGNAL 1|   SIGHUP|2021-05-09|13:19:12.254|\n","|RECEIVED SIGNAL 15|  SIGTERM|2021-05-09|10:24:14.998|\n","+------------------+---------+----------+------------+\n","only showing top 4 rows\n","\n"]}],"source":["splitMsg.show(4)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["frequencies: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n","vAR_ErrorCount: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ErrorName: string, Description: string ... 1 more field]\n"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["def frequencies(df: DataFrame) = df.groupBy(\"ErrorName\",\"Description\").count().orderBy('count.desc)\n","var vAR_ErrorCount = frequencies(splitMsg)"]},{"cell_type":"code","execution_count":11,"metadata":{"scrolled":true},"outputs":[{"data":{"text/plain":["vAR_ErrorCount: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ErrNam: string, Description: string ... 1 more field]\n"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["vAR_ErrorCount = vAR_ErrorCount.withColumnRenamed(\"ErrorName\",\"ErrNam\")"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/plain":["transform_expr: String = transform(ts_array, (x,i) -> (bigint((to_timestamp(ts_array[i+1]))) - (bigint(to_timestamp(x))))/60)\n","Frq: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n","frq: org.apache.spark.sql.DataFrame = [ErrorName: string, frequency: array<double>]\n"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["val transform_expr = \"transform(ts_array, (x,i) -> (bigint((to_timestamp(ts_array[i+1]))) - (bigint(to_timestamp(x))))/60)\"\n","def Frq(df : DataFrame) = df.groupBy(\"ErrorName\").agg(array_sort(collect_list(col(\"Time\"))).alias(\"ts_array\")) \n","    .withColumn(\"frequency\", expr(transform_expr))\n","    .drop(\"ts_array\")\n","val frq = Frq(splitMsg)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/plain":["vAR_dm: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ErrorName: string, frequency: array<interval> ... 3 more fields]\n"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["val vAR_dm = frq.join(vAR_ErrorCount).where(frq(\"ErrorName\") === vAR_ErrorCount(\"ErrNam\"))"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------+-----------------------------------------------------------+-----+\n","|ErrorName                     |frequency                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |ErrNam                        |Description                                                |count|\n","+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------+-----------------------------------------------------------+-----+\n","|sleep interrupted             |[0.00185 seconds, 0.007817 seconds, 4 minutes 51.691133 seconds, 0.003033 seconds, 0.002317 seconds, null]                                                                                                                                                                                                                                                                                                                                                                                          |sleep interrupted             |ExpiredTokenRemover received java.lang.InterruptedException|6    |\n","|SIGHUP                        |[0.000083 seconds, 0.000083 seconds, 0.0088 seconds, 0.00005 seconds, 1.7651 seconds, 0.000733 seconds, 0.000867 seconds, 0 seconds, 4 minutes 49.923433 seconds, 0.000017 seconds, 0 seconds, 0.0021 seconds, 0.0029 seconds, 1.776317 seconds, 0.000433 seconds, 0.000217 seconds, 0.000733 seconds, 2 minutes 53.171917 seconds, 0.000567 seconds, 0.000817 seconds, 0.001733 seconds, 0.000917 seconds, null]                                                                                   |SIGHUP                        |RECEIVED SIGNAL 1                                          |23   |\n","|SIGTERM                       |[0.00005 seconds, 0.00015 seconds, 0.00815 seconds, 0.000017 seconds, 1.765417 seconds, 0.001133 seconds, 0.0002 seconds, 0.000133 seconds, 0.015383 seconds, 0.0063 seconds, 4 minutes 49.902567 seconds, 0.000333 seconds, 0.00025 seconds, 0.000617 seconds, 0.001433 seconds, 1.778017 seconds, 0.00015 seconds, 0.00075 seconds, 0.000433 seconds, 0.009533 seconds, 0.000133 seconds, 2 minutes 53.1625 seconds, 0.000083 seconds, 0.001067 seconds, 0.002083 seconds, 0.001883 seconds, null]|SIGTERM                       |RECEIVED SIGNAL 15                                         |27   |\n","|java.lang.InterruptedException|[4 minutes 51.702067 seconds, 2 minutes 54.952483 seconds, null]                                                                                                                                                                                                                                                                                                                                                                                                                                    |java.lang.InterruptedException|Returning, interrupted                                     |3    |\n","+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------+-----------------------------------------------------------+-----+\n","\n"]}],"source":["vAR_dm.show(false)"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------------+-----------------------------------------------------------+-----+\n","|ErrNam                         |Description                                                |count|\n","+-------------------------------+-----------------------------------------------------------+-----+\n","| SIGTERM                       |RECEIVED SIGNAL 15                                         |17   |\n","| SIGHUP                        |RECEIVED SIGNAL 1                                          |15   |\n","|null                           |RECEIVED SIGNAL TERM                                       |8    |\n","| sleep interrupted             |ExpiredTokenRemover received java.lang.InterruptedException|5    |\n","| java.lang.InterruptedException|Returning, interrupted                                     |3    |\n","+-------------------------------+-----------------------------------------------------------+-----+\n","\n"]}],"source":["vAR_ErrorCount.show(false)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"ename":"java.lang.RuntimeException","evalue":" Failed to write to BigQuery","output_type":"error","traceback":["java.lang.RuntimeException: Failed to write to BigQuery","  at com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:92)","  at com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:43)","  at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:113)","  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)","  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)","  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)","  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)","  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)","  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)","  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)","  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)","  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)","  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)","  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)","  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)","  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)","  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)","  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)","  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)","  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)","  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)","  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)","  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)","  ... 42 elided","Caused by: org.apache.spark.sql.AnalysisException: Parquet data source does not support array<interval> data type.","  at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$verifySchema$1(DataSourceUtils.scala:67)","  at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$verifySchema$1$adapted(DataSourceUtils.scala:65)","  at scala.collection.Iterator.foreach(Iterator.scala:943)","  at scala.collection.Iterator.foreach$(Iterator.scala:943)","  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)","  at scala.collection.IterableLike.foreach(IterableLike.scala:74)","  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)","  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:102)","  at org.apache.spark.sql.execution.datasources.DataSourceUtils$.verifySchema(DataSourceUtils.scala:65)","  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:130)","  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)","  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)","  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)","  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)","  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)","  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)","  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)","  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)","  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)","  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)","  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)","  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)","  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)","  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)","  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)","  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)","  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)","  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)","  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)","  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)","  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)","  at com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:87)","  ... 65 more",""]}],"source":["vAR_dm.write \n","  .format(\"bigquery\") \n","  .option(\"table\", \"divine-course-311407:Errors.ErrFrq1\") \n","  .option(\"temporaryGcsBucket\",vAR_bucket)\n","  .mode(\"overwrite\") \n","  .save()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"spylon-kernel","language":"scala","name":"spylon-kernel"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","help_links":[{"text":"MetaKernel Magics","url":"https://metakernel.readthedocs.io/en/latest/source/README.html"}],"mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"0.4.1"},"name":"Model_DF_v4"},"nbformat":4,"nbformat_minor":4}