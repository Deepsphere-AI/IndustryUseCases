{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"data": {"text/plain": "Intitializing Scala interpreter ..."}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/plain": "Spark Web UI available at http://dsclusterbq-m:8088/proxy/application_1620947548405_0001\nSparkContext available as 'sc' (version = 3.1.1, master = yarn, app id = application_1620947548405_0001)\nSparkSession available as 'spark'\n"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "Import libraries started by = 2021-05-13T23:17:33.664Z\nImport libraries ended by = 2021-05-13T23:17:33.712Z\n"}, {"data": {"text/plain": "import org.joda.time.{DateTime, DateTimeZone}\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.{Row, SparkSession, SQLContext}\nimport spark.sqlContext.implicits._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.linalg.{Vectors, Vector}\n"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n     Step #1 :  Import Libraries\n*/\nimport org.joda.time.{DateTime, DateTimeZone}\nprintln(\"Import libraries started by = \" +DateTime.now(DateTimeZone.UTC))\n\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.{Row, SparkSession, SQLContext}\nimport spark.sqlContext.implicits._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.linalg.{Vectors,Vector}\n\nprintln(\"Import libraries ended by = \" +DateTime.now(DateTimeZone.UTC))\n"}, {"cell_type": "code", "execution_count": 34, "metadata": {"id": "BUszZR9z1yTl", "outputId": "e7ad22c6-50a4-4900-820b-c1138bf4dfaf"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Data load started by: 2021-05-12T20:50:06.085Z\nData load ended by: 2021-05-12T20:50:06.122Z\nDefault partition applied by system =  = 2\n"}, {"data": {"text/plain": "vAR_SalesData_csv_rdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[167] at map at <console>:105\n"}, "execution_count": 34, "metadata": {}, "output_type": "execute_result"}], "source": "/*\nStep #2 :  Load Training Data\n*/\n\n/* Reading csv data as RDD using .textFile method[takes the path and number of partitions as arguments]\n   maping each row with split. LabeledPoint assembles the target vector and the features of a dataset. \n   Cache stores all the RDD in-memory. */\nprintln(\"Data load started by: \" + DateTime.now(DateTimeZone.UTC))\nval vAR_SalesData_csv_rdd = sc.textFile(\"gs://dssparkbucket/DS.AI_SalesData_RDD.csv\").map { line =>\n      val p = line.split(',')\n      LabeledPoint(p(6).toDouble, Vectors.dense(p(0).toDouble,p(1).toDouble,p(2).toDouble,p(3).toDouble,p(4).toDouble,p(5).toDouble,p(7).toDouble,p(8).toDouble,p(9).toDouble,p(10).toDouble,p(11).toDouble,p(12).toDouble))\n    }.cache()\n\nprintln(\"Data load ended by: \" + DateTime.now(DateTimeZone.UTC))\nprintln(\"Default partition applied by system =  = \" + vAR_SalesData_csv_rdd.partitions.size)"}, {"cell_type": "code", "execution_count": 35, "metadata": {"id": "X0mE_-Wb1yTq", "outputId": "0a2c5574-4cb4-4bfc-a74b-b5d1b63a4718"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Split the loaded data into training and test\n"}, {"data": {"text/plain": "vAR_training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[168] at randomSplit at <console>:102\nvAR_test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[169] at randomSplit at <console>:102\n"}, "execution_count": 35, "metadata": {}, "output_type": "execute_result"}], "source": "/* \nStep #3 :Training and testing subsets by spliting using randomSplit method at 7:3 ratio. seed makes sure the subsets are mutualy exclusive \n*/\nprintln(\"Split the loaded data into training and test\")\nval Array(vAR_training, vAR_test) = vAR_SalesData_csv_rdd.randomSplit(Array(0.7, 0.3), seed = 11L)\n"}, {"cell_type": "code", "execution_count": 36, "metadata": {"id": "pmUwwGGd1yTu", "outputId": "cc7b984f-a435-4b9e-ded2-9d05fe6f77ce"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Parameter Update started by = 2021-05-12T20:50:07.042Z\nParameter Update Ended by = 2021-05-12T20:50:07.042Z\n"}, {"data": {"text/plain": "vAR_categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()\nvAR_impurity: String = variance\nvAR_maxBins: Int = 32\nvAR_numTrees: Int = 15\nvAR_featureSubsetStrategy: String = auto\n"}, "execution_count": 36, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n  Step #4 :  Setup Parameters\n*/\nprintln(\"Parameter Update started by = \" + DateTime.now(DateTimeZone.UTC))\nval vAR_categoricalFeaturesInfo = Map[Int, Int]()\nval vAR_impurity =  \"variance\"\nval vAR_maxBins = 32\nval vAR_numTrees = 15 // number of trees to run in parallel\nval vAR_featureSubsetStrategy = \"auto\" // Let the algorithm choose.\nprintln(\"Parameter Update Ended by = \" + DateTime.now(DateTimeZone.UTC))\n\n"}, {"cell_type": "code", "execution_count": 37, "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Train, Test , Validate and Prediction of RandomForest Algorithm started by = 2021-05-12T20:50:07.606Z\nTest Mean Squared Error = 367.07838749999996\nTrain, Test , Validate and Prediction of RandomForest Algorithm ended by = 2021-05-12T20:50:08.523Z\n"}, {"data": {"text/plain": "vAR_model: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel regressor of depth 4 with 19 nodes\nvAR_labelsAndPredictions: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[194] at map at <console>:111\nvAR_testMSE: Double = 367.07838749999996\n"}, "execution_count": 37, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n  Step #5 :  Setup Pipeline, Train , Test , Validate and Predict\n*/\nprintln(\"Train, Test , Validate and Prediction of DecisionTree Algorithm started by = \" + DateTime.now(DateTimeZone.UTC))\n\nval vAR_model = DecisionTree.trainRegressor(vAR_training, vAR_categoricalFeaturesInfo, vAR_impurity,\n  vAR_maxDepth, vAR_maxBins)\nval vAR_labelsAndPredictions = vAR_test.map { point =>\n  val vAR_prediction = vAR_model.predict(point.features)\n  (point.label, vAR_prediction)\n}\nval vAR_testMSE = vAR_labelsAndPredictions.map{ case (v, p) => math.pow(v - p, 2) }.mean()\nprintln(s\"Test Mean Squared Error = $vAR_testMSE\")\n\nprintln(\"Train, Test , Validate and Prediction of DecisionTree Algorithm ended by = \" + DateTime.now(DateTimeZone.UTC))\n"}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Apply Repartition started by= 2021-05-12T20:50:08.917Z\nRepartition count set as = 4\nApply Repartition Ended by= 2021-05-12T20:50:08.925Z\nRepartition Applied count = 4\n"}, {"data": {"text/plain": "vAR_repart_count_set: Int = 4\nvAR_SalesData_csv_rdd_part: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[200] at repartition at <console>:105\n"}, "execution_count": 38, "metadata": {}, "output_type": "execute_result"}], "source": "/*\nStep #6 : Apply Repartition count\n*/\nprintln(\"Apply Repartition started by= \" + DateTime.now(DateTimeZone.UTC))\n//Apply Repartition to compare performance with default partition\nval vAR_repart_count_set = 4\nprintln(\"Repartition count set as = \" + vAR_repart_count_set)\nval vAR_SalesData_csv_rdd_part = vAR_SalesData_csv_rdd.repartition(4)\nprintln(\"Apply Repartition Ended by= \" + DateTime.now(DateTimeZone.UTC))\nprintln(\"Repartition Applied count = \" + vAR_SalesData_csv_rdd_part.partitions.size)\n\n"}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "After Repartition Split the loaded data into training and test\n"}, {"data": {"text/plain": "vAR_training_part: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[201] at randomSplit at <console>:102\nvAR_test_part: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[202] at randomSplit at <console>:102\n"}, "execution_count": 39, "metadata": {}, "output_type": "execute_result"}], "source": "/* \nStep #7 :Training and testing subsets by spliting using randomSplit method at 7:3 ratio. seed makes sure the subsets are mutualy exclusive \n*/\nprintln(\"After Repartition Split the loaded data into training and test\")\nval Array(vAR_training_part,vAR_test_part) = vAR_SalesData_csv_rdd_part.randomSplit(Array(0.7, 0.3), seed = 11L)"}, {"cell_type": "code", "execution_count": 40, "metadata": {"id": "pmUwwGGd1yTu", "outputId": "cc7b984f-a435-4b9e-ded2-9d05fe6f77ce"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "After Repartition parameter setup started by: 2021-05-12T20:50:09.834Z\nAfter Repartition parameter setup ended by: 2021-05-12T20:50:09.834Z\n"}, {"data": {"text/plain": "vAR_categoricalFeaturesInfo_part: scala.collection.immutable.Map[Int,Int] = Map()\nvAR_numTrees_part: Int = 30\nvAR_featureSubsetStrategy_part: String = auto\nvAR_impurity_part: String = variance\nvAR_maxBins_part: Int = 32\n"}, "execution_count": 40, "metadata": {}, "output_type": "execute_result"}], "source": "/* \nStep #8 : Setup parameters\n*/\nprintln(\"After Repartition parameter setup started by: \" +DateTime.now(DateTimeZone.UTC))\n\nval vAR_categoricalFeaturesInfo_part = Map[Int, Int]()\nval vAR_numTrees_part = 30 // number of trees to run in parallel  after repartition\nval vAR_featureSubsetStrategy_part = \"auto\" // Let the algorithm choose.\nval vAR_impurity_part = \"variance\"\nval vAR_maxBins_part = 32\n\nprintln(\"After Repartition parameter setup ended by: \" +DateTime.now(DateTimeZone.UTC))\n\n"}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "After Repartition Model Fit, Prediction  of RandomForest Algorithm  started by: 2021-05-12T20:50:10.546Z\nTest Mean Squared Error = 184.345905\nLearned regression forest model:\n DecisionTreeModel regressor of depth 4 with 11 nodes\n  If (feature 4 <= 14366.5)\n   If (feature 1 <= 875.0)\n    Predict: 13.639999999999999\n   Else (feature 1 > 875.0)\n    If (feature 8 <= 5.5)\n     If (feature 3 <= 6.470000000000001)\n      Predict: 3.64\n     Else (feature 3 > 6.470000000000001)\n      Predict: 10.055\n    Else (feature 8 > 5.5)\n     If (feature 3 <= 6.470000000000001)\n      Predict: 13.639999999999999\n     Else (feature 3 > 6.470000000000001)\n      Predict: 6.470000000000001\n  Else (feature 4 > 14366.5)\n   Predict: 18.295\n\nAfter Repartition Model Fit, Prediction  of RandomForest Algorithm  ended by: 2021-05-12T20:50:11.447Z\n"}, {"data": {"text/plain": "vAR_model_part: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel regressor of depth 4 with 11 nodes\nvAR_labelsAndPredictions_part: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[224] at map at <console>:114\nvAR_testMSE_part: Double = 184.345905\n"}, "execution_count": 41, "metadata": {}, "output_type": "execute_result"}], "source": "/*\nStep #9 : Train Data  after repartition\n*/\nprintln(\"After Repartition Model Fit, Prediction  of DecisionTree Algorithm  started by: \" + DateTime.now(DateTimeZone.UTC))\nval vAR_model_part = DecisionTree.trainRegressor(vAR_training_part, vAR_categoricalFeaturesInfo_part, vAR_impurity_part,\n  vAR_maxDepth_part, vAR_maxBins_part)\n\n/*\nStep #10 : Evaluate model on test instances and compute test error\n*/\nval vAR_labelsAndPredictions_part = vAR_test_part.map { point =>\n  val vAR_prediction_part = vAR_model_part.predict(point.features)\n  (point.label, vAR_prediction_part)\n}\n\nval vAR_testMSE_part = vAR_labelsAndPredictions_part.map{ case(v, p) => math.pow((v - p), 2)}.mean()\nprintln(s\"Test Mean Squared Error = $vAR_testMSE_part\")\nprintln(s\"Learned regression forest model:\\n ${vAR_model_part.toDebugString}\")\n\nprintln(\"After Repartition Model Fit, Prediction  of DecisionTree Algorithm  ended by: \" + DateTime.now(DateTimeZone.UTC))"}], "metadata": {"celltoolbar": "Raw Cell Format", "colab": {"collapsed_sections": [], "name": "DataPl1-Copy1.ipynb", "provenance": []}, "kernelspec": {"display_name": "spylon-kernel", "language": "scala", "name": "spylon-kernel"}, "language_info": {"codemirror_mode": "text/x-scala", "file_extension": ".scala", "help_links": [{"text": "MetaKernel Magics", "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"}], "mimetype": "text/x-scala", "name": "scala", "pygments_lexer": "scala", "version": "0.4.1"}}, "nbformat": 4, "nbformat_minor": 4}