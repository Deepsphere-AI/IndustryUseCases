{"cells": [{"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Import libraries started by = 2021-05-11T17:44:33.349Z\nImport libraries ended by = 2021-05-11T17:44:33.349Z\n"}, {"data": {"text/plain": "import org.joda.time.{DateTime, DateTimeZone}\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.{Row, SparkSession}\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.VectorIndexer\nimport org.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\n"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n     Step #1 :  Import Libraries\n*/\nimport org.joda.time.{DateTime, DateTimeZone}\nprintln(\"Import libraries started by = \" +DateTime.now(DateTimeZone.UTC))\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.{Row, SparkSession}\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.VectorIndexer\nimport org.apache.spark.ml.regression.DecisionTreeRegressionModel\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\nprintln(\"Import libraries ended by = \" + DateTime.now(DateTimeZone.UTC))\n\n"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Data load started by = 2021-05-11T17:44:34.445Z\nData load ended by = 2021-05-11T17:44:46.521Z\nDefault partitions applied by system = 1\n"}, {"data": {"text/plain": "vAR_df_data: org.apache.spark.sql.DataFrame = [TransactionID: string, ProductID: string ... 11 more fields]\n"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n  Step #2 :  Load Training Data\n*/\nprintln(\"Data load started by = \" + DateTime.now(DateTimeZone.UTC))\nval vAR_df_data = spark.read.option(\"header\",true).csv(\"gs://dssparkbucket/DS.AI_SalesData.csv\")\nprintln(\"Data load ended by = \" + DateTime.now(DateTimeZone.UTC))\n//check for partitions\nprintln(\"Default partitions applied by system = \" + vAR_df_data.rdd.partitions.length)\n"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Data transform started by: 2021-05-11T17:44:47.955Z\nVectorize the feature columns\nData transform ended by: 2021-05-11T17:44:48.602Z\n"}, {"data": {"text/plain": "vAR_df: org.apache.spark.sql.DataFrame = [TransactionID: int, ProductID: int ... 11 more fields]\nvAR_assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_16b6ca18473c, handleInvalid=error, numInputCols=12\nvAR_output: org.apache.spark.sql.DataFrame = [TransactionID: int, ProductID: int ... 12 more fields]\n"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n  Step #3 : transform data for training\n*/\nprintln(\"Data transform started by: \" + DateTime.now(DateTimeZone.UTC))\nval vAR_df = vAR_df_data.withColumn(\"TransactionID\",col(\"TransactionID\").cast(IntegerType))\n        .withColumn(\"ProductID\",col(\"ProductID\").cast(IntegerType))\n        .withColumn(\"Quantity\",col(\"Quantity\").cast(IntegerType))\n        .withColumn(\"ActualCost\",col(\"ActualCost\").cast(IntegerType))\n        .withColumn(\"CustomerID\",col(\"CustomerID\").cast(IntegerType))\n        .withColumn(\"TotalDue\",col(\"TotalDue\").cast(IntegerType))\n        .withColumn(\"LineTotal\",col(\"LineTotal\").cast(IntegerType))\n        .withColumn(\"MakeFlag\",col(\"MakeFlag\").cast(IntegerType))\n        .withColumn(\"FinishedGoodsFlag\",col(\"FinishedGoodsFlag\").cast(IntegerType))\n        .withColumn(\"SalesReasonID\",col(\"SalesReasonID\").cast(IntegerType))\n        .withColumn(\"AverageRate\",col(\"AverageRate\").cast(IntegerType))\n        .withColumn(\"EndOfDayRate\",col(\"EndOfDayRate\").cast(IntegerType))\n        .withColumn(\"SalesLastYear\",col(\"SalesLastYear\").cast(IntegerType))     \n /*\n   VectorAssembler is a transformer as it takes the input dataframe and returns the transformed dataframe\n   with a new column which is vector representation of all the features.\n   Select Features columns as \"TransactionID\", \"Quantity\", \"ActualCost\", \"CustomerID\", \"TotalDue\", \"LineTotal\", \"FinishedGoodsFlag\", \"SalesReasonID\", \"AverageRate\", \"EndOfDayRate\", \"SalesLastYear\", \"ProductID\"\n */\nprintln(\"Vectorize the feature columns\")\nval vAR_assembler = new VectorAssembler()\n  .setInputCols(Array(\"TransactionID\",\"Quantity\",\"ActualCost\",\"CustomerID\",\"TotalDue\",\"LineTotal\",\"FinishedGoodsFlag\",\"SalesReasonID\",\"AverageRate\",\"EndOfDayRate\",\"SalesLastYear\",\"ProductID\"))\n  .setOutputCol(\"features\")\nval vAR_output = vAR_assembler.transform(vAR_df)\nprintln(\"Data transform ended by: \" + DateTime.now(DateTimeZone.UTC))"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Split the loaded data into training and test\n"}, {"data": {"text/plain": "vAR_trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [TransactionID: int, ProductID: int ... 12 more fields]\nvAR_testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [TransactionID: int, ProductID: int ... 12 more fields]\n"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n  Step #4 :  Select  Training,Test Data as 70:30\n*/\nprintln(\"Split the loaded data into training and test\")\nval Array(vAR_trainingData, vAR_testData) = vAR_output.randomSplit(Array(0.7, 0.3), seed = 1234L)"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Data prep started by = 2021-05-11T17:44:50.879Z\nIndex the feature column data\nInstantiate the DecisionTreeRegressor algorithm\nSetup the Pipeline\nData prep ended by = 2021-05-11T17:44:55.256Z\n"}, {"data": {"text/plain": "vAR_featureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = VectorIndexerModel: uid=vecIdx_9a3504e6c8ee, numFeatures=12, handleInvalid=error\nvAR_dectree: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_0d298550b640\nvAR_pipeline: org.apache.spark.ml.Pipeline = pipeline_a2c28b01aff6\n"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n  Step #5 :  Setup Pipeline\n*/\n\nprintln(\"Data prep started by = \" + DateTime.now(DateTimeZone.UTC))\n\nprintln(\"Index the feature column data\")\nval vAR_featureIndexer = new VectorIndexer()\n  .setInputCol(\"features\")\n  .setOutputCol(\"indexedFeatures\")\n  .setMaxCategories(13)\n  .fit(vAR_output)\n\nprintln(\"Instantiate the DecisionTreeRegressor algorithm\")\nval vAR_dectree = new DecisionTreeRegressor()\n  .setLabelCol(\"MakeFlag\")\n  .setFeaturesCol(\"indexedFeatures\")\n\nprintln(\"Setup the Pipeline\")\nval vAR_pipeline = new Pipeline()\n  .setStages(Array(vAR_featureIndexer, vAR_dectree))\n\nprintln(\"Data prep ended by = \" + DateTime.now(DateTimeZone.UTC))"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Train, Test , Validate and Predict started by = 2021-05-11T17:44:56.143Z\n+----------+--------+--------------------+\n|prediction|MakeFlag|            features|\n+----------+--------+--------------------+\n|       0.0|       0|[103860.0,1.0,7.0...|\n|       0.0|       0|[103860.0,1.0,7.0...|\n|       0.0|       0|[103862.0,1.0,32....|\n|       0.0|       0|[103870.0,1.0,24....|\n|       0.0|       0|[103871.0,1.0,7.0...|\n+----------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 0.0\nLearned regression tree model:\n DecisionTreeRegressionModel: uid=dtr_0d298550b640, depth=0, numNodes=1, numFeatures=12\n  Predict: 0.0\n\nTrain, Test , Validate and Predict ended by = 2021-05-11T17:45:01.969Z\n"}, {"data": {"text/plain": "vAR_model: org.apache.spark.ml.PipelineModel = pipeline_a2c28b01aff6\nvAR_predictions: org.apache.spark.sql.DataFrame = [TransactionID: int, ProductID: int ... 14 more fields]\nvAR_evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_a596eb160f01, metricName=rmse, throughOrigin=false\nvAR_rmse: Double = 0.0\nvAR_treeModel: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel: uid=dtr_0d298550b640, depth=0, numNodes=1, numFeatures=12\n"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n  Step #6 :  Train , Test , Validate and Predict\n*/\nprintln(\"Train, Test , Validate and Predict started by = \" + DateTime.now(DateTimeZone.UTC))\n\n// Train model.\nval vAR_model = vAR_pipeline.fit(vAR_trainingData)\n\n// Make predictions.\nval vAR_predictions = vAR_model.transform(vAR_testData)\n\n// Select example rows to display.\nvAR_predictions.select(\"prediction\", \"MakeFlag\", \"features\").show(5)\n\n// Select (prediction, true label) and compute test error.\nval vAR_evaluator = new RegressionEvaluator()\n  .setLabelCol(\"MakeFlag\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"rmse\")\n\nval vAR_rmse = vAR_evaluator.evaluate(vAR_predictions)\nprintln(s\"Root Mean Squared Error (RMSE) on test data = $vAR_rmse\")\n\nval vAR_treeModel = vAR_model.stages(1).asInstanceOf[DecisionTreeRegressionModel]\nprintln(s\"Learned regression tree model:\\n ${vAR_treeModel.toDebugString}\")\n\nprintln(\"Train, Test , Validate and Predict ended by = \" + DateTime.now(DateTimeZone.UTC))"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Partition Applied default by System  = 1\nRepartition started by = 2021-05-11T17:45:02.734Z\nRepartition count set as = 4\n4\nRepartition ended by = 2021-05-11T17:45:03.285Z\n"}, {"data": {"text/plain": "vAR_repart_count_set: Int = 4\nvAR_repart_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [TransactionID: int, ProductID: int ... 11 more fields]\n"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n Step #7 : Apply Repartition count\n*/\nprintln(\"Partition Applied default by System  = \" + vAR_df.rdd.getNumPartitions)\nprintln(\"Repartition started by = \" + DateTime.now(DateTimeZone.UTC))\n//Apply Repartition to compare Data Parallelism performance with default partition\nval vAR_repart_count_set = 4\nprintln(\"Repartition count set as = \" + vAR_repart_count_set)\nval vAR_repart_df = vAR_df.repartition(vAR_repart_count_set)\nprintln(vAR_repart_df.rdd.partitions.length)\nprintln(\"Repartition ended by = \" + DateTime.now(DateTimeZone.UTC))"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "After partition Vectorize the feature columns\n"}, {"data": {"text/plain": "vAR_repart_assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_63d960061d2f, handleInvalid=error, numInputCols=12\nvAR_repart_output: org.apache.spark.sql.DataFrame = [TransactionID: int, ProductID: int ... 12 more fields]\n"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "println(\"After partition Vectorize the feature columns\")\nval vAR_repart_assembler = new VectorAssembler()\n  .setInputCols(Array(\"TransactionID\",\"Quantity\",\"ActualCost\",\"CustomerID\",\"TotalDue\",\"LineTotal\",\"FinishedGoodsFlag\",\"SalesReasonID\",\"AverageRate\",\"EndOfDayRate\",\"SalesLastYear\",\"ProductID\"))\n  .setOutputCol(\"features\")\nval vAR_repart_output = vAR_repart_assembler.transform(vAR_repart_df)"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "After partition Split the loaded data into training and test\n"}, {"data": {"text/plain": "vAR_repart_training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [TransactionID: int, ProductID: int ... 12 more fields]\nvAR_repart_test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [TransactionID: int, ProductID: int ... 12 more fields]\n"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "println(\"After partition Split the loaded data into training and test\")\nval Array(vAR_repart_training, vAR_repart_test) = vAR_repart_output.randomSplit(Array(0.7, 0.3), seed = 1234L)"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "After partition Data prep started by = 2021-05-11T17:45:05.244Z\nAfter partition Index the feature column data\nAfter partition Instantiate the DecisionTreeRegressor algorithm\nAfter partition Setup the Pipeline\nAfter partition Data prep ended by = 2021-05-11T17:45:06.216Z\n"}, {"data": {"text/plain": "vAR_repart_featureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = VectorIndexerModel: uid=vecIdx_42e83d54d5c4, numFeatures=12, handleInvalid=error\nvAR_repart_dectree: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_b2df5a0ea5c6\nvAR_repart_pipeline: org.apache.spark.ml.Pipeline = pipeline_7fc82dd7ef6d\n"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "println(\"After partition Data prep started by = \" +  DateTime.now(DateTimeZone.UTC))\n\nprintln(\"After partition Index the feature column data\")\nval vAR_repart_featureIndexer = new VectorIndexer()\n  .setInputCol(\"features\")\n  .setOutputCol(\"indexedFeatures\")\n  .setMaxCategories(13)\n  .fit(vAR_repart_output)\n\nprintln(\"After partition Instantiate the DecisionTreeRegressor algorithm\")\nval vAR_repart_dectree = new DecisionTreeRegressor()\n  .setLabelCol(\"MakeFlag\")\n  .setFeaturesCol(\"indexedFeatures\")\n\nprintln(\"After partition Setup the Pipeline\")\n// Chain indexer and tree in a Pipeline.\nval vAR_repart_pipeline = new Pipeline()\n  .setStages(Array(vAR_repart_featureIndexer, vAR_repart_dectree))\n\nprintln(\"After partition Data prep ended by = \" + DateTime.now(DateTimeZone.UTC))"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "After partition Train, Test after partition started by = 2021-05-11T17:45:06.785Z\nAfter partition Train, Test ended by = 2021-05-11T17:45:08.797Z\n"}, {"data": {"text/plain": "vAR_repart_model: org.apache.spark.ml.PipelineModel = pipeline_7fc82dd7ef6d\nvAR_repart_predictions: org.apache.spark.sql.DataFrame = [TransactionID: int, ProductID: int ... 14 more fields]\n"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "println(\"After partition Train, Test after partition started by = \" + DateTime.now(DateTimeZone.UTC))\n\n// Train model. \nval vAR_repart_model = vAR_repart_pipeline.fit(vAR_repart_training)\n\n// Make predictions.\nval vAR_repart_predictions = vAR_repart_model.transform(vAR_repart_test)\n\nprintln(\"After partition Train, Test ended by = \" +DateTime.now(DateTimeZone.UTC))"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+--------+--------------------+\n|prediction|MakeFlag|            features|\n+----------+--------+--------------------+\n|       0.0|       0|[103868.0,1.0,2.0...|\n|       0.0|       0|[103868.0,1.0,2.0...|\n|       0.0|       0|[103870.0,1.0,24....|\n|       0.0|       0|[103868.0,1.0,2.0...|\n|       0.0|       0|[103869.0,1.0,4.0...|\n+----------+--------+--------------------+\nonly showing top 5 rows\n\nAfter partition Root Mean Squared Error (RMSE) on test data = 0.0\nAfter partition Learned regression tree model:\n DecisionTreeRegressionModel: uid=dtr_b2df5a0ea5c6, depth=0, numNodes=1, numFeatures=12\n  Predict: 0.0\n\n"}, {"data": {"text/plain": "vAR_repart_evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_672beeb84bb9, metricName=rmse, throughOrigin=false\nvAR_repart_rmse: Double = 0.0\nvAR_repart_treeModel: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel: uid=dtr_b2df5a0ea5c6, depth=0, numNodes=1, numFeatures=12\n"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "// Select example rows to display.\nvAR_repart_predictions.select(\"prediction\", \"MakeFlag\", \"features\").show(5)\n\n// Select (prediction, true label) and compute test error.\nval vAR_repart_evaluator = new RegressionEvaluator()\n  .setLabelCol(\"MakeFlag\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"rmse\")\nval vAR_repart_rmse = vAR_repart_evaluator.evaluate(vAR_repart_predictions)\nprintln(s\"After partition Root Mean Squared Error (RMSE) on test data = $vAR_repart_rmse\")\n\nval vAR_repart_treeModel = vAR_repart_model.stages(1).asInstanceOf[DecisionTreeRegressionModel]\nprintln(s\"After partition Learned regression tree model:\\n ${vAR_repart_treeModel.toDebugString}\")\n"}], "metadata": {"kernelspec": {"display_name": "spylon-kernel", "language": "scala", "name": "spylon-kernel"}, "language_info": {"codemirror_mode": "text/x-scala", "file_extension": ".scala", "help_links": [{"text": "MetaKernel Magics", "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"}], "mimetype": "text/x-scala", "name": "scala", "pygments_lexer": "scala", "version": "0.4.1"}}, "nbformat": 4, "nbformat_minor": 4}