{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"autoscroll": "auto"}, "outputs": [{"data": {"text/plain": "Intitializing Scala interpreter ..."}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/plain": "Spark Web UI available at http://dsclusterbq-m:8088/proxy/application_1620786445671_0001\nSparkContext available as 'sc' (version = 3.1.1, master = yarn, app id = application_1620786445671_0001)\nSparkSession available as 'spark'\n"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "Import libraries started by = 2021-05-12T02:31:21.291Z\nImport libraries ended by = 2021-05-12T02:31:21.347Z\n"}, {"data": {"text/plain": "import org.joda.time.{DateTime, DateTimeZone}\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\nimport org.apache.spark.ml.regression.RandomForestRegressor\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n     Step #1 :  Import Libraries\n*/\nimport org.joda.time.{DateTime, DateTimeZone}\nprintln(\"Import libraries started by = \" +DateTime.now(DateTimeZone.UTC))\n\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\nimport org.apache.spark.ml.regression.RandomForestRegressor\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n\nprintln(\"Import libraries ended by = \" +DateTime.now(DateTimeZone.UTC))"}, {"cell_type": "code", "execution_count": 2, "metadata": {"autoscroll": "auto"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Data load Started by: 2021-05-12T02:31:22.006Z\nData load  Ended by: 2021-05-12T02:31:34.826Z\nDefault partitions applied by system = 1\n"}, {"data": {"text/plain": "vAR_SalesData_csv_df: org.apache.spark.sql.DataFrame = [TransactionID: int, ProductID: int ... 11 more fields]\n"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n  Step #2 :  Load Training Data\n*/\n  println(\"Data load Started by: \" + DateTime.now(DateTimeZone.UTC))\n  val vAR_SalesData_csv_df = spark.read\n    .option(\"header\", true)\n    .option(\"inferSchema\", \"true\")\n    .csv(\"gs://dssparkbucket/DS.AI_SalesData.csv\")\n  println(\"Data load  Ended by: \" + DateTime.now(DateTimeZone.UTC))\n  println(\"Default partition applied by system = \" + vAR_SalesData_csv_df.rdd.getNumPartitions)"}, {"cell_type": "code", "execution_count": 3, "metadata": {"autoscroll": "auto"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Data transform Started by: 2021-05-12T02:31:35.946Z\nData transform Ended by: 2021-05-12T02:31:40.254Z\n"}, {"data": {"text/plain": "vAR_assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_b0bd7f0e24fd, handleInvalid=error, numInputCols=12\nvAR_transformed_output: org.apache.spark.sql.DataFrame = [TransactionID: int, ProductID: int ... 12 more fields]\nvAR_labelIndexer: org.apache.spark.ml.feature.StringIndexerModel = StringIndexerModel: uid=strIdx_d5cc61ec71e6, handleInvalid=error\nvAR_featureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = VectorIndexerModel: uid=vecIdx_81d5ec82ceae, numFeatures=12, handleInvalid=error\n"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "\n/*\n  Step #3 : transform data for training\n*/\n\n  println(\"Data transform Started by: \" + DateTime.now(DateTimeZone.UTC))\n\n /*\n   VectorAssembler is a transformer as it takes the input dataframe and returns the transformed dataframe\n   with a new column which is vector representation of all the features.\n   Select Features columns as \"TransactionID\", \"Quantity\", \"ActualCost\", \"CustomerID\", \"TotalDue\", \"LineTotal\", \"FinishedGoodsFlag\", \"SalesReasonID\", \"AverageRate\", \"EndOfDayRate\", \"SalesLastYear\", \"ProductID\"\n */\n  val vAR_assembler = new VectorAssembler()\n    .setInputCols(Array(\"TransactionID\", \"Quantity\", \"ActualCost\", \"CustomerID\", \"TotalDue\", \"LineTotal\", \"FinishedGoodsFlag\", \"SalesReasonID\", \"AverageRate\", \"EndOfDayRate\", \"SalesLastYear\", \"ProductID\"))\n    .setOutputCol(\"features\")\n  val vAR_transformed_output = vAR_assembler.transform(vAR_SalesData_csv_df)\n   \n  val vAR_labelIndexer = new StringIndexer()\n  .setInputCol(\"MakeFlag\")\n  .setOutputCol(\"indexedLabel\")\n  .fit(vAR_transformed_output)\n\n // Set maxCategories so features with > 4 distinct values are treated as continuous.\n val vAR_featureIndexer = new VectorIndexer()\n  .setInputCol(\"features\")\n  .setOutputCol(\"indexedFeatures\")\n  .setMaxCategories(4)\n  .fit(vAR_transformed_output)\n\nprintln(\"Data transform Ended by: \" + DateTime.now(DateTimeZone.UTC))\n\n"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Split the loaded data into training and test\n"}, {"data": {"text/plain": "vAR_splitupdata: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([TransactionID: int, ProductID: int ... 12 more fields], [TransactionID: int, ProductID: int ... 12 more fields])\nvAR_training_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [TransactionID: int, ProductID: int ... 12 more fields]\nvAR_test_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [TransactionID: int, ProductID: int ... 12 more fields]\n"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n  Step #4 :  Select  Training,Test Data as 70:30\n*/\n  println(\"Split the loaded data into training and test\")\n  val vAR_splitupdata = vAR_transformed_output.randomSplit(Array(0.70, 0.30))\n  val vAR_training_data = vAR_splitupdata(0)\n  val vAR_test_data = vAR_splitupdata(1)"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Data prep started by = 2021-05-12T02:31:41.766Z\nData Prep Ended by = 2021-05-12T02:31:41.840Z\n"}, {"data": {"text/plain": "vAR_rf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_811e9c94c94a\nvAR_labelConverter: org.apache.spark.ml.feature.IndexToString = idxToStr_3aca36832261\nvAR_pipeline: org.apache.spark.ml.Pipeline = pipeline_6f3afd6381b5\n"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n  Step #5 :  Setup Pipeline\n*/\nprintln(\"Data prep started by = \" + DateTime.now(DateTimeZone.UTC))\n// Train a RandomForest model.\nval vAR_rf = new RandomForestRegressor()\n  .setLabelCol(\"indexedLabel\")\n  .setFeaturesCol(\"indexedFeatures\")\n  .setNumTrees(15)  // number of trees to run in parallel  \n\n// Convert indexed labels back to original labels.\nval vAR_labelConverter = new IndexToString()\n  .setInputCol(\"prediction\")\n  .setOutputCol(\"predictedLabel\")\n  .setLabels(vAR_labelIndexer.labels)\n\n// Chain indexers and forest in a Pipeline\nval vAR_pipeline = new Pipeline()\n  .setStages(Array(vAR_labelIndexer, vAR_featureIndexer, vAR_rf, vAR_labelConverter))\n\nprintln(\"Data Prep Ended by = \" + DateTime.now(DateTimeZone.UTC))"}, {"cell_type": "code", "execution_count": 6, "metadata": {"autoscroll": "auto"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Train, Test , Validate and Predict started by = 2021-05-12T02:31:42.401Z\n+----------+--------+--------------------+\n|prediction|MakeFlag|            features|\n+----------+--------+--------------------+\n|       0.0|       0|[103860.0,1.0,7.9...|\n|       0.0|       0|[103861.0,1.0,3.9...|\n|       0.0|       0|[103861.0,1.0,3.9...|\n|       0.0|       0|[103868.0,1.0,2.2...|\n|       0.0|       0|[103868.0,1.0,2.2...|\n+----------+--------+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 0.0\nTrain, Test , Validate and Predict ended by = 2021-05-12T02:31:55.002Z\n"}, {"data": {"text/plain": "vAR_model: org.apache.spark.ml.PipelineModel = pipeline_6f3afd6381b5\nvAR_predictions: org.apache.spark.sql.DataFrame = [TransactionID: int, ProductID: int ... 16 more fields]\nvAR_evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_df316fb9d76e, metricName=rmse, throughOrigin=false\nvAR_rmse: Double = 0.0\n"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "\n/*\n  Step #6 :  Train , Test , Validate and Predict\n  */\n\nprintln(\"Train, Test , Validate and Predict started by = \" + DateTime.now(DateTimeZone.UTC))\n\n// Train model.  This also runs the indexers.\nval vAR_model = vAR_pipeline.fit(vAR_training_data)\n\n// Make predictions.\nval vAR_predictions = vAR_model.transform(vAR_test_data)\n\n// Select example rows to display.\nvAR_predictions.select(\"prediction\", \"MakeFlag\", \"features\").show(5)\n\n// Select (prediction, true label) and compute test error.\nval vAR_evaluator = new RegressionEvaluator()\n  .setLabelCol(\"MakeFlag\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"rmse\")\n\nval vAR_rmse = vAR_evaluator.evaluate(vAR_predictions)\nprintln(s\"Root Mean Squared Error (RMSE) on test data = $vAR_rmse\")\n\nprintln(\"Train, Test , Validate and Predict ended by = \" + DateTime.now(DateTimeZone.UTC))\n\n\n\n\n"}, {"cell_type": "code", "execution_count": 7, "metadata": {"autoscroll": "auto"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Partition Applied default by System  = 1\nApply Repartition Started by : 2021-05-12T02:31:55.417Z\nRepartition count set as = 4\nApply Repartition Ended by= 2021-05-12T02:31:55.422Z\nRepartition Applied count = 4\n"}, {"data": {"text/plain": "vAR_repart_count_set: Int = 4\nvAR_SalesData_csv_df_part: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [TransactionID: int, ProductID: int ... 11 more fields]\n"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "/*\nStep #7 : Apply Repartition count\n*/\n\n  println(\"Partition Applied default by System  = \" + vAR_SalesData_csv_df.rdd.getNumPartitions)\n  println(\"Apply Repartition Started by : \" + DateTime.now(DateTimeZone.UTC))\n \n  //Apply Repartition to compare performance with default partition\n  val vAR_repart_count_set = 4\n  println(\"Repartition count set as = \" + vAR_repart_count_set)\n  val vAR_SalesData_csv_df_part = vAR_SalesData_csv_df.repartition(vAR_repart_count_set)\n  println(\"Apply Repartition Ended by= \" + DateTime.now(DateTimeZone.UTC))\n\n  println(\"Repartition Applied count = \" + vAR_SalesData_csv_df_part.rdd.getNumPartitions)"}, {"cell_type": "code", "execution_count": 8, "metadata": {"autoscroll": "auto"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "After Repartition Data transform Started by: 2021-05-12T02:31:56.213Z\nAfter Repartition Data transform Ended by: 2021-05-12T02:31:56.285Z\nAfter Repartition Data Prep started by: 2021-05-12T02:31:56.290Z\nAfter Repartition Data prep ended by: 2021-05-12T02:31:56.561Z\n"}, {"data": {"text/plain": "vAR_assembler_part: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_9c72138ac573, handleInvalid=error, numInputCols=12\nvAR_transformed_output_part: org.apache.spark.sql.DataFrame = [TransactionID: int, ProductID: int ... 12 more fields]\nvAR_splitupdata_part: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([TransactionID: int, ProductID: int ... 12 more fields], [TransactionID: int, ProductID: int ... 12 more fields])\nvAR_training_data_part: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [TransactionID: int, ProductID: int ... 12 more fields]\nvAR_test_data_part: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [TransactionID: int, ProductID: int ... 12 more fields]\nvAR_featureIndexer_part: org.apache.spark.ml.feature...\n"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "/*\nStep #8 : VectorAssembler is a transformer as it takes the input dataframe and returns the transformed dataframe\n               with a new column which is vector representation of all the features.\n               Select Features columns as \"TransactionID\", \"Quantity\", \"ActualCost\", \"CustomerID\", \"TotalDue\", \"LineTotal\", \"FinishedGoodsFlag\", \"SalesReasonID\", \"AverageRate\", \"EndOfDayRate\", \"SalesLastYear\", \"ProductID\"\n*/\n  println(\"After Repartition Data transform Started by: \" + DateTime.now(DateTimeZone.UTC))\n  val vAR_assembler_part = new VectorAssembler()\n    .setInputCols(Array(\"TransactionID\", \"Quantity\", \"ActualCost\", \"CustomerID\", \"TotalDue\", \"LineTotal\", \"FinishedGoodsFlag\", \"SalesReasonID\", \"AverageRate\", \"EndOfDayRate\", \"SalesLastYear\", \"ProductID\"))\n    .setOutputCol(\"features\")\n  val vAR_transformed_output_part = vAR_assembler_part.transform(vAR_SalesData_csv_df_part)\n  /*\n  Step #9 :  Select  Training,Test Data after repartition\n  */\n  val vAR_splitupdata_part = vAR_transformed_output_part.randomSplit(Array(0.80, 0.20))\n  val vAR_training_data_part = vAR_splitupdata_part(0)\n  val vAR_test_data_part = vAR_splitupdata_part(1)\n\nprintln(\"After Repartition Data transform Ended by: \" + DateTime.now(DateTimeZone.UTC))\n\n\n/*\n Step #10 :  Index the features Data after repartition\n*/\nprintln(\"After Repartition Data Prep started by: \" + DateTime.now(DateTimeZone.UTC))\n\n  val vAR_featureIndexer_part = new VectorIndexer()\n    .setInputCol(\"features\")\n    .setOutputCol(\"indexedFeatures\")\n    .setMaxCategories(13)\n    .fit(vAR_transformed_output)\n/*\nStep #11 :  select the Algorithm with columns indexedFeatures, MakeFlag after repartition\n*/\n  val vAR_rf_part = new RandomForestRegressor( )\n    .setLabelCol(\"MakeFlag\")\n    .setFeaturesCol(\"indexedFeatures\")\n    .setNumTrees(30)   // number of trees to run in parallel  after repartition\n\n/*  \nStep #12 :  Configure an ML pipeline, which consists of two stages after repartition \n*/\n  val vAR_pipeline_part = new Pipeline()\n    .setStages(Array(vAR_featureIndexer_part, vAR_rf_part))\n/*\nStep #13 :  setup Evaluator after repartition for testing\n*/\n  val vAR_evaluator_part = new RegressionEvaluator()\n    .setLabelCol(\"MakeFlag\")\n    .setPredictionCol(\"prediction\")\n    .setMetricName(\"rmse\")\n  println(\"After Repartition Data prep ended by: \" + DateTime.now(DateTimeZone.UTC))\n\n  "}, {"cell_type": "code", "execution_count": 9, "metadata": {"autoscroll": "auto"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "After Repartition Model Fit, Predict Started by: 2021-05-12T02:31:56.970Z\n+-------------+---------+--------+----------+----------+--------+---------+--------+-----------------+-------------+-----------+------------+-------------+--------------------+--------------------+----------+\n|TransactionID|ProductID|Quantity|ActualCost|CustomerID|TotalDue|LineTotal|MakeFlag|FinishedGoodsFlag|SalesReasonID|AverageRate|EndOfDayRate|SalesLastYear|            features|     indexedFeatures|prediction|\n+-------------+---------+--------+----------+----------+--------+---------+--------+-----------------+-------------+-----------+------------+-------------+--------------------+--------------------+----------+\n|       103869|      921|       1|      4.99|     12107| 35.6584|    24.99|       0|                1|           10|     1.5955|      1.5969|      5693989|[103869.0,1.0,4.9...|[4.0,0.0,2.0,2.0,...|       0.0|\n+-------------+---------+--------+----------+----------+--------+---------+--------+-----------------+-------------+-----------+------------+-------------+--------------------+--------------------+----------+\n\n(After Repartition Model Train completed by:,2021-05-12T02:32:00.230Z)\n"}, {"data": {"text/plain": "vAR_Model_part: org.apache.spark.ml.PipelineModel = pipeline_a0f2990afac6\nvAR_predictions_part: org.apache.spark.sql.DataFrame = [TransactionID: int, ProductID: int ... 14 more fields]\n"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "/*\nStep #14 : Train Data  after repartition\n*/\n  println(\"After Repartition Model Fit, Predict Started by: \" + DateTime.now(DateTimeZone.UTC))\n  val vAR_Model_part = vAR_pipeline_part.fit(vAR_training_data_part)\n\n/*\nStep #15 : Predict on Test Data after repartition\n*/\n  val vAR_predictions_part = vAR_Model_part.transform(vAR_test_data_part)\n  vAR_predictions_part.show()\n\n  println(\"After Repartition Model Train completed by:\", DateTime.now(DateTimeZone.UTC))\n\n"}], "metadata": {"kernelspec": {"display_name": "spylon-kernel", "language": "scala", "name": "spylon-kernel"}, "language_info": {"codemirror_mode": "text/x-scala", "file_extension": ".scala", "help_links": [{"text": "MetaKernel Magics", "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"}], "mimetype": "text/x-scala", "name": "scala", "pygments_lexer": "scala", "version": "0.4.1"}, "name": "Model_DF_v4"}, "nbformat": 4, "nbformat_minor": 4}