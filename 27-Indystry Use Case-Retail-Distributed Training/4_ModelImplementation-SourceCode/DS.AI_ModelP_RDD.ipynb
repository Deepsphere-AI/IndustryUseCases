{"cells": [{"cell_type": "code", "execution_count": 37, "metadata": {"autoscroll": "auto"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Import libraries started by 2021-05-12T17:49:15.312Z\nImport libraries ended by 2021-05-12T17:49:15.312Z\n"}, {"data": {"text/plain": "import org.joda.time.{DateTime, DateTimeZone}\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.sql.SparkSession\n"}, "execution_count": 37, "metadata": {}, "output_type": "execute_result"}], "source": "/*\nStep #1 :  Import Required Libraries\n*/\nimport org.joda.time.{DateTime, DateTimeZone}\nprintln(\"Import libraries started by \" + DateTime.now(DateTimeZone.UTC))\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.sql.SparkSession\nprintln(\"Import libraries ended by \" + DateTime.now(DateTimeZone.UTC))\n"}, {"cell_type": "code", "execution_count": 38, "metadata": {"autoscroll": "auto"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Data load Started by: 2021-05-12T17:49:15.738Z\nData load Ended by: 2021-05-12T17:49:15.792Z\nPartition applied default by system = 2\n"}, {"data": {"text/plain": "vAR_SalesData_csv_rdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[204] at map at <console>:46\n"}, "execution_count": 38, "metadata": {}, "output_type": "execute_result"}], "source": "/*\nStep #2 :  Load Training Data\n*/\nprintln(\"Data load Started by: \" + DateTime.now(DateTimeZone.UTC))\nval vAR_SalesData_csv_rdd = sc.textFile(\"gs://dssparkbucket/DS.AI_SalesData_RDD.csv\").map { line =>\n      val p = line.split(',')\n      LabeledPoint(p(6).toDouble, Vectors.dense(p(0).toDouble,p(1).toDouble,p(2).toDouble,p(3).toDouble,p(4).toDouble,p(5).toDouble,p(7).toDouble,p(8).toDouble,p(9).toDouble,p(10).toDouble,p(11).toDouble,p(12).toDouble))\n    }.cache()\nprintln(\"Data load Ended by: \" + DateTime.now(DateTimeZone.UTC))\n\nprintln(\"Partition applied default by system = \" + vAR_SalesData_csv_rdd.getNumPartitions)\n"}, {"cell_type": "code", "execution_count": 39, "metadata": {"autoscroll": "auto"}, "outputs": [{"data": {"text/plain": "vAR_training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[205] at randomSplit at <console>:46\nvAR_test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[206] at randomSplit at <console>:46\n"}, "execution_count": 39, "metadata": {}, "output_type": "execute_result"}], "source": "/* \nStep #3 :Training and testing subsets by spliting using randomSplit method at 7:3 ratio. seed makes sure the subsets are mutualy exclusive \n*/\nval Array(vAR_training, vAR_test) = vAR_txt_RDD.randomSplit(Array(0.7, 0.3), seed = 12345)\n\n"}, {"cell_type": "code", "execution_count": 40, "metadata": {"autoscroll": "auto"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Parameter Update started by = 2021-05-12T17:49:16.638Z\nParameter Update ended by = 2021-05-12T17:49:16.647Z\n"}, {"data": {"text/plain": "vAR_numClasses: Int = 3\nvAR_categoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()\nvAR_numTrees: Int = 15\nvAR_featureSubsetStrategy: String = auto\nvAR_impurity: String = variance\nvAR_maxDepth: Int = 3\nvAR_maxBins: Int = 32\n"}, "execution_count": 40, "metadata": {}, "output_type": "execute_result"}], "source": "/*\nStep #4 : Setup parameters\n*/\nprintln(\"Parameter Update started by = \" + DateTime.now(DateTimeZone.UTC))\nval vAR_numClasses = 3\nval vAR_categoricalFeaturesInfo = Map[Int, Int]()\nval vAR_numTrees = 15 // number of trees to run in parallel\nval vAR_featureSubsetStrategy = \"auto\"\nval vAR_impurity = \"variance\"\nval vAR_maxDepth = 3\nval vAR_maxBins = 32\n\nprintln(\"Parameter Update ended by = \" + DateTime.now(DateTimeZone.UTC))\n"}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Train, Test , Validate and Prediction of RandomForest Algorithm started by = 2021-05-12T17:49:16.990Z\nTest Mean Squared Error = 141.3716633454979\nTest Error = 1.0\nTrain, Test , Validate and Prediction of RandomForest Algorithm ended by = 2021-05-12T17:49:17.694Z\n"}, {"data": {"text/plain": "vAR_model: org.apache.spark.mllib.tree.model.RandomForestModel =\nTreeEnsembleModel regressor with 15 trees\n\nvAR_labelAndPreds: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[225] at map at <console>:59\nvAR_testMSE: Double = 141.3716633454979\nvAR_testErr: Double = 1.0\n"}, "execution_count": 41, "metadata": {}, "output_type": "execute_result"}], "source": "/*\n  Step #5 :  Setup Pipeline, Train , Test , Validate and Predict\n*/\nprintln(\"Train, Test , Validate and Prediction of RandomForest Algorithm started by = \" + DateTime.now(DateTimeZone.UTC))\n\nval vAR_model = RandomForest.trainRegressor(vAR_training, vAR_categoricalFeaturesInfo,\n  vAR_numTrees, vAR_featureSubsetStrategy, vAR_impurity, vAR_maxDepth, vAR_maxBins)\nval vAR_labelAndPreds = vAR_test.map { point =>\n    val vAR_prediction = vAR_model.predict(point.features)\n      (point.label, vAR_prediction)\n    }\nval vAR_testMSE = vAR_labelAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.mean()\nprintln(s\"Test Mean Squared Error = $vAR_testMSE\")\nval vAR_testErr = vAR_labelAndPreds.filter(r => r._1 != r._2).count.toDouble / vAR_test.count()\nprintln(s\"Test Error = $vAR_testErr\")\n\nprintln(\"Train, Test , Validate and Prediction of RandomForest Algorithm ended by = \" + DateTime.now(DateTimeZone.UTC))\n"}, {"cell_type": "code", "execution_count": 42, "metadata": {"autoscroll": "auto"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Apply Repartition started by= 2021-05-12T17:49:18.142Z\nRepartition count set as = 4\nApply Repartition Ended by= 2021-05-12T17:49:18.169Z\nRepartition Applied count = 4\nAfter Repartition Split the loaded data into training and test\n"}, {"data": {"text/plain": "vAR_repart_count_set: Int = 4\nvAR_SalesData_csv_rdd_part: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[232] at repartition at <console>:52\nvAR_training_part: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[233] at randomSplit at <console>:60\nvAR_test_part: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[234] at randomSplit at <console>:60\n"}, "execution_count": 42, "metadata": {}, "output_type": "execute_result"}], "source": "/*\nStep #6 : Apply Repartition count\n*/\nprintln(\"Apply Repartition started by= \" + DateTime.now(DateTimeZone.UTC))\n//Apply Repartition to compare performance with default partition\nval vAR_repart_count_set = 4\nprintln(\"Repartition count set as = \" + vAR_repart_count_set)\nval vAR_SalesData_csv_rdd_part = vAR_SalesData_csv_rdd.repartition(4)\nprintln(\"Apply Repartition Ended by= \" + DateTime.now(DateTimeZone.UTC))\nprintln(\"Repartition Applied count = \" + vAR_SalesData_csv_rdd_part.partitions.size)\n\n/*\nStep #7 :  Select  Train and Test Data\n*/\nprintln(\"After Repartition Split the loaded data into training and test\")\nval Array(vAR_training_part, vAR_test_part) = vAR_SalesData_csv_rdd_part.randomSplit(Array(0.7, 0.3), seed = 12345)\n\n"}, {"cell_type": "code", "execution_count": 43, "metadata": {"autoscroll": "auto"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "After Repartition parameter setup started by: 2021-05-12T17:49:18.432Z\nAfter Repartition parameter setup ended by: 2021-05-12T17:49:18.433Z\n"}, {"data": {"text/plain": "vAR_numClasses_part: Int = 3\nvAR_categoricalFeaturesInfo_part: scala.collection.immutable.Map[Int,Int] = Map()\nvAR_numTrees_part: Int = 30\nvAR_featureSubsetStrategy_part: String = auto\nvAR_impurity_part: String = variance\nvAR_maxDepth_part: Int = 4\nvAR_maxBins_part: Int = 32\n"}, "execution_count": 43, "metadata": {}, "output_type": "execute_result"}], "source": "/*\nStep #8 : Setup parameters\n*/\nprintln(\"After Repartition parameter setup started by: \" +DateTime.now(DateTimeZone.UTC))\n\nval vAR_numClasses_part = 3\nval vAR_categoricalFeaturesInfo_part = Map[Int, Int]()\nval vAR_numTrees_part = 30 // number of trees to run in parallel after repartition\nval vAR_featureSubsetStrategy_part = \"auto\"\nval vAR_impurity_part = \"variance\"\nval vAR_maxDepth_part = 4\nval vAR_maxBins_part = 32\n\nprintln(\"After Repartition parameter setup ended by: \" +DateTime.now(DateTimeZone.UTC))"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"data": {"text/plain": "Intitializing Scala interpreter ..."}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/plain": "Spark Web UI available at http://dsclusterbq-m:8088/proxy/application_1620850277284_0002\nSparkContext available as 'sc' (version = 3.1.1, master = yarn, app id = application_1620850277284_0002)\nSparkSession available as 'spark'\n"}, "metadata": {}, "output_type": "display_data"}, {"ename": "<console>", "evalue": "26: error: not found: value DateTime", "output_type": "error", "traceback": ["<console>:26: error: not found: value DateTime", "println(\"After Repartition Model Fit, Prediction  of RandomForest Algorithm  started by: \" + DateTime.now(DateTimeZone.UTC))", "                                                                                             ^", "<console>:26: error: not found: value DateTimeZone", "println(\"After Repartition Model Fit, Prediction  of RandomForest Algorithm  started by: \" + DateTime.now(DateTimeZone.UTC))", "                                                                                                          ^", "<console>:27: error: not found: value RandomForest", "val vAR_model_part = RandomForest.trainRegressor(vAR_training_part, vAR_categoricalFeaturesInfo_part,", "                     ^", "<console>:27: error: not found: value vAR_training_part", "val vAR_model_part = RandomForest.trainRegressor(vAR_training_part, vAR_categoricalFeaturesInfo_part,", "                                                 ^", "<console>:27: error: not found: value vAR_categoricalFeaturesInfo_part", "val vAR_model_part = RandomForest.trainRegressor(vAR_training_part, vAR_categoricalFeaturesInfo_part,", "                                                                    ^", "<console>:28: error: not found: value vAR_numTrees_part", "  vAR_numTrees_part, vAR_featureSubsetStrategy_part, vAR_impurity_part, vAR_maxDepth_part, vAR_maxBins_part)", "  ^", "<console>:28: error: not found: value vAR_featureSubsetStrategy_part", "  vAR_numTrees_part, vAR_featureSubsetStrategy_part, vAR_impurity_part, vAR_maxDepth_part, vAR_maxBins_part)", "                     ^", "<console>:28: error: not found: value vAR_impurity_part", "  vAR_numTrees_part, vAR_featureSubsetStrategy_part, vAR_impurity_part, vAR_maxDepth_part, vAR_maxBins_part)", "                                                     ^", "<console>:28: error: not found: value vAR_maxDepth_part", "  vAR_numTrees_part, vAR_featureSubsetStrategy_part, vAR_impurity_part, vAR_maxDepth_part, vAR_maxBins_part)", "                                                                        ^", "<console>:28: error: not found: value vAR_maxBins_part", "  vAR_numTrees_part, vAR_featureSubsetStrategy_part, vAR_impurity_part, vAR_maxDepth_part, vAR_maxBins_part)", "                                                                                           ^", "<console>:33: error: not found: value vAR_test_part", "val vAR_labelAndPreds_part = vAR_test_part.map { point =>", "                             ^", "<console>:45: error: not found: value DateTime", "println(\"After Repartition Model Fit, Prediction  of RandomForest Algorithm  ended by: \" + DateTime.now(DateTimeZone.UTC))", "                                                                                           ^", "<console>:45: error: not found: value DateTimeZone", "println(\"After Repartition Model Fit, Prediction  of RandomForest Algorithm  ended by: \" + DateTime.now(DateTimeZone.UTC))", "                                                                                                        ^", ""]}], "source": "/*\nStep #9 : Train Data  after repartition\n*/\nprintln(\"After Repartition Model Fit, Prediction  of RandomForest Algorithm  started by: \" + DateTime.now(DateTimeZone.UTC))\nval vAR_model_part = RandomForest.trainRegressor(vAR_training_part, vAR_categoricalFeaturesInfo_part,\n  vAR_numTrees_part, vAR_featureSubsetStrategy_part, vAR_impurity_part, vAR_maxDepth_part, vAR_maxBins_part)\n\n/*\nStep #10 : Evaluate model on test instances and compute test error\n*/\nval vAR_labelAndPreds_part = vAR_test_part.map { point =>\n    val prediction_part = vAR_model_part.predict(point.features)\n    (point.label, prediction_part)\n  }\n\nval vAR_testMSE_part = vAR_labelAndPreds_part.map{ case(v, p) => math.pow((v - p), 2)}.mean()\nprintln(s\"After Repartition, Test Mean Squared Error = $vAR_testMSE_part\")\nval vAR_testErr_part = vAR_labelAndPreds_part.filter(r => r._1 != r._2).count.toDouble / vAR_test_part.count()\nprintln(s\"After Repartition, Test Error = $vAR_testErr_part\")\n\nprintln(\"After Repartition Model Fit, Prediction  of RandomForest Algorithm  ended by: \" + DateTime.now(DateTimeZone.UTC))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "spylon-kernel", "language": "scala", "name": "spylon-kernel"}, "language_info": {"codemirror_mode": "text/x-scala", "file_extension": ".scala", "help_links": [{"text": "MetaKernel Magics", "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"}], "mimetype": "text/x-scala", "name": "scala", "pygments_lexer": "scala", "version": "0.4.1"}, "name": "spylon-kernel"}, "nbformat": 4, "nbformat_minor": 4}