{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\AI\\Resume-and-CV-Summarization-and-Parsing-with-Spacy-in-Python-master\\ML\\INI\\INI_FILE_RESUME.ini\n",
      "C:\\AI\\Resume-and-CV-Summarization-and-Parsing-with-Spacy-in-Python-master\\ML\\TRAINING DATA\\SkillDescription.csv\n",
      "C:\\AI\\Resume-and-CV-Summarization-and-Parsing-with-Spacy-in-Python-master\\ML\\TEST DATA\\\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "#                 INI File Configuration                         #\n",
    "##################################################################\n",
    "\n",
    "import configparser\n",
    "\n",
    "import os\n",
    "\n",
    "vAR_Config = configparser.ConfigParser(allow_no_value=True)\n",
    "\n",
    "vAR_INI_FILE_PATH = os.getenv('RSE')\n",
    "print(vAR_INI_FILE_PATH)\n",
    "\n",
    "vAR_Config.read(vAR_INI_FILE_PATH)\n",
    "\n",
    "vAR_Data = vAR_Config.sections()\n",
    "\n",
    "vAR_Config.sections()\n",
    "\n",
    "vAR_Data = vAR_Config['FILE PATH']['TRAIN_DATA']\n",
    "vAR_Data_CSV = vAR_Config['FILE PATH']['TRAIN_DATA_CSV']\n",
    "vAR_Test_Dir = vAR_Config['FILE PATH']['TEST_DATA_DIR']\n",
    "vAR_Test = vAR_Config['FILE PATH']['TEST_DATA']\n",
    "vAR_Model_Outcome = vAR_Config['FILE PATH']['MODEL_OUTCOME']\n",
    "vAR_Job_Title = vAR_Config['VARIABLES']['JOB_TITLE']\n",
    "print(vAR_Data_CSV)\n",
    "print(vAR_Test_Dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "#           Load the Training Data                 #\n",
    "####################################################\n",
    "vAR_train_data = pickle.load(open(vAR_Data,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vAR_training_data = vAR_train_data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "#        Create the Model and Train the Training Data     #\n",
    "###########################################################\n",
    "\n",
    "nlp = spacy.blank('en') #create a blank model\n",
    "\n",
    "def train_model(train_data):\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner') #add the entities\n",
    "        nlp.add_pipe(ner, last = True)\n",
    "        \n",
    "    for _, annotation in train_data:\n",
    "        for ent in annotation['entities']:\n",
    "            ner.add_label(ent[2])\n",
    "            \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes): #only train ner\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(10): #training the model for 10 iterations\n",
    "            print(\"Starting Iteration \"+str(itn))\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            index = 0\n",
    "            for text,annotations in train_data:\n",
    "                #print(index)\n",
    "                try:\n",
    "                    nlp.update(\n",
    "                        [text],\n",
    "                        [annotations],\n",
    "                        drop=0.2,\n",
    "                        sgd=optimizer,\n",
    "                        losses=losses)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                  #print(text)\n",
    "                    \n",
    "            print(losses)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:635: UserWarning: [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  proc.begin_training(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Hartej Kathuria Data Analyst Intern - Oracle Retai...\" with entities \"[(2246, 2573, 'Skills'), (1435, 1480, 'Email Addre...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Jitendra Babu FI/CO Consultant in Tech Mahindra - ...\" with entities \"[(5510, 5514, 'Graduation Year'), (5481, 5508, 'Co...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Ramya. P Hyderabad, Telangana - Email me on Indeed...\" with entities \"[(4542, 4549, 'Skills'), (4178, 4187, 'Skills'), (...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Navas Koya Test Engineer  Mangalore, Karnataka - E...\" with entities \"[(2110, 2404, 'Skills'), (2055, 2064, 'Location'),...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Ravi Shivgond Bidar, Karnataka - Email me on Indee...\" with entities \"[(1341, 1384, 'Email Address'), (1131, 1136, 'Loca...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Kartik Sharma Systems Engineer - Infosys Ltd  Delh...\" with entities \"[(3086, 3255, 'Skills'), (3046, 3058, 'Skills'), (...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"kimaya sonawane Thane, Maharashtra - Email me on I...\" with entities \"[(802, 806, 'Graduation Year'), (524, 661, 'Skills...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Harini Komaravelli Test Analyst at Oracle, Hyderab...\" with entities \"[(2275, 2281, 'Companies worked at'), (2235, 2241,...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Khushboo Choudhary Developer  Noida, Uttar Pradesh...\" with entities \"[(1466, 1549, 'Skills'), (1363, 1421, 'College Nam...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Ravi Shankar Working as Escalation Engineer with M...\" with entities \"[(4016, 4025, 'Companies worked at'), (3941, 3950,...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Imgeeyaul Ansari java developer  Pune, Maharashtra...\" with entities \"[(1894, 2173, 'Skills'), (1726, 1851, 'Skills'), (...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Madas Peddaiah Anantapur, Andhra Pradesh - Email m...\" with entities \"[(2934, 2938, 'Graduation Year'), (2917, 2921, 'Gr...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Jay Madhavi Navi Mumbai, Maharashtra - Email me on...\" with entities \"[(1520, 1524, 'Graduation Year'), (1507, 1518, 'De...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Jyotirbindu Patnaik Associate consultant@SAP labs ...\" with entities \"[(3052, 3067, 'Skills'), (2993, 3016, 'College Nam...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Kowsick Somasundaram Certified Network Associate T...\" with entities \"[(696, 1129, 'Skills'), (625, 661, 'College Name')...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Prakriti Shaurya Senior System Engineer - Infosys ...\" with entities \"[(1368, 1560, 'Skills'), (1096, 1133, 'Skills'), (...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Koushik Katta Devops  Hyderabad, Telangana - Email...\" with entities \"[(2957, 3074, 'Skills'), (2943, 2948, 'Graduation ...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Karthik GV Architect - Microsoft India  Hyderabad,...\" with entities \"[(4048, 4169, 'Skills'), (4034, 4038, 'Graduation ...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Lakshika Neelakshi Senior Systems Engineer - Infos...\" with entities \"[(7260, 7599, 'Skills'), (3628, 3637, 'Location'),...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Puneet Singh Associate Software Engineer  Bengalur...\" with entities \"[(989, 1007, 'Skills'), (952, 968, 'Skills'), (919...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Ramesh HP CES ASSOCIATE CONSULTANT  Bangalore, Kar...\" with entities \"[(2668, 2945, 'Skills'), (2618, 2638, 'College Nam...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Manisha Bharti Software Automation Engineer  Pune,...\" with entities \"[(2833, 4400, 'Skills'), (2819, 2823, 'Graduation ...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Mohini Gupta Server Support Engineer  Gurgaon, Har...\" with entities \"[(2326, 2333, 'Location'), (1821, 2095, 'Skills'),...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Pulkit Saxena New Delhi, Delhi - Email me on Indee...\" with entities \"[(3673, 4270, 'Skills'), (3497, 3502, 'Companies w...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Madhuri Sripathi Banglore, Karnataka, Karnataka - ...\" with entities \"[(4563, 4747, 'Skills'), (4538, 4552, 'College Nam...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Nitin Tr PeopleSoft Consultant  Bangalore Urban, K...\" with entities \"[(3511, 3749, 'Skills'), (3313, 3364, 'Skills'), (...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Navjyot Singh Rathore Ulhasnagar, Maharashtra - Em...\" with entities \"[(605, 753, 'Skills'), (403, 407, 'Graduation Year...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Rahul Bollu Software Engineer - Disney  Hyderabad,...\" with entities \"[(4093, 4190, 'Skills'), (3940, 4089, 'Skills'), (...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Mohamed Ameen System engineer  Bengaluru, Karnatak...\" with entities \"[(1632, 1636, 'Graduation Year'), (931, 1013, 'Ski...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Ijas Nizamuddin Associate Consultant - State Stree...\" with entities \"[(4652, 4850, 'Skills'), (4607, 4612, 'Graduation ...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Nidhi Pandit Test Engineer - Infosys Limited  - Em...\" with entities \"[(3132, 3611, 'Skills'), (3005, 3083, 'Skills'), (...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\anaco\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"Nikhileshkumar Ikhar Product development engineer ...\" with entities \"[(3036, 3078, 'Skills'), (2922, 3018, 'Skills'), (...\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 7032.0718545313575}\n",
      "Starting Iteration 1\n",
      "{'ner': 2752.027018873894}\n",
      "Starting Iteration 2\n",
      "{'ner': 2091.48133283827}\n",
      "Starting Iteration 3\n",
      "{'ner': 2557.8300599010704}\n",
      "Starting Iteration 4\n",
      "{'ner': 2637.003698563514}\n",
      "Starting Iteration 5\n",
      "{'ner': 2811.85712009312}\n",
      "Starting Iteration 6\n",
      "{'ner': 2231.0424583720087}\n",
      "Starting Iteration 7\n",
      "{'ner': 2491.134448603386}\n",
      "Starting Iteration 8\n",
      "{'ner': 2061.861194533721}\n",
      "Starting Iteration 9\n",
      "{'ner': 2441.961462909221}\n"
     ]
    }
   ],
   "source": [
    "train_model(vAR_training_data) #input training data into the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#    save the model   #\n",
    "#######################\n",
    "nlp.to_disk('nlp_model1')  #saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model = spacy.load('nlp_model1')  #loading our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Govardhana K Senior Software Engineer  Bengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/ b2de315d95905b68  Total IT experience 5 Years 6 Months Cloud Lending Solutions INC 4 Month • Salesforce Developer Oracle 5 Years 2 Month • Core Java Developer Languages Core Java, Go Lang Oracle PL-SQL programming, Sales Force Developer with APEX.  Designations & Promotions  Willing to relocate: Anywhere  WORK EXPERIENCE  Senior Software Engineer  Cloud Lending Solutions -  Bangalore, Karnataka -  January 2018 to Present  Present  Senior Consultant  Oracle -  Bangalore, Karnataka -  November 2016 to December 2017  Staff Consultant  Oracle -  Bangalore, Karnataka -  January 2014 to October 2016  Associate Consultant  Oracle -  Bangalore, Karnataka -  November 2012 to December 2013  EDUCATION  B.E in Computer Science Engineering  Adithya Institute of Technology -  Tamil Nadu  September 2008 to June 2012  https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN   SKILLS  APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years), Algorithms (3 years)  LINKS  https://www.linkedin.com/in/govardhana-k-61024944/  ADDITIONAL INFORMATION  Technical Proficiency:  Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle PL-SQL programming, Sales Force with APEX. Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice  Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases: Oracle Middleware: Web logic, OC4J Product FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x  https://www.linkedin.com/in/govardhana-k-61024944/',\n",
       " {'entities': [(1749, 1755, 'Companies worked at'),\n",
       "   (1696, 1702, 'Companies worked at'),\n",
       "   (1417, 1423, 'Companies worked at'),\n",
       "   (1356, 1793, 'Skills'),\n",
       "   (1209, 1215, 'Companies worked at'),\n",
       "   (1136, 1248, 'Skills'),\n",
       "   (928, 932, 'Graduation Year'),\n",
       "   (858, 889, 'College Name'),\n",
       "   (821, 856, 'Degree'),\n",
       "   (787, 791, 'Graduation Year'),\n",
       "   (744, 750, 'Companies worked at'),\n",
       "   (722, 742, 'Designation'),\n",
       "   (658, 664, 'Companies worked at'),\n",
       "   (640, 656, 'Designation'),\n",
       "   (574, 580, 'Companies worked at'),\n",
       "   (555, 573, 'Designation'),\n",
       "   (470, 493, 'Companies worked at'),\n",
       "   (444, 469, 'Designation'),\n",
       "   (308, 314, 'Companies worked at'),\n",
       "   (234, 240, 'Companies worked at'),\n",
       "   (175, 198, 'Companies worked at'),\n",
       "   (93, 137, 'Email Address'),\n",
       "   (39, 48, 'Location'),\n",
       "   (13, 38, 'Designation'),\n",
       "   (0, 12, 'Name')]})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vAR_train_data[0] #displaying the first data in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#     Importing libraries for parsing and preprocessing testing data   #\n",
    "########################################################################\n",
    "from tika import parser\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize \n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the dataset\n",
    "def loadSkillDataset():\n",
    "    skillDataset = pd.read_csv(vAR_Data_CSV)\n",
    "    frontEnd = list(skillDataset['Front_End'])\n",
    "    backEnd = list(skillDataset['Back_End'])\n",
    "    ml = list(skillDataset['Machine_Learning'])\n",
    "    ad = list(skillDataset['Android_Developer'])\n",
    "    educationLevel = list(skillDataset['Education'])\n",
    "    cleanedFrontEndList = [x for x in frontEnd if str(x) != 'nan']\n",
    "    cleanedBackEndList = [x for x in backEnd if str(x) != 'nan']\n",
    "    cleanedmlList = [x for x in ml if str(x) != 'nan']\n",
    "    cleanedadList = [x for x in ad if str(x) != 'nan']\n",
    "    cleanedEducationLevel = [x for x in educationLevel if str(x) != 'nan']\n",
    "    return cleanedFrontEndList , cleanedBackEndList , cleanedmlList , cleanedadList, cleanedEducationLevel\n",
    "\n",
    "frontEndList , backEndList , mlList , androidDevelopmentList, eLevelList = loadSkillDataset()\n",
    "\n",
    "\n",
    "#function to preprocess the testing data\n",
    "def preprocess(doc_location):\n",
    "    resumeFile = doc_location\n",
    "    resumeFileData = parser.from_file(resumeFile)\n",
    "    fileContent = resumeFileData['content']\n",
    "    obtainedResumeText = fileContent\n",
    "    refinedText = []\n",
    "    refinedResume1 = []\n",
    "    resumeFinalPhone = []\n",
    "    oneFourthOfResume = obtainedResumeText[0:len(obtainedResumeText)//4] \n",
    "    refResume = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", oneFourthOfResume)\n",
    "    finalResume = re.findall(r\"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\",oneFourthOfResume)\n",
    "    \n",
    "    if len(refResume) > 1:\n",
    "        refinedText = refResume[0]\n",
    "    else:\n",
    "        refinedText = refResume\n",
    "        \n",
    "    for i in range(len(finalResume)):\n",
    "        if len(finalResume[i])>=10:\n",
    "            refinedResume1 = finalResume[i]\n",
    "\n",
    "#Capitalize the first letter\n",
    "    firstLetterCapitalizedObtainedResumeText = []\n",
    "    capitalizingString = \" \"\n",
    "    obtainedResumeTextLowerCase = obtainedResumeText.lower()\n",
    "    obtainedResumeTextUpperCase = obtainedResumeText.upper()\n",
    "    splitListOfObtainedResumeText = obtainedResumeText.split()\n",
    "    for i in splitListOfObtainedResumeText:\n",
    "        firstLetterCapitalizedObtainedResumeText.append(i.capitalize())        \n",
    "    firstLetterCapitalizedText = capitalizingString.join(firstLetterCapitalizedObtainedResumeText)\n",
    "    obtainedResumeText = firstLetterCapitalizedText\n",
    "\n",
    "    obtainedResumeText.strip('/n')\n",
    "    newLineRemovedResumeText = obtainedResumeText    \n",
    "    resSpecification = {'Education':eLevelList}\n",
    "\n",
    "# Create an empty list where the stopwords would be stored\n",
    "    stopwordsEdu = []\n",
    "    for area in resSpecification.keys():\n",
    "        if area == 'Education':\n",
    "            stopeduWord = []\n",
    "            for word in resSpecification[area]:\n",
    "                if word in obtainedResumeText:\n",
    "                    stopeduWord.append(word)\n",
    "            stopwordsEdu.append(stopeduWord)\n",
    "\n",
    "\n",
    "    bagOfWords = stopwordsEdu\n",
    "\n",
    "    #print(bagOfWords)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(obtainedResumeText) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "\n",
    "    filtered_sentence = [] \n",
    "    joinEmptyString = \" \"\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    filteredTextForSkillExtraction = joinEmptyString.join(filtered_sentence)\n",
    "\n",
    "    resumeTechnicalSkillSpecificationList = {'Front End':frontEndList,\n",
    "\n",
    "            'Back End':backEndList, 'Machine Learning':mlList,'Android Developer':androidDevelopmentList}\n",
    "\n",
    "    frontend = 0\n",
    "    backend = 0\n",
    "    ml = 0\n",
    "    ad = 0\n",
    "\n",
    "\n",
    "    skillScores = []\n",
    "    resumeRefin = []\n",
    "\n",
    "    for area in resumeTechnicalSkillSpecificationList.keys():\n",
    "\n",
    "        if area == 'Front End':\n",
    "            frontEndWord = []\n",
    "            for word in resumeTechnicalSkillSpecificationList[area]:\n",
    "                if word in filteredTextForSkillExtraction:\n",
    "                    frontend += 1\n",
    "                    frontEndWord.append(word)\n",
    "            resumeRefin.append(frontEndWord)\n",
    "            skillScores.append(frontend)\n",
    "\n",
    "        elif area == 'Back End':\n",
    "            backEndWord = []\n",
    "            for word in resumeTechnicalSkillSpecificationList[area]:\n",
    "                if word in filteredTextForSkillExtraction:\n",
    "                    backend += 1\n",
    "                    backEndWord.append(word)\n",
    "            resumeRefin.append(backEndWord)\n",
    "            skillScores.append(backend)\n",
    "\n",
    "        elif area == 'Machine Learning':\n",
    "            mlWord = []\n",
    "            for word in resumeTechnicalSkillSpecificationList[area]:\n",
    "                if word in filteredTextForSkillExtraction:\n",
    "                    ml += 1\n",
    "                    mlWord.append(word)\n",
    "            resumeRefin.append(mlWord)\n",
    "            skillScores.append(ml)\n",
    "\n",
    "        elif area == 'Android Developer':\n",
    "            adWord = []\n",
    "            for word in resumeTechnicalSkillSpecificationList[area]:\n",
    "                if word in filteredTextForSkillExtraction:\n",
    "                    ad += 1\n",
    "                    adWord.append(word)\n",
    "            resumeRefin.append(adWord)\n",
    "            skillScores.append(ad)\n",
    "    technicalSkillScore = skillScores\n",
    "    global finresumeRefin\n",
    "    finresumeRefin = resumeRefin\n",
    "    global flattenlist\n",
    "    flattenlist = []\n",
    "    for sublist in finresumeRefin:\n",
    "        for item in sublist:\n",
    "            flattenlist.append(item)\n",
    "\n",
    "    dataList = {\"Skills\":finresumeRefin}\n",
    "    #softwareDevelopemtTechnicalSkills = pd.DataFrame(dataList,index=resumeTechnicalSkillSpecificationList.keys())\n",
    "    #print(\"Email Address:\",finalExtractedEmail,\"Phone Number:\",finalExtractedPhone,\"Academic Qualifications:\",bagOfWords,\"Skills:\",finresumeRefin)\n",
    "    import sys, fitz\n",
    "    doc1 = firstLetterCapitalizedText\n",
    "    text2 = \" \".join([\"Name:\",str(doc1.split()[0]),str(refinedText),\"Phone Number:\",str(refinedResume1),\"Academic Qualifications:\",str(bagOfWords),\"Skills\",str(finresumeRefin),str(doc1)])\n",
    "    preprocessed_text = text2.split(None, 1)[1]\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Document Name  \\\n",
      "5     ashray_anand_420580386.pdf   \n",
      "7  Asutosh_Sarangi_420580386.pdf   \n",
      "3     Asfad_Nowman_420580386.pdf   \n",
      "6             ashwiniKumarCV.pdf   \n",
      "0    Anushka Paradkar-Resume.pdf   \n",
      "1        Anushya_C_420580386.pdf   \n",
      "2   Aseem_Chaudhry_420580386.pdf   \n",
      "8    Aswath_Ramana_420580386.pdf   \n",
      "4    Ashish_Namdeo_420580386.pdf   \n",
      "\n",
      "                      Information / Skills Extracted  Score  \n",
      "5  LOCATION Oracle [['React'], ['Java', 'Python',...     26  \n",
      "7  COMPANIES WORKED AT Oracle [[], ['Python', 'Ya...     20  \n",
      "3  COMPANIES WORKED AT Microsoft [[], ['Java', 'P...     18  \n",
      "6  LOCATION Gurgaon [[], ['Java', 'Python', 'Ngin...     18  \n",
      "0  LOCATION Microsoft [[], ['Python'], ['R ', 'Py...     12  \n",
      "1  COMPANIES WORKED AT Microsoft [[], ['Python'],...     12  \n",
      "2  COMPANIES WORKED AT B.tech + [[], ['Python'], ...     12  \n",
      "8  LOCATION Chennai [[], ['Python', 'Django'], ['...     12  \n",
      "4  LOCATION Microsoft [[], ['Java', 'C++ '], ['Ja...      6  \n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#              Function for Ranking Resume                    #\n",
    "###############################################################\n",
    "\n",
    "from skillset import data_scientist,data_engineer,Cloud_engineer,AI \n",
    "def resume_ranking(title,skills):\n",
    "    if (title == 'data scientist'):\n",
    "        #data_scientist = ['Python','R','SQL','Spark','Hadoop','Tableau','scala','Tableau','SAS','Java','AWS','Tensorflow','Hive','NumPy','MatLab','NoSQL','Azure','C++','Kafka','Scikit-learn','Keras','Torch','C','SPSS','Pyspark','Linux','Pig','Pandas','Qlik','Weka','Octave','MongoDB','Mahout','Caffe','D3.js','Grafana','Spotfire','Impala','UNIX','Apache lucene','Cognos','PowerBI','EC2','Lambda','Sagemaker','DynamoDB','Microsoft azure','Mxnet','AWS Neptune','Neo4j','Gensim','Go','Elasticsearch','Cassandra']\n",
    "        skills_set = set(skills)\n",
    "        data_scientist_set = set(data_scientist)\n",
    "        common_set = skills_set & data_scientist_set\n",
    "        common = list(common_set)\n",
    "        global score\n",
    "        score = 0\n",
    "        global scores_dict\n",
    "        scores_dict = {}\n",
    "        for i in common:\n",
    "            if (i == 'python' or i == 'Python'):\n",
    "                score += 12\n",
    "            elif (i == 'R' or i == 'r'):\n",
    "                score += 11\n",
    "            elif (i == 'SQL' or i == 'sql'):\n",
    "                score += 10\n",
    "            elif (i == 'Spark' or i == 'spark'):\n",
    "                score += 9\n",
    "            elif (i == 'Hadoop' or i == 'hadoop'):\n",
    "                score += 8\n",
    "            elif (i == 'Tableau' or i == 'tableau'):\n",
    "                score += 8\n",
    "            elif (i == 'scala' or i == 'Scala'):\n",
    "                score += 7\n",
    "            elif (i == 'SAS' or i == 'sas' ):\n",
    "                score += 6\n",
    "            elif (i == 'Java' or i == 'java' ):\n",
    "                score += 6\n",
    "            elif (i == 'AWS' or i == 'aws' ):\n",
    "                score += 5\n",
    "            elif (i == 'Tenserflow' or i == 'tenserflow' ):\n",
    "                score += 5\n",
    "                \n",
    "        scores_dict = {\"Title\": title,\n",
    "                       \"Score\": score}\n",
    "\n",
    "        return scores_dict\n",
    "    \n",
    "    if (title == 'data engineer'):\n",
    "        #data_scientist = ['SQL','Python','Spark','Java','scala','Hadoop','Kafka','Hive','AWS','NoSQL','Azure','Cassandra','Airflow','Redshift','MongoDB','UNIX','Tableau','Kubernetes','Agile','Linux','Hbase','Docker','AWS Athena','PigGo','Snowflake','Shell','MapReduce','RDBMS','Apache Flink','R','C++','Lambda','Elasticsearch','scrum','C','Pyspark','Impala','EC2','Googlecloud platform','Apache beam','Perl','SAS','Pandas','Qlik','Mahout','D3.js','PowerBI','Neo4j','AWS KMS','Apache ranger','Apache atlas','ArangoDB','Arrow','Luiji','AWS Firehose','Jupyter']\n",
    "        skills_set = set(skills)\n",
    "        data_scientist_set = set(data_engineer)\n",
    "        common_set = skills_set & data_engineer_set\n",
    "        common = list(common_set)\n",
    "        #global score\n",
    "        score = len(common)\n",
    "        scores_dict = {}\n",
    "        for i in common:\n",
    "            if (i == 'SQL' or i == 'sql'):\n",
    "                score += 12\n",
    "            elif (i == 'python' or i == 'Python'):\n",
    "                score += 11\n",
    "            elif (i == 'Spark' or i == 'spark'):\n",
    "                score += 10\n",
    "            elif (i == 'Java' or i == 'java' ):\n",
    "                score += 9\n",
    "            elif (i == 'scala' or i == 'Scala'):\n",
    "                score += 8\n",
    "            elif (i == 'Hadoop' or i == 'hadoop'):\n",
    "                score += 8\n",
    "            elif (i == 'kafka' or i == 'Kafka'):\n",
    "                score += 7\n",
    "            elif (i == 'Hive' or i == 'hive' ):\n",
    "                score += 6\n",
    "            elif (i == 'Cassandra' or i == 'cassandra' ):\n",
    "                score += 6\n",
    "            elif (i == 'Airflow' or i == 'airflow' ):\n",
    "                score += 5\n",
    "        scores_dict = {\"Title\": title,\n",
    "                       \"Score\": score}\n",
    "        \n",
    "        return scores_dict\n",
    "    \n",
    "    if (title == 'Cloud engineer'):\n",
    "        #data_scientist = ['Go','Python','Azure','Linux','Java','SQL','Kubernetes','Docker','AWS RDS','scala','Googlecloud platform','Shell','AWS CLI','Terraform','CI/CD','AWS EC2','Chef','Paas','AWS VPC','UNIX','AWS Lambda','AWS S3','Vmware','.Net','NoSQL','Cassandra','Redshift','Perl','Spring','Spark','Hadoop','Tableau','Kafka','mysql','AWS Cloudwatch','AWS ELB','AWS IAM','Spring boot','Maven','Orchestration tools','Ruby','Saas','Cloud foundry','Hive','C#','C++','DynamoDB','Ruby','Amazon SNS','AWS SQS','Shell scripts','NodeJS','Gradle,Redhat' ,'SAS','PowerBI','Neo4j','Cosmos','Nifi','PHP','AWS Glacier','AWS WAF','AWS EFS','Spring MVC','Hudson','IBM blue mix','Superset','Jira','looker','Specflow','Octopusdeploy','Spring security','Tensorflow','SPSS','Pyspark','Pig','D3.js','Grafana','Spotfire','Impala','Elasticsearch','MapReduce','Snowflake','Unix shell','Cloudera','RabbitMQ','Alteryx','AWS Route53','AWS CloudFront','AWS Codebuild']\n",
    "        skills_set = set(skills)\n",
    "        data_scientist_set = set(Cloud_engineer)\n",
    "        common_set = skills_set & Cloud_engineer_set\n",
    "        common = list(common_set)\n",
    "        #global score\n",
    "        score = len(common)\n",
    "        #global scores_dict\n",
    "        scores_dict = {}\n",
    "        for i in common:\n",
    "            if (i == 'Go' or i == 'go'):\n",
    "                score += 12\n",
    "            elif (i == 'python' or i == 'Python'):\n",
    "                score += 11\n",
    "            elif (i == 'Linux' or i == 'linux'):\n",
    "                score += 10\n",
    "            elif (i == 'Java' or i == 'java' ):\n",
    "                score += 9\n",
    "            elif (i == 'sql' or i == 'SQL'):\n",
    "                score += 8\n",
    "            elif (i == 'Kubernetes' or i == 'kubernetes'):\n",
    "                score += 8\n",
    "            elif (i == 'Docker' or i == 'docker'):\n",
    "                score += 7\n",
    "            elif (i == 'AWS RDS' or i == 'aws rds' ):\n",
    "                score += 6\n",
    "            elif (i == 'scala' or i == 'Scala' ):\n",
    "                score += 6\n",
    "            elif (i == 'Googlecloud platform' or i == 'googlecloud platform' ):\n",
    "                score += 5\n",
    "            elif (i == 'Shell' or i == 'shell' ):\n",
    "                score += 4\n",
    "            elif (i == 'AWS CLI' or i == 'aws cli' ):\n",
    "                score += 4\n",
    "        scores_dict = {\"Title\": title,\n",
    "                       \"Score\": score}\n",
    "        return scores_dict\n",
    "    \n",
    "    if (title == 'AI'):\n",
    "        #data_scientist = ['Python','Tensorflow','Java','Spark','Pytorch','Keras','Torch','SQL','Scikit-learn','Hadoop','R','scala','C++','Pandas','Azure','Docker','Googlecloud platform','Kafka','Kubernetes','NumPy','NoSQL','C#','Hive','Linux','Caffe','AWS Sagemaker','Go','Agile','Cassandra','Tableau','Airflow','Shell','CI/CD','MatLab','AWS EC2','Neo4j','Jupyter','RabbitMQ','Salesforce','Ruby','mysql,','SAS','Pyspark','Qlik','Mahout','D3.js','Grafana','Spotfire','Impala','UNIX','AWS Lambda','DynamoDB','Mxnet','MapReduce','Hbase','scrum','Redshift','Perl','kaldi','Sphinx','Word2Vec','Sentence2Vec','Wav2Vec','PHP','Perforce','Dataiku','PowerBI']\n",
    "        skills_set = set(skills)\n",
    "        data_scientist_set = set(AI)\n",
    "        common_set = skills_set & AI_set\n",
    "        common = list(common_set)\n",
    "         #global score\n",
    "        score = len(common)\n",
    "        #global scores_dict\n",
    "        scores_dict = {}\n",
    "        for i in common:\n",
    "            if (i == 'python' or i == 'Python'):\n",
    "                score += 12\n",
    "            elif (i == 'Tensorflow' or i == 'tensorflow'):\n",
    "                score += 11\n",
    "            elif (i == 'Java' or i == 'java'):\n",
    "                score += 10\n",
    "            elif (i == 'Spark' or i == 'spark'):\n",
    "                score += 9\n",
    "            elif (i == 'Pytorch' or i == 'pytorch'):\n",
    "                score += 8\n",
    "            elif (i == 'Keras' or i == 'keras'):\n",
    "                score += 8\n",
    "            elif (i == 'Torch' or i == 'torch'):\n",
    "                score += 7\n",
    "            elif (i == 'SQL' or i == 'sql' ):\n",
    "                score += 6\n",
    "            elif (i == 'Scikit-learn' or i == 'scikit-learn' ):\n",
    "                score += 6\n",
    "            elif (i == 'Hadoop' or i == 'Hadoop' ):\n",
    "                score += 5\n",
    "            elif (i == 'R' or i == 'r' ):\n",
    "                score += 5\n",
    "            elif (i == 'scala' or i == 'Scala'):\n",
    "                score += 4\n",
    "            elif (i == 'C++' or i == 'c++'):\n",
    "                score += 4\n",
    "        \n",
    "        scores_dict = {\"Title\": title,\n",
    "                       \"Score\": score}\n",
    "        return scores_dict \n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.width', 1000)\n",
    "#pd.set_option('display.height', 1000)\n",
    "\n",
    "#############################################################\n",
    "#      Input the resumes from folder into the model         #\n",
    "#############################################################\n",
    "\n",
    "# Creating empty lists to store the extracted entites from model and output the same into the Dataframe\n",
    "Extracted_skills = []\n",
    "scoreresult = []\n",
    "scores_list = []\n",
    "pdf_directory = vAR_Test_Dir\n",
    "entries = os.listdir(vAR_Test) #getting the names of files from the specified folder\n",
    "resumes_text = []\n",
    "doc_name_list = []\n",
    "scores_list = []\n",
    "for entry in entries: #iterating through the names of files in the folder\n",
    "    #print(entry)\n",
    "    pdf_name = entry\n",
    "    doc_name_list.append(entry)\n",
    "    pdf_location = pdf_directory + pdf_name\n",
    "    text = preprocess(pdf_location)\n",
    "    skillnames = flattenlist\n",
    "    resumes_text.append(text)\n",
    "    doc = nlp_model(text)\n",
    "    title = vAR_Job_Title\n",
    "    scores_dict = resume_ranking(title,flattenlist)\n",
    "    scores_list.append(score)\n",
    "    for ent in doc.ents: \n",
    "        #print(f'{ent.label_.upper():{30}- {ent.text}})\n",
    "        final_output = \" \".join([ent.label_.upper(),ent.text,str(finresumeRefin)])\n",
    "    Extracted_skills.append(final_output)\n",
    "#Create a dataframe with the lists created\n",
    "df = pd.DataFrame(list(zip(doc_name_list,Extracted_skills,scores_list)),columns = ['Document Name','Information / Skills Extracted','Score'])\n",
    "df.sort_values(by=['Score'], inplace=True, ascending=False)\n",
    "print(df)\n",
    "#print(set(data_scientist))\n",
    "df.to_excel(vAR_Model_Outcome, index = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
