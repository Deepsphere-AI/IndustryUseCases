{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6df2f5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 09:35:03.083 Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "2022-02-24 09:35:03.212 Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "2022-02-24 09:35:03.245 Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "2022-02-24 09:35:05.968 instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "2022-02-24 09:35:05.976 instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "2022-02-24 09:35:05.983 instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "2022-02-24 09:35:05.990 instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "2022-02-24 09:35:29.180 loading archive file https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz from cache at C:\\Users\\ds_008\\.allennlp\\cache\\60c14844468543e4329ce7e8d3444fa1f9f7057b4b0de5b3f4a597eb57113d32.73aa20bab6336a582588814d8458d040b59536ca1f60b6a769a2da61c7aa3c9a\n",
      "2022-02-24 09:35:29.187 extracting archive file C:\\Users\\ds_008\\.allennlp\\cache\\60c14844468543e4329ce7e8d3444fa1f9f7057b4b0de5b3f4a597eb57113d32.73aa20bab6336a582588814d8458d040b59536ca1f60b6a769a2da61c7aa3c9a to temp dir C:\\Users\\ds_008\\AppData\\Local\\Temp\\tmpotdyz382\n",
      "2022-02-24 09:35:55.224 _jsonnet not loaded, treating C:\\Users\\ds_008\\AppData\\Local\\Temp\\tmpotdyz382\\config.json as json\n",
      "2022-02-24 09:35:55.230 _jsonnet not loaded, treating snippet as json\n",
      "2022-02-24 09:35:55.234 instantiating registered subclass constituency_parser of <class 'allennlp.models.model.Model'>\n",
      "2022-02-24 09:35:55.236 type = default\n",
      "2022-02-24 09:35:55.238 instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
      "2022-02-24 09:35:55.240 Loading token dictionary from C:\\Users\\ds_008\\AppData\\Local\\Temp\\tmpotdyz382\\vocabulary.\n",
      "2022-02-24 09:35:55.251 instantiating class <class 'allennlp.models.model.Model'> from params {'type': 'constituency_parser', 'pos_tag_embedding': {'embedding_dim': 50, 'vocab_namespace': 'pos'}, 'initializer': [['tag_projection_layer.*weight', {'type': 'xavier_normal'}], ['feedforward_layer.*weight', {'type': 'xavier_normal'}], ['encoder._module.weight_ih.*', {'type': 'xavier_normal'}], ['encoder._module.weight_hh.*', {'type': 'orthogonal'}]], 'span_extractor': {'type': 'bidirectional_endpoint', 'input_dim': 500}, 'encoder': {'type': 'lstm', 'input_size': 1074, 'hidden_size': 250, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}, 'feedforward': {'input_dim': 500, 'num_layers': 1, 'hidden_dims': 250, 'activations': 'relu', 'dropout': 0.1}, 'evalb_directory_path': 'scripts/EVALB', 'text_field_embedder': {'elmo': {'dropout': 0.2, 'do_layer_norm': False, 'type': 'elmo_token_embedder', 'options_file': 'C:\\\\Users\\\\ds_008\\\\AppData\\\\Local\\\\Temp\\\\tmpotdyz382\\\\fta/model.text_field_embedder.elmo.options_file', 'weight_file': 'C:\\\\Users\\\\ds_008\\\\AppData\\\\Local\\\\Temp\\\\tmpotdyz382\\\\fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab'}\n",
      "2022-02-24 09:35:55.253 model.type = constituency_parser\n",
      "2022-02-24 09:35:55.255 instantiating class <class 'allennlp.models.constituency_parser.SpanConstituencyParser'> from params {'pos_tag_embedding': {'embedding_dim': 50, 'vocab_namespace': 'pos'}, 'initializer': [['tag_projection_layer.*weight', {'type': 'xavier_normal'}], ['feedforward_layer.*weight', {'type': 'xavier_normal'}], ['encoder._module.weight_ih.*', {'type': 'xavier_normal'}], ['encoder._module.weight_hh.*', {'type': 'orthogonal'}]], 'span_extractor': {'type': 'bidirectional_endpoint', 'input_dim': 500}, 'encoder': {'type': 'lstm', 'input_size': 1074, 'hidden_size': 250, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2}, 'feedforward': {'input_dim': 500, 'num_layers': 1, 'hidden_dims': 250, 'activations': 'relu', 'dropout': 0.1}, 'evalb_directory_path': 'scripts/EVALB', 'text_field_embedder': {'elmo': {'dropout': 0.2, 'do_layer_norm': False, 'type': 'elmo_token_embedder', 'options_file': 'C:\\\\Users\\\\ds_008\\\\AppData\\\\Local\\\\Temp\\\\tmpotdyz382\\\\fta/model.text_field_embedder.elmo.options_file', 'weight_file': 'C:\\\\Users\\\\ds_008\\\\AppData\\\\Local\\\\Temp\\\\tmpotdyz382\\\\fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab'}\n",
      "2022-02-24 09:35:55.258 instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'elmo': {'dropout': 0.2, 'do_layer_norm': False, 'type': 'elmo_token_embedder', 'options_file': 'C:\\\\Users\\\\ds_008\\\\AppData\\\\Local\\\\Temp\\\\tmpotdyz382\\\\fta/model.text_field_embedder.elmo.options_file', 'weight_file': 'C:\\\\Users\\\\ds_008\\\\AppData\\\\Local\\\\Temp\\\\tmpotdyz382\\\\fta/model.text_field_embedder.elmo.weight_file'}} and extras {'vocab'}\n",
      "2022-02-24 09:35:55.260 model.text_field_embedder.type = basic\n",
      "2022-02-24 09:35:55.261 model.text_field_embedder.embedder_to_indexer_map = None\n",
      "2022-02-24 09:35:55.263 model.text_field_embedder.allow_unmatched_keys = False\n",
      "2022-02-24 09:35:55.264 model.text_field_embedder.token_embedders = None\n",
      "2022-02-24 09:35:55.266 instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'dropout': 0.2, 'do_layer_norm': False, 'type': 'elmo_token_embedder', 'options_file': 'C:\\\\Users\\\\ds_008\\\\AppData\\\\Local\\\\Temp\\\\tmpotdyz382\\\\fta/model.text_field_embedder.elmo.options_file', 'weight_file': 'C:\\\\Users\\\\ds_008\\\\AppData\\\\Local\\\\Temp\\\\tmpotdyz382\\\\fta/model.text_field_embedder.elmo.weight_file'} and extras {'vocab'}\n",
      "2022-02-24 09:35:55.267 model.text_field_embedder.elmo.type = elmo_token_embedder\n",
      "2022-02-24 09:35:55.272 model.text_field_embedder.elmo.options_file = C:\\Users\\ds_008\\AppData\\Local\\Temp\\tmpotdyz382\\fta/model.text_field_embedder.elmo.options_file\n",
      "2022-02-24 09:35:55.275 model.text_field_embedder.elmo.weight_file = C:\\Users\\ds_008\\AppData\\Local\\Temp\\tmpotdyz382\\fta/model.text_field_embedder.elmo.weight_file\n",
      "2022-02-24 09:35:55.277 model.text_field_embedder.elmo.requires_grad = False\n",
      "2022-02-24 09:35:55.278 model.text_field_embedder.elmo.do_layer_norm = False\n",
      "2022-02-24 09:35:55.280 model.text_field_embedder.elmo.dropout = 0.2\n",
      "2022-02-24 09:35:55.282 model.text_field_embedder.elmo.namespace_to_cache = None\n",
      "2022-02-24 09:35:55.285 model.text_field_embedder.elmo.projection_dim = None\n",
      "2022-02-24 09:35:55.286 model.text_field_embedder.elmo.scalar_mix_parameters = None\n",
      "2022-02-24 09:35:55.288 Initializing ELMo\n",
      "2022-02-24 09:36:08.874 instantiating class <class 'allennlp.modules.span_extractors.span_extractor.SpanExtractor'> from params {'type': 'bidirectional_endpoint', 'input_dim': 500} and extras {'vocab'}\n",
      "2022-02-24 09:36:08.878 model.span_extractor.type = bidirectional_endpoint\n",
      "2022-02-24 09:36:08.879 instantiating class <class 'allennlp.modules.span_extractors.bidirectional_endpoint_span_extractor.BidirectionalEndpointSpanExtractor'> from params {'input_dim': 500} and extras {'vocab'}\n",
      "2022-02-24 09:36:08.880 model.span_extractor.input_dim = 500\n",
      "2022-02-24 09:36:08.882 model.span_extractor.forward_combination = y-x\n",
      "2022-02-24 09:36:08.883 model.span_extractor.backward_combination = x-y\n",
      "2022-02-24 09:36:08.885 model.span_extractor.num_width_embeddings = None\n",
      "2022-02-24 09:36:08.886 model.span_extractor.span_width_embedding_dim = None\n",
      "2022-02-24 09:36:08.890 model.span_extractor.bucket_widths = False\n",
      "2022-02-24 09:36:08.891 model.span_extractor.use_sentinels = True\n",
      "2022-02-24 09:36:08.898 instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'type': 'lstm', 'input_size': 1074, 'hidden_size': 250, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2} and extras {'vocab'}\n",
      "2022-02-24 09:36:08.900 model.encoder.type = lstm\n",
      "2022-02-24 09:36:08.901 model.encoder.batch_first = True\n",
      "2022-02-24 09:36:08.902 model.encoder.stateful = False\n",
      "2022-02-24 09:36:08.903 Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2022-02-24 09:36:08.905 CURRENTLY DEFINED PARAMETERS: \n",
      "2022-02-24 09:36:08.905 model.encoder.input_size = 1074\n",
      "2022-02-24 09:36:08.907 model.encoder.hidden_size = 250\n",
      "2022-02-24 09:36:08.908 model.encoder.num_layers = 2\n",
      "2022-02-24 09:36:08.909 model.encoder.bidirectional = True\n",
      "2022-02-24 09:36:08.910 model.encoder.dropout = 0.2\n",
      "2022-02-24 09:36:08.911 model.encoder.batch_first = True\n",
      "2022-02-24 09:36:08.999 instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'input_dim': 500, 'num_layers': 1, 'hidden_dims': 250, 'activations': 'relu', 'dropout': 0.1} and extras {'vocab'}\n",
      "2022-02-24 09:36:09.002 model.feedforward.input_dim = 500\n",
      "2022-02-24 09:36:09.004 model.feedforward.num_layers = 1\n",
      "2022-02-24 09:36:09.006 model.feedforward.hidden_dims = 250\n",
      "2022-02-24 09:36:09.007 model.feedforward.activations = relu\n",
      "2022-02-24 09:36:09.009 instantiating registered subclass relu of <class 'allennlp.nn.activations.Activation'>\n",
      "2022-02-24 09:36:09.011 model.feedforward.dropout = 0.1\n",
      "2022-02-24 09:36:09.016 model.pos_tag_embedding.num_embeddings = None\n",
      "2022-02-24 09:36:09.019 model.pos_tag_embedding.vocab_namespace = pos\n",
      "2022-02-24 09:36:09.021 model.pos_tag_embedding.embedding_dim = 50\n",
      "2022-02-24 09:36:09.023 model.pos_tag_embedding.pretrained_file = None\n",
      "2022-02-24 09:36:09.025 model.pos_tag_embedding.projection_dim = None\n",
      "2022-02-24 09:36:09.026 model.pos_tag_embedding.trainable = True\n",
      "2022-02-24 09:36:09.028 model.pos_tag_embedding.padding_index = None\n",
      "2022-02-24 09:36:09.030 model.pos_tag_embedding.max_norm = None\n",
      "2022-02-24 09:36:09.031 model.pos_tag_embedding.norm_type = 2.0\n",
      "2022-02-24 09:36:09.032 model.pos_tag_embedding.scale_grad_by_freq = False\n",
      "2022-02-24 09:36:09.035 model.pos_tag_embedding.sparse = False\n",
      "2022-02-24 09:36:09.039 instantiating class <class 'allennlp.nn.initializers.Initializer'> from params {'type': 'xavier_normal'} and extras set()\n",
      "2022-02-24 09:36:09.040 model.initializer.0.1.type = xavier_normal\n",
      "2022-02-24 09:36:09.042 Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2022-02-24 09:36:09.044 CURRENTLY DEFINED PARAMETERS: \n",
      "2022-02-24 09:36:09.046 instantiating class <class 'allennlp.nn.initializers.Initializer'> from params {'type': 'xavier_normal'} and extras set()\n",
      "2022-02-24 09:36:09.049 model.initializer.1.1.type = xavier_normal\n",
      "2022-02-24 09:36:09.051 Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2022-02-24 09:36:09.053 CURRENTLY DEFINED PARAMETERS: \n",
      "2022-02-24 09:36:09.054 instantiating class <class 'allennlp.nn.initializers.Initializer'> from params {'type': 'xavier_normal'} and extras set()\n",
      "2022-02-24 09:36:09.055 model.initializer.2.1.type = xavier_normal\n",
      "2022-02-24 09:36:09.056 Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2022-02-24 09:36:09.057 CURRENTLY DEFINED PARAMETERS: \n",
      "2022-02-24 09:36:09.058 instantiating class <class 'allennlp.nn.initializers.Initializer'> from params {'type': 'orthogonal'} and extras set()\n",
      "2022-02-24 09:36:09.059 model.initializer.3.1.type = orthogonal\n",
      "2022-02-24 09:36:09.060 Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2022-02-24 09:36:09.062 CURRENTLY DEFINED PARAMETERS: \n",
      "2022-02-24 09:36:09.064 model.evalb_directory_path = scripts/EVALB\n",
      "2022-02-24 09:36:09.066 Initializing parameters\n",
      "2022-02-24 09:36:09.072 Initializing encoder._module.weight_ih_l0 using encoder._module.weight_ih.* initializer\n",
      "2022-02-24 09:36:09.106 Initializing encoder._module.weight_hh_l0 using encoder._module.weight_hh.* initializer\n",
      "2022-02-24 09:36:09.140 Initializing encoder._module.weight_ih_l0_reverse using encoder._module.weight_ih.* initializer\n",
      "2022-02-24 09:36:09.160 Initializing encoder._module.weight_hh_l0_reverse using encoder._module.weight_hh.* initializer\n",
      "2022-02-24 09:36:09.188 Initializing encoder._module.weight_ih_l1 using encoder._module.weight_ih.* initializer\n",
      "2022-02-24 09:36:09.203 Initializing encoder._module.weight_hh_l1 using encoder._module.weight_hh.* initializer\n",
      "2022-02-24 09:36:09.234 Initializing encoder._module.weight_ih_l1_reverse using encoder._module.weight_ih.* initializer\n",
      "2022-02-24 09:36:09.248 Initializing encoder._module.weight_hh_l1_reverse using encoder._module.weight_hh.* initializer\n",
      "2022-02-24 09:36:09.276 Initializing feedforward_layer._module._linear_layers.0.weight using feedforward_layer.*weight initializer\n",
      "2022-02-24 09:36:09.281 Initializing tag_projection_layer._module.weight using tag_projection_layer.*weight initializer\n",
      "2022-02-24 09:36:09.284 Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2022-02-24 09:36:09.286    encoder._module.bias_hh_l0\n",
      "2022-02-24 09:36:09.287    encoder._module.bias_hh_l0_reverse\n",
      "2022-02-24 09:36:09.289    encoder._module.bias_hh_l1\n",
      "2022-02-24 09:36:09.291    encoder._module.bias_hh_l1_reverse\n",
      "2022-02-24 09:36:09.292    encoder._module.bias_ih_l0\n",
      "2022-02-24 09:36:09.293    encoder._module.bias_ih_l0_reverse\n",
      "2022-02-24 09:36:09.294    encoder._module.bias_ih_l1\n",
      "2022-02-24 09:36:09.296    encoder._module.bias_ih_l1_reverse\n",
      "2022-02-24 09:36:09.296    feedforward_layer._module._linear_layers.0.bias\n",
      "2022-02-24 09:36:09.298    pos_tag_embedding.weight\n",
      "2022-02-24 09:36:09.299    span_extractor._end_sentinel\n",
      "2022-02-24 09:36:09.301    span_extractor._start_sentinel\n",
      "2022-02-24 09:36:09.302    tag_projection_layer._module.bias\n",
      "2022-02-24 09:36:09.304    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.input_linearity.weight\n",
      "2022-02-24 09:36:09.305    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.bias\n",
      "2022-02-24 09:36:09.306    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.weight\n",
      "2022-02-24 09:36:09.310    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_projection.weight\n",
      "2022-02-24 09:36:09.311    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.input_linearity.weight\n",
      "2022-02-24 09:36:09.312    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.bias\n",
      "2022-02-24 09:36:09.313    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.weight\n",
      "2022-02-24 09:36:09.314    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_projection.weight\n",
      "2022-02-24 09:36:09.315    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.input_linearity.weight\n",
      "2022-02-24 09:36:09.317    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.bias\n",
      "2022-02-24 09:36:09.318    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.weight\n",
      "2022-02-24 09:36:09.320    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_projection.weight\n",
      "2022-02-24 09:36:09.321    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.input_linearity.weight\n",
      "2022-02-24 09:36:09.323    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.bias\n",
      "2022-02-24 09:36:09.324    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.weight\n",
      "2022-02-24 09:36:09.325    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_projection.weight\n",
      "2022-02-24 09:36:09.326    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._char_embedding_weights\n",
      "2022-02-24 09:36:09.327    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.bias\n",
      "2022-02-24 09:36:09.329    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.weight\n",
      "2022-02-24 09:36:09.330    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.bias\n",
      "2022-02-24 09:36:09.331    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.weight\n",
      "2022-02-24 09:36:09.333    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.bias\n",
      "2022-02-24 09:36:09.333    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.weight\n",
      "2022-02-24 09:36:09.335    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.bias\n",
      "2022-02-24 09:36:09.336    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.weight\n",
      "2022-02-24 09:36:09.337    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.bias\n",
      "2022-02-24 09:36:09.339    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.weight\n",
      "2022-02-24 09:36:09.340    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.bias\n",
      "2022-02-24 09:36:09.342    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.weight\n",
      "2022-02-24 09:36:09.343    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.bias\n",
      "2022-02-24 09:36:09.345    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.weight\n",
      "2022-02-24 09:36:09.346    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.bias\n",
      "2022-02-24 09:36:09.348    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.weight\n",
      "2022-02-24 09:36:09.350    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.bias\n",
      "2022-02-24 09:36:09.351    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.weight\n",
      "2022-02-24 09:36:09.352    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.bias\n",
      "2022-02-24 09:36:09.353    text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.weight\n",
      "2022-02-24 09:36:09.354    text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma\n",
      "2022-02-24 09:36:09.355    text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0\n",
      "2022-02-24 09:36:09.356    text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1\n",
      "2022-02-24 09:36:09.358    text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2\n",
      "2022-02-24 09:36:10.313 instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'ptb_trees', 'use_pos_tags': True, 'token_indexers': {'elmo': {'type': 'elmo_characters'}}} and extras set()\n",
      "2022-02-24 09:36:10.314 dataset_reader.type = ptb_trees\n",
      "2022-02-24 09:36:10.315 instantiating class <class 'allennlp.data.dataset_readers.penn_tree_bank.PennTreeBankConstituencySpanDatasetReader'> from params {'use_pos_tags': True, 'token_indexers': {'elmo': {'type': 'elmo_characters'}}} and extras set()\n",
      "2022-02-24 09:36:10.315 instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'type': 'elmo_characters'} and extras set()\n",
      "2022-02-24 09:36:10.317 dataset_reader.token_indexers.elmo.type = elmo_characters\n",
      "2022-02-24 09:36:10.318 instantiating class <class 'allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer'> from params {} and extras set()\n",
      "2022-02-24 09:36:10.318 dataset_reader.token_indexers.elmo.namespace = elmo_characters\n",
      "2022-02-24 09:36:10.319 dataset_reader.token_indexers.elmo.tokens_to_add = None\n",
      "2022-02-24 09:36:10.320 dataset_reader.token_indexers.elmo.token_min_padding_length = 0\n",
      "2022-02-24 09:36:10.321 dataset_reader.use_pos_tags = True\n",
      "2022-02-24 09:36:10.322 dataset_reader.lazy = False\n",
      "2022-02-24 09:36:10.323 dataset_reader.label_namespace_prefix = \n",
      "2022-02-24 09:36:10.324 dataset_reader.pos_label_namespace = pos\n",
      "2022-02-24 09:36:10.325 instantiating registered subclass constituency-parser of <class 'allennlp.predictors.predictor.Predictor'>\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import re\n",
    "import heapq\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "# from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "import nltk\n",
    "from fuzzywuzzy import fuzz\n",
    "import streamlit  as st\n",
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\")\n",
    "\n",
    "\n",
    "# Tokenizing sentence using nltk sent_tokenize\n",
    "\n",
    "def tokenize_sentences_tf(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
    "    return sentences[0]\n",
    "\n",
    "def get_context_tf(quest,unit,grade,title):\n",
    "    return {\n",
    "      \"unit\": unit,\n",
    "      \"grade\" : grade,\n",
    "      \"title\" : title,\n",
    "      \"type\" : \"True Or False\",\n",
    "      \"quest\": quest,\n",
    "      \"length\":len(quest),\n",
    "\n",
    "  }\n",
    "\n",
    "# Method returns parts of speech tree for given sentence\n",
    "\n",
    "def pos_tree_from_sentence(text):\n",
    "    sentence = tokenize_sentences_tf(text)\n",
    "    sentence = sentence.rstrip('?:!.,;')\n",
    "    parser_output = predictor.predict(sentence=sentence)\n",
    "    tree_string = parser_output[\"trees\"]\n",
    "    tree = Tree.fromstring(tree_string)\n",
    "    return tree\n",
    "\n",
    "\n",
    "# split at right most nounphrase or verbphrase\n",
    "\n",
    "def get_flattened(t):\n",
    "    sent_str_final = None\n",
    "    if t is not None:\n",
    "        sent_str = [\" \".join(x.leaves()) for x in list(t)]\n",
    "        sent_str_final = [\" \".join(sent_str)]\n",
    "        sent_str_final = sent_str_final[0]\n",
    "    return sent_str_final\n",
    "\n",
    "\n",
    "def get_right_most_VP_or_NP(parse_tree,last_NP = None,last_VP = None):\n",
    "    if len(parse_tree.leaves()) == 1:\n",
    "        return last_NP,last_VP\n",
    "    last_subtree = parse_tree[-1]\n",
    "    if last_subtree.label() == \"NP\":\n",
    "        last_NP = last_subtree\n",
    "    elif last_subtree.label() == \"VP\":\n",
    "        last_VP = last_subtree\n",
    "    \n",
    "    return get_right_most_VP_or_NP(last_subtree,last_NP,last_VP)\n",
    "\n",
    "def get_termination_portion(main_string, sub_string):\n",
    "    combined_sub_string = sub_string.replace(\" \", \"\")\n",
    "    main_string_list = main_string.split()\n",
    "    last_index = len(main_string_list)\n",
    "    for i in range(last_index):\n",
    "        check_string_list = main_string_list[i:]\n",
    "        check_string = \"\".join(check_string_list)\n",
    "        check_string = check_string.replace(\" \", \"\")\n",
    "        if check_string == combined_sub_string:\n",
    "            return \" \".join(main_string_list[:i])\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_np_vp(tree,sentence):\n",
    "    last_nounphrase, last_verbphrase =  get_right_most_VP_or_NP(tree)\n",
    "    last_nounphrase_flattened = get_flattened(last_nounphrase)\n",
    "    last_verbphrase_flattened = get_flattened(last_verbphrase)\n",
    "    longest_phrase_to_use = ''\n",
    "    if last_nounphrase is not None and last_verbphrase is not None:\n",
    "        longest_phrase_to_use = max(last_nounphrase_flattened, last_verbphrase_flattened)      \n",
    "    elif last_nounphrase is not None:\n",
    "        longest_phrase_to_use = last_nounphrase_flattened      \n",
    "    elif last_verbphrase is not None:\n",
    "        longest_phrase_to_use = last_verbphrase_flattened        \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    longest_phrase_to_use = re.sub(r\"-LRB- \", \"(\", longest_phrase_to_use)\n",
    "    longest_phrase_to_use = re.sub(r\" -RRB-\", \")\", longest_phrase_to_use)\n",
    "    sentence = sentence.rstrip('?:!.,;')\n",
    "    split_sentence = get_termination_portion(sentence, longest_phrase_to_use)\n",
    "    return split_sentence\n",
    "\n",
    "def summarize_text(article_text):\n",
    "    # Removing Square Brackets and Extra Spaces\n",
    "    vAR_article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "    vAR_article_text = re.sub(r'\\s+', ' ', vAR_article_text)\n",
    "    # Removing special characters and digits\n",
    "    vAR_formatted_article_text = re.sub('[^a-zA-Z]', ' ', vAR_article_text )\n",
    "    vAR_formatted_article_text = re.sub(r'\\s+', ' ', vAR_formatted_article_text)\n",
    "    # Converting Text To Sentences\n",
    "    vAR_sentence_list = nltk.sent_tokenize(article_text)\n",
    "    vAR_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    # Find Weighted Frequency of Occurrence\n",
    "    vAR_word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(vAR_formatted_article_text):\n",
    "        if word not in vAR_stopwords:\n",
    "            if word not in vAR_word_frequencies.keys():\n",
    "                vAR_word_frequencies[word] = 1\n",
    "            else:\n",
    "                vAR_word_frequencies[word] += 1\n",
    "    if len(vAR_word_frequencies) >0:\n",
    "        maximum_frequncy = max(vAR_word_frequencies.values())\n",
    "    else:\n",
    "        maximum_frequncy = 1\n",
    "\n",
    "    for word in vAR_word_frequencies.keys():\n",
    "        vAR_word_frequencies[word] = (vAR_word_frequencies[word]/maximum_frequncy)\n",
    "    # Calculating Sentence Scores\n",
    "    vAR_sentence_scores = {}\n",
    "    for sent in vAR_sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in vAR_word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in vAR_sentence_scores.keys():\n",
    "                        vAR_sentence_scores[sent] = vAR_word_frequencies[word]\n",
    "                    else:\n",
    "                        vAR_sentence_scores[sent] += vAR_word_frequencies[word]\n",
    "    vAR_summary_sentences = heapq.nlargest(15, vAR_sentence_scores, key=vAR_sentence_scores.get)\n",
    "    vAR_summary = ' '.join(vAR_summary_sentences)\n",
    "    return vAR_summary\n",
    "\n",
    "def fuzzy_dup_remove(sentences):\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        if i<len(sentences)-1:\n",
    "            score = fuzz.WRatio(sentences[i],sentences[i+1])\n",
    "            if score > 90:\n",
    "                sentences.remove(sentences[i+1])\n",
    "    return sentences\n",
    "\n",
    "# @st.cache(show_spinner=False)\n",
    "def alternate_sentences(sentences):\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "#    print('Before GPT2Tokenizer -',current_time)\n",
    "    # GPT2tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # GPT2model = AutoModelWithLMHead.from_pretrained(\"gpt2\",pad_token_id=GPT2tokenizer.eos_token_id)\n",
    "    GPT2tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    GPT2model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=GPT2tokenizer.eos_token_id)\n",
    "    #Below is for GPU seetings\n",
    "    # GPT2tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "    # GPT2model = TFGPT2LMHeadModel.from_pretrained(\"distilgpt2\",pad_token_id=GPT2tokenizer.eos_token_id)\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "#    print('After GPT2Tokenizer -',current_time)\n",
    "    alt_sent_list = []\n",
    "    generated_sentences=[]\n",
    "    print(len(sentences))\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        if i <10:\n",
    "            pos = pos_tree_from_sentence(sentence)\n",
    "            # alt_sentence = alternate_sentences(pos,sentence)\n",
    "            # alt_sent_list.append(alt_sentence)\n",
    "            # flat_list = [item for sublist in alt_sent_list for item in sublist]\n",
    "#            print('sentence - ',sentence)\n",
    "            partial_sentence = get_np_vp(pos,sentence)\n",
    "            if  partial_sentence is not None:\n",
    "#                print('partial_sentence - ',partial_sentence)\n",
    "                input_ids = GPT2tokenizer.encode(partial_sentence,return_tensors='tf')\n",
    "                maximum_length = len(partial_sentence.split())+40\n",
    "                # Activate top_k sampling and top_p sampling with only from 90% most likely words\n",
    "#                print('type of inputid - ',type(input_ids))\n",
    "#                print('inputids - ',input_ids)\n",
    "#                print('pad_token_id - ',GPT2tokenizer.eos_token_id)\n",
    "                sample_outputs = GPT2model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    do_sample=True,\n",
    "                    max_length=maximum_length, \n",
    "                    top_p=1.0, # 0.85 \n",
    "                    top_k=30,   #30\n",
    "                    repetition_penalty  = 1.2,num_return_sequences=1)\n",
    "#                print('sample - ',sample_outputs)\n",
    "                sentence = sentence.replace(\"\\n\",\"\")\n",
    "#                print('############## - ',sentence)\n",
    "                for i, sample_output in enumerate(sample_outputs):\n",
    "                    decoded_sentence = GPT2tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "                    # final_sentence = decoded_sentence\n",
    "                    final_sentence = tokenize.sent_tokenize(decoded_sentence)[0]\n",
    "                    final_sentence = final_sentence.replace(\"\\r\\n\",\"\")\n",
    "                    final_sentence = final_sentence.replace(\"\\n\",\"\")\n",
    "                    final_sentence = final_sentence.replace(\"\\r\",\"\")\n",
    "                    generated_sentences.append(final_sentence)\n",
    "                sentence = sentence.replace(\"\\r\",\"\")\n",
    "                generated_sentences.append(sentence)\n",
    "#    print('$$$$$$$$$$$$$$$$ - ',generated_sentences)\n",
    "    # generated_sentences = sorted(set(generated_sentences))\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95668bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Topic Here: Introduction to Machine Learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2022-02-24 15:52:03.006 \n",
      "\n",
      "====== WebDriver manager ======\n",
      "2022-02-24 15:52:03.015 ====== WebDriver manager ======\n",
      "Current google-chrome version is 98.0.4758\n",
      "2022-02-24 15:52:06.838 Current google-chrome version is 98.0.4758\n",
      "Get LATEST chromedriver version for 98.0.4758 google-chrome\n",
      "2022-02-24 15:52:06.844 Get LATEST chromedriver version for 98.0.4758 google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/98.0.4758.102/chromedriver_win32.zip\n",
      "2022-02-24 15:52:07.061 Trying to download new driver from https://chromedriver.storage.googleapis.com/98.0.4758.102/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\ds_008\\.wdm\\drivers\\chromedriver\\win32\\98.0.4758.102]\n",
      "2022-02-24 15:52:09.215 Driver has been saved in cache [C:\\Users\\ds_008\\.wdm\\drivers\\chromedriver\\win32\\98.0.4758.102]\n",
      "C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:164: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:167: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:168: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:169: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:430: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "\n",
      "\n",
      "2022-02-24 15:52:31.864 \n",
      "\n",
      "====== WebDriver manager ======\n",
      "2022-02-24 15:52:31.867 ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the top URLs to extract content:\n",
      "URL #1 - https://www.digitalocean.com/community/tutorials/an-introduction-to-machine-learning#:~:text=Machine%20learning%20is%20a%20subfield%20of%20artificial%20intelligence%20(AI).&text=Because%20of%20this%2C%20machine%20learning,has%20benefitted%20from%20machine%20learning.\n",
      "URL #2 - https://www.geeksforgeeks.org/introduction-machine-learning/\n",
      "URL #3 - https://towardsdatascience.com/introduction-to-machine-learning-for-beginners-eed6024fdb08\n",
      "URL #4 - https://developers.google.com/machine-learning/crash-course/ml-intro\n",
      "URL #5 - https://www.edureka.co/blog/introduction-to-machine-learning/\n",
      "URL #6 - https://www.javatpoint.com/machine-learning\n",
      "URL #7 - https://www.udacity.com/course/intro-to-machine-learning--ud120\n",
      "URL #8 - https://www.simplilearn.com/tutorials/machine-learning-tutorial/introduction-to-machine-learning\n",
      "URL #9 - https://www.softwaretestinghelp.com/machine-learning-tutorials/\n",
      "URL #10 - https://www.coursera.org/learn/machine-learning-duke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current google-chrome version is 98.0.4758\n",
      "2022-02-24 15:52:34.409 Current google-chrome version is 98.0.4758\n",
      "Get LATEST chromedriver version for 98.0.4758 google-chrome\n",
      "2022-02-24 15:52:34.413 Get LATEST chromedriver version for 98.0.4758 google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/98.0.4758.102/chromedriver_win32.zip\n",
      "2022-02-24 15:52:34.542 Trying to download new driver from https://chromedriver.storage.googleapis.com/98.0.4758.102/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\ds_008\\.wdm\\drivers\\chromedriver\\win32\\98.0.4758.102]\n",
      "2022-02-24 15:52:35.606 Driver has been saved in cache [C:\\Users\\ds_008\\.wdm\\drivers\\chromedriver\\win32\\98.0.4758.102]\n",
      "\n",
      "\n",
      "2022-02-24 15:52:52.123 \n",
      "\n",
      "====== WebDriver manager ======\n",
      "2022-02-24 15:52:52.128 ====== WebDriver manager ======\n",
      "Current google-chrome version is 98.0.4758\n",
      "2022-02-24 15:52:54.456 Current google-chrome version is 98.0.4758\n",
      "Get LATEST chromedriver version for 98.0.4758 google-chrome\n",
      "2022-02-24 15:52:54.459 Get LATEST chromedriver version for 98.0.4758 google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/98.0.4758.102/chromedriver_win32.zip\n",
      "2022-02-24 15:52:54.602 Trying to download new driver from https://chromedriver.storage.googleapis.com/98.0.4758.102/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\ds_008\\.wdm\\drivers\\chromedriver\\win32\\98.0.4758.102]\n",
      "2022-02-24 15:52:55.597 Driver has been saved in cache [C:\\Users\\ds_008\\.wdm\\drivers\\chromedriver\\win32\\98.0.4758.102]\n",
      "C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:465: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "2022-02-24 15:51:50\n",
      "2022-02-24 16:07:12\n",
      "15.366666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"After learning the initial steps of Reinforcement Learning, we'll see how to use them in some cases.\",\n",
       " \"After learning the initial  steps of Reinforcement Learning, we'll move to Q Learning, as well as Deep Q Learning.\",\n",
       " 'The below block diagram explains the working of Machine Learning algorithm:The need for machine learning is a necessity in all programming applications and also can help us to make our data more accessible.',\n",
       " 'The below block diagram explains the working of Machine Learning algorithm:The need for machine learning is increasing day by day.',\n",
       " \"If it's less than 0.5, you won’t be able to.In this section of the introduction to machine learning tutorial, we will discuss some amazing use cases of using a model-learned AI (AI) for various cognitive functions - tasks like classification and decision making, data analysis etc.\",\n",
       " \"If it's less than 0.5, you won’t be able to.In this section of the introduction to machine learning tutorial, we will discuss some amazing use cases of machine learning.\",\n",
       " 'Machine learning uses various algorithms for building mathematical models and predictions by using data, statistics or computer programming languages.',\n",
       " 'Machine learning uses various algorithms for building mathematical models and making predictions using historical data or information.',\n",
       " 'As an aspirant, you can enroll in Simplilearn’s Machine Learning Certification Course to take advanced skills including:[1] https://simpl.io/learn-to--mathematical_data%2F (If this is',\n",
       " 'As an aspirant, you can enroll in Simplilearn’s Machine Learning Certification Course to receive training from industry experts or self-paced learning.',\n",
       " \"Machine learning is being developed and the company's software for machine Learning algorithms has already begun to build a prototype.\",\n",
       " 'Machine learning is what powers that recommendation system.Machine learning also plays a role in customer segmentation, a crucial aspect of business success for all e-commerce platforms.',\n",
       " 'With a little more practice, you will become adept at applying the supervised, unsupervised and reinforcement machine learning methods to your own work-flows.',\n",
       " 'With a little more practice, you will become adept at applying the supervised, unsupervised and reinforcement machine learning methods to make predictions with different types of data.',\n",
       " \"Similarly, you can think of supervised learning as a type of Machine Learning that uses the following types and functions:In an example from this post I'll say it again.\",\n",
       " 'Similarly, you can think of supervised learning as a type of Machine Learning that involves a guide.',\n",
       " 'The labeled data set is nothing but the training data set.Think of unsupervised learning as a smart kid that learns from his environment to learn what it does best, by going beyond some \"best-practice\" and looking further down in history towards higher dimensions like geography or astronomy:You don',\n",
       " 'The labeled data set is nothing but the training data set.Think of unsupervised learning as a smart kid that learns without any guidance.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "starttime = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "import streamlit as st\n",
    "import streamlit.components as stc\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import base64\n",
    "\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "#import trafilatura\n",
    "import altair as alt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import docx\n",
    "from docx import Document\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from docx.shared import Inches, Cm\n",
    "from docx.shared import RGBColor\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.enum.text import WD_UNDERLINE\n",
    "from docx.enum.table import WD_ALIGN_VERTICAL\n",
    "from docx.oxml import OxmlElement, ns\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from docx.enum.text import WD_LINE_SPACING\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import mock\n",
    "\n",
    "from docx.oxml.ns import qn\n",
    "from docx.oxml import OxmlElement\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import comtypes.client\n",
    "\n",
    "from rake_nltk import Metric, Rake\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from summa import summarizer\n",
    "\n",
    "import random\n",
    "\n",
    "#import os\n",
    "#import easyocr\n",
    "#import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "#import numpy as np\n",
    "\n",
    "from collections import Iterable\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "#import gensim\n",
    "#from gensim.summarization import summarize\n",
    "\n",
    "from lsa_summarizer import LsaSummarizer\n",
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def img_to_bytes(img_path):\n",
    "    img_bytes = Path(img_path).read_bytes()\n",
    "    encoded = base64.b64encode(img_bytes).decode()\n",
    "    return encoded\n",
    "\n",
    "header_html = \"<img src='data:image/png;base64,{}' class='img-fluid'>\".format(\n",
    "    img_to_bytes(\"C:\\\\Users\\\\ds_008\\\\Downloads\\\\DSLogo.png\")\n",
    ")\n",
    "st.markdown(\n",
    "    header_html, unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "\n",
    "st.title(\"Content Creation for the Given Topic using **_Web Scraping_** and **_NLP_**\")\n",
    "st.sidebar.title(\"Content Creation for the Given Topic using Web Scraping and NLP\")\n",
    "#st.markdown(\"This application is to extract URLs and text content related to the given topic:\")\n",
    "st.markdown(\"***\")\n",
    "st.sidebar.markdown(\"This application is to extract URLs and text content for the given topic\")\n",
    "\n",
    "def get_source(url):\n",
    "    \"\"\"Return the source code for the provided URL. \n",
    "\n",
    "    Args: \n",
    "        url (string): URL of the page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        response (object): HTTP response object from requests_html. \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "        return response\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        \n",
    "\n",
    "url_filter = [\"youtube\", \"pdf\", \"pptx\", \"docx\", \"ashx\", \"quora\", \"stackoverflow\", \"facebook\", \"stackexchange\", \"researchgate\",\n",
    "             \"www.researchgate.net\", \"https://arxiv\", \"arxiv.org\", \"https://wasp-sweden\", \n",
    "              \"wasp-sweden.org\", \"indiascienceandtechnology\", \"www.indiascienceandtechnology.gov.in\", \"https://u2b\", \n",
    "              \"u2b.com\", \"https://cis-india\", \"cis-india.org\",\n",
    "             \"shapingtomorrow\", \"www.shapingtomorrow.com\", \"frontiersin\", \"www.frontiersin.org\", \"sciedupress\", \"www.sciedupress.com\", \"http://proceedings\", \"proceedings.mlr.press\", \"aurecongroup\", \"www.aurecongroup.com\",\n",
    "             \"clootrack.com\", \"https://clootrack\", \"offshore-technology\", \"birlasoft\", \"www.birlasoft.com\", \"informatec\",\n",
    "             \"rm.coe.int\", \"www.ifc.org\", \"readcube\", \"consumersinternational\", \"www.consumersinternational.org\", \"mdpi\", \"www.mdpi.com\", \"www.theconsumergoodsforum.com\",\n",
    "             \"shapingtomorrow\", \"www.shapingtomorrow.com\"]\n",
    "def Extract_URLs_New(Topic):\n",
    "    query = Topic\n",
    "#    custom_path = 'C:/Users/Darcey/Documents/NEW_PYTHON'\n",
    "#    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "#    driver_location = \"C:\\\\Users\\\\Darcey\\\\Documents\\\\NEW_PYTHON\\\\chromedriver.exe\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--lang=en,en_US')\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    options.add_argument('--log-level=3')\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    # options.add_argument('--disable-gpu')\n",
    "    # options.add_argument('--no-sandbox')\n",
    "    options.add_argument('Accept=text/html,application/xhtml+xml,application/xml;q=0.9,image/webp')\n",
    "    # options.add_argument('Accept-Encoding= gzip')\n",
    "    # options.add_argument('Accept-Language= en-US,en;q=0.9,es;q=0.8')\n",
    "    # options.add_argument('Upgrade-Insecure-Requests: 1')\n",
    "    # options.add_argument('image/apng,*/*;q=0.8,application/signed-exchange;v=b3')\n",
    "    # options.add_argument('user-agent=' + ua['google chrome'])\n",
    "    # options.add_argument('proxy-server=' + \"115.42.65.14:8080\")\n",
    "    # options.add_argument('Referer=' + \"https://www.google.com/\")\n",
    "    options.add_argument('--headless')\n",
    "#    options.add_argument(\"--disable-extensions\")\n",
    "#    driver = webdriver.Chrome(executable_path=driver_location,options=options)\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "    driver.get(\"https://www.google.com/search?q={}&oq={}&hl=en&num=11\".format(urllib.parse.quote(query),urllib.parse.quote(query)))\n",
    "    p = driver.find_elements_by_class_name(\"tF2Cxc\")\n",
    "    titles = driver.find_elements_by_class_name(\"yuRUbf\")\n",
    "    descriptions = driver.find_elements_by_class_name(\"IsZvec\")\n",
    "    time.sleep(10)\n",
    "\n",
    "    link_list = []\n",
    "    description_list = []\n",
    "    featured = False\n",
    "    featured_links = 0\n",
    "    title_list = []\n",
    "    featured_max = 0\n",
    "    featured_num = 0\n",
    "\n",
    "    df1 = pd.DataFrame(columns=[\"URLs\"])\n",
    "    for index in range(len(p)):\n",
    "        p_items = p[index].get_attribute(\"innerHTML\")\n",
    "    #    print(p_items)\n",
    "        items_soup = BeautifulSoup(p_items,\"html.parser\")\n",
    "        if(featured==False):\n",
    "            if((len(items_soup.text.split(\"\\n\")) != 2)):\n",
    "#                print(items_soup.text.split(\"\\n\"))\n",
    "    #            df = df.append({'A': items_soup.text.split(\"\\n\")\n",
    "    #            df[\"B\"] = items_soup.text.split(\"\\n\")[1]\n",
    "    #            if ((items_soup.select(\".IsZvec\") != None)\n",
    "    #                  and (items_soup.select(\".IsZvec\")[0].text != \"\") and (items_soup.select(\".IsZvec\") != \"\")):\n",
    "                a = items_soup.select(\"a\",recursive=False)[0][\"href\"]\n",
    "                if not any(y in a.lower().split(\".\") for y in url_filter) or not any(z in a.lower().split(\"/\") for z in url_filter):\n",
    "#                    st.write(a)\n",
    "                    df1 = df1.append({'URLs': a}, ignore_index = True)\n",
    "#                    print(df1)\n",
    "                    link_list.append(a)\n",
    "        title_list.append(titles[index].text)\n",
    "        description_list.append(\"descriptions[index].text\")\n",
    "    description_list_new = []\n",
    "    title_list_new = []\n",
    "    for index in range(len(description_list)):\n",
    "    #    if (description_list[index] == \"\"):\n",
    "    #        pass\n",
    "    #    elif (re.findall(r'<\\w{1,}\\s\\w{1,}>',description_list[index]) != []):\n",
    "    #        pass\n",
    "    #    else:\n",
    "        description_list_new.append(description_list[index])\n",
    "        title_list_new.append(title_list[index])\n",
    "    description_list = description_list_new\n",
    "    title_list = title_list_new\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"Title\", \"Description\"])\n",
    "    i=0\n",
    "    for title in range(len(title_list)):\n",
    "#        print(title_list[title])\n",
    "#        print(description_list[title])\n",
    "#        print(\"=======================\")\n",
    "        df = df.append({'Title': title_list[title], 'Description': description_list[title]}, ignore_index = True)\n",
    "    #    df.loc[i].B = description_list[title]\n",
    "        i+=1\n",
    "\n",
    "    #print(link_list)\n",
    "    #print(len(title_list))\n",
    "    #print(len(link_list))\n",
    "\n",
    "    #for x in link_list:\n",
    "    #    print(x)\n",
    "    df2 = pd.concat([df, df1], axis=1)\n",
    "#    st.write(df2)\n",
    "    return df2\n",
    "\n",
    "##################### To add page number in the footer ###############################################\n",
    "\n",
    "def create_element(name):\n",
    "    return OxmlElement(name)\n",
    "\n",
    "\n",
    "def create_attribute(element, name, value):\n",
    "    element.set(ns.qn(name), value)\n",
    "\n",
    "\n",
    "def add_page_number(paragraph):\n",
    "    paragraph.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
    "\n",
    "    page_run = paragraph.add_run()\n",
    "    t1 = create_element('w:t')\n",
    "    create_attribute(t1, 'xml:space', 'preserve')\n",
    "    t1.text = 'Page '\n",
    "    page_run._r.append(t1)\n",
    "\n",
    "    page_num_run = paragraph.add_run()\n",
    "\n",
    "    fldChar1 = create_element('w:fldChar')\n",
    "    create_attribute(fldChar1, 'w:fldCharType', 'begin')\n",
    "\n",
    "    instrText = create_element('w:instrText')\n",
    "    create_attribute(instrText, 'xml:space', 'preserve')\n",
    "    instrText.text = \"PAGE\"\n",
    "\n",
    "    fldChar2 = create_element('w:fldChar')\n",
    "    create_attribute(fldChar2, 'w:fldCharType', 'end')\n",
    "\n",
    "    page_num_run._r.append(fldChar1)\n",
    "    page_num_run._r.append(instrText)\n",
    "    page_num_run._r.append(fldChar2)\n",
    "\n",
    "    of_run = paragraph.add_run()\n",
    "    t2 = create_element('w:t')\n",
    "    create_attribute(t2, 'xml:space', 'preserve')\n",
    "    t2.text = ' of '\n",
    "    of_run._r.append(t2)\n",
    "\n",
    "    fldChar3 = create_element('w:fldChar')\n",
    "    create_attribute(fldChar3, 'w:fldCharType', 'begin')\n",
    "\n",
    "    instrText2 = create_element('w:instrText')\n",
    "    create_attribute(instrText2, 'xml:space', 'preserve')\n",
    "    instrText2.text = \"NUMPAGES\"\n",
    "\n",
    "    fldChar4 = create_element('w:fldChar')\n",
    "    create_attribute(fldChar4, 'w:fldCharType', 'end')\n",
    "\n",
    "    num_pages_run = paragraph.add_run()\n",
    "    num_pages_run._r.append(fldChar3)\n",
    "    num_pages_run._r.append(instrText2)\n",
    "    num_pages_run._r.append(fldChar4)\n",
    "    \n",
    "##################### To add page number in the footer ###############################################\n",
    "\n",
    "def remove_control_characters(s):\n",
    "    return \"\".join(ch for ch in s if unicodedata.category(ch)[0]!=\"C\")\n",
    "\n",
    "wdFormatPDF = 17\n",
    "def convertFiletoPDF(file):\n",
    "    in_file = os.path.abspath(file)\n",
    "    out_file = os.path.abspath(file.replace(\".docx\", \".pdf\"))\n",
    "    word = comtypes.client.CreateObject('Word.Application')\n",
    "    word.Visible = True\n",
    "    time.sleep(3)\n",
    "    doc = word.Documents.Open(in_file)\n",
    "    doc.SaveAs(out_file, FileFormat=wdFormatPDF)\n",
    "    doc.Close()\n",
    "    word.Quit()\n",
    "\n",
    "\n",
    "def processed_text(doc, Topic, file):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_words = [\"definition\",\"what\", \"define\", \"explain\", \"mean\", \"detail\", \"short\", \"note\"]\n",
    "    unwanted_words = [\"course\", \"courses\", \"certification\", \"certifications\", \"professional\", \"professionals\", \n",
    "                      \"salary\", \"salaries\", \"week\", \"weeks\", \"accenture\", \"ibm\"]\n",
    "    stop_words = list(stop_words.union(new_words))\n",
    "    document = Document(file)\n",
    "    lkk=[]\n",
    "    if any(x in Topic.lower().split() for x in new_words):\n",
    "#        print(\"Present\")\n",
    "        test1 = [word for word in Topic.lower().split() if word not in stop_words]\n",
    "        test2 = \" \".join(test1)\n",
    "#        st.write(test2)\n",
    "        ind = [i for i, para in enumerate(document.paragraphs) if test2 in para.text.lower()]\n",
    "#        print(\"============================\", test2, \"=================================\")\n",
    "#        print(len(ind))\n",
    "        if ind:\n",
    "            for i, para in enumerate(document.paragraphs):\n",
    "    #            print(para.text.lower())\n",
    "                if len(para.text.split(' '))>15:\n",
    "                    if not any(y in para.text.lower().split(\" \") for y in unwanted_words):\n",
    "                        for k in range(len(ind)):\n",
    "                            if i == ind[k]:\n",
    "                                lkk.append(para.text)\n",
    "#                                st.write(para.text)\n",
    "                            lkk1 = [re.sub(\"\\[.*?\\]\",\"\", _) for _ in lkk]\n",
    "\n",
    "                            if len(lkk) ==3:\n",
    "                                break\n",
    "#            for k in lkk1:\n",
    "#                st.write(k)\n",
    "#            st.write(\"Raw Text\", \" \".join(lkk1))\n",
    "#            st.write(\"Summarized Text:\", summarizer.summarize(\" \".join(lkk1), ratio=0.4))\n",
    "    else:\n",
    "#        print(\"Not Present\")\n",
    "        def iter_headings(paragraphs):\n",
    "            for paragraph in paragraphs:\n",
    "                if paragraph.style.name.startswith('Heading'):\n",
    "                    yield paragraph\n",
    "        heads = []\n",
    "        for heading in iter_headings(doc.paragraphs):\n",
    "            if not any(z in heading.text.lower().split(\" \") for z in unwanted_words):\n",
    "        #    print(heading.text)\n",
    "                heads.append(heading.text)\n",
    "#        st.write(heads)\n",
    "        \n",
    "        lst1 = [re.sub(\"\\[.*?\\]\",\"\", _) for _ in heads]\n",
    "        lst2 = [re.sub('[^a-zA-Z0-9\\s]+', '', _) for _ in lst1]\n",
    "\n",
    "        strlst = \",\".join(lst2)\n",
    "        #d(w) Degree of word only.\n",
    "        r = Rake(ranking_metric=Metric.WORD_DEGREE, min_length=2, max_length=2) # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "\n",
    "        r.extract_keywords_from_text(strlst)\n",
    "\n",
    "        li = r.get_ranked_phrases() # To get keyword phrases ranked highest to lowest.\n",
    "\n",
    "        li1 = li[:10]\n",
    "#        st.write(li1)\n",
    "\n",
    "#        file = \"Model Output - \" + str(Topic)+\".docx\"\n",
    "\n",
    "        document = Document(file)\n",
    "\n",
    "        lkk = []\n",
    "        for x in li1:\n",
    "\n",
    "            # Find the index of the text and store it\n",
    "            ind = [i for i, para in enumerate(document.paragraphs) if x in para.text.lower()]\n",
    "#            print(\"============================\", x, \"=================================\")\n",
    "#            print(len(ind))\n",
    "            if ind:\n",
    "                for i, para in enumerate(document.paragraphs):\n",
    "                    if len(para.text.split(' '))>15:\n",
    "                        if not any(y in para.text.lower().split(\" \") for y in unwanted_words):\n",
    "                            for k in range(len(ind)):\n",
    "                                if i == ind[k]:\n",
    "                                    lkk.append(para.text)\n",
    "    #                                st.write(para.text) \n",
    "                                lkk1 = [re.sub(\"\\[.*?\\]\",\"\", _) for _ in lkk]\n",
    "\n",
    "def img_chk(img1):\n",
    "    IMAGE_PATH = img1\n",
    "\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    result = reader.readtext(IMAGE_PATH,paragraph=\"False\")\n",
    "    return result\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def clean_string(text):\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    text = text.lower()\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "unit2 = [\"Introductoin to Machine Learning\"]\n",
    "\n",
    "summarizer = LsaSummarizer()\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "summarizer.stop_words = stopwords\n",
    "#course_name = input(\"Enter the Course Name Here:\")\n",
    "\n",
    "#for x in tqdm(unit2):\n",
    "\n",
    "\n",
    "Topic = input(\"Enter the Topic Here: \")\n",
    "new_words = [\"definition\",\"what\", \"define\", \"explain\", \"mean\", \"detail\", \"short\", \"note\"]\n",
    "unwanted_words = [\"course\", \"courses\", \"certification\", \"certifications\", \"professional\", \"professionals\", \"salary\", \n",
    "                  \"salaries\", \"week\", \"weeks\", \"accenture\", \"ibm\", \"post\", \"posts\", \"contribute\", \"contributed\", \n",
    "                  \"remember\", \"career\", \"careers\", \"hiring\", \"recruitment\", \"blogs\", \"twitter\", \"resources\", \n",
    "                  \"community\", \"coursera\", \"glassdoor\", \"udacity\", \"frequently\", \"offered\", \"phd\", \"faq\", \"faqs\", \n",
    "                  \"reply\", \"article\", \"articles\", \"thank\", \"thanks\", \"about\", \"conclusion\", \"conclusions\", \"newsletter\", \n",
    "                 \"newsletters\", \"subscribe\", \"partner\", \"copyright\", \"copyrights\", \"journal\", \"journals\", \"contribution\",\n",
    "                 \"contributions\"]\n",
    "\n",
    "i=1\n",
    "#if len(Topic)>0:\n",
    "#    if st.sidebar.button(\"Extract URLs for the given topic\"):\n",
    "#        with st.spinner(\"Extracting...\"):\n",
    "df2 = Extract_URLs_New(Topic)\n",
    "df2_filter = df2[(df2['Title']!= \"\")]# | (df2['URLs']!= np.nan)]\n",
    "df2_filter = df2_filter[(df2['URLs'].notnull())].copy()\n",
    "df2_filter['url_rank'] = np.arange(len(df2_filter)) + 1\n",
    "df3 = df2_filter[['url_rank', 'URLs']]\n",
    "df3['URLRank_URLS'] = \"URL #\" + df3['url_rank'].astype(str) + \" - \" + df3['URLs']\n",
    "#            clean_links = Extract_Ranked_urls(links)\n",
    "print(\"Below are the top URLs to extract content:\")\n",
    "\n",
    "for x in df3['URLRank_URLS']:\n",
    "    print(x)\n",
    "#    st.sidebar.markdown(\"*******************************\")\n",
    "#    if st.sidebar.button(\"Download Contents from URLs\"):\n",
    "#        with st.spinner(\"Downloading...\"):\n",
    "df2 = Extract_URLs_New(Topic)\n",
    "df2_filter = df2[df2['Title']!= \"\"]\n",
    "df2_filter = df2_filter[(df2['URLs'].notnull())].copy()\n",
    "#            clean_links = Extract_Ranked_urls(links)\n",
    "\n",
    "############ Converting listof urls to dataframe and then to tuple to create the table in word document##########\n",
    "\n",
    "#            df = pd.DataFrame(clean_links, columns = ['urls'])\n",
    "df2_filter['url_rank'] = np.arange(len(df2_filter)) + 1\n",
    "df3 = df2_filter[['url_rank', 'URLs']]\n",
    "Search_for_These_values = ['youtube','.pdf','.pptx'] \n",
    "pattern = '|'.join(Search_for_These_values)\n",
    "df4 = df3.loc[~df3['URLs'].str.contains(pattern, case=False)]\n",
    "#            print (df1)\n",
    "datat = tuple(df3.itertuples(index=False, name=None))\n",
    "\n",
    "\n",
    "#            print(datat)\n",
    "doccc1 = docx.Document()\n",
    "#docc1.add_page_break()\n",
    "#            text_split = []\n",
    "df2 = Extract_URLs_New(Topic)\n",
    "df2_filter = df2[df2['Title']!= \"\"]\n",
    "df2_filter = df2_filter[(df2['URLs'].notnull())].copy()\n",
    "#            clean_links = Extract_Ranked_urls(links)\n",
    "ua = UserAgent()\n",
    "i=1\n",
    "#            urlheads = []\n",
    "list3 =[]\n",
    "for url in df2_filter['URLs']:\n",
    "    urlheads = []\n",
    "    with mock.patch.object(requests.cookies.RequestsCookieJar, 'update', lambda *args, **kwargs: 0):\n",
    "        req = Request(url , headers={'User-Agent': ua.random})\n",
    "        time.sleep(5)\n",
    "        req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)')\n",
    "        try:\n",
    "\n",
    "            webpage = urlopen(req).read()\n",
    "\n",
    "\n",
    "        except:\n",
    "            try:\n",
    "                chrome_options2 = Options()\n",
    "                chrome_options2.add_argument(\"--headless\")\n",
    "                chrome_options2.add_argument('--log-level=3')\n",
    "                chrome_options2.add_argument(\"--disable-gpu\")\n",
    "                chrome_options2.add_argument(\"--disable-notifications\")\n",
    "                chrome_options2.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "                driver2 = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options = chrome_options2)\n",
    "                driver2.get(url)\n",
    "                # this is just to ensure that the page is loaded\n",
    "                time.sleep(5) \n",
    "                html = driver2.page_source\n",
    "                soup2 = BeautifulSoup(html, \"html.parser\")\n",
    "                heading_tags = [\"h1\", \"h2\", \"h3\", \"h4\"]\n",
    "                list2 = []\n",
    "                list22=[]\n",
    "                z=1\n",
    "                for tags in soup2.find_all([heading_tags, 'p']):\n",
    "                    if not tags.find([heading_tags, 'p']):\n",
    "                        try:\n",
    "                            list1 = tags.name + ' -> ' + tags.text.strip()\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                page_soup = soup(webpage, \"html.parser\")\n",
    "                time.sleep(5)\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                heading_tags = [\"h1\", \"h2\", \"h3\", \"h4\"]\n",
    "                list2 = []\n",
    "#                        z=1\n",
    "                for tags in page_soup.find_all([heading_tags, 'p']):\n",
    "                    if not tags.find([heading_tags, 'p']):\n",
    "                        try:\n",
    "                            list1 = tags.name + ' -> ' + tags.text.strip()\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        list2.append(list1)\n",
    "\n",
    "\n",
    "    list3.append(\"url -> \" + url)\n",
    "    list3.append(list2)\n",
    "\n",
    "\n",
    "#doccc1 = docx.Document()\n",
    "\n",
    "\n",
    "##################### To add Title Page ###############################################    \n",
    "\n",
    "doccc1.add_paragraph(\"\")\n",
    "doccc1.add_paragraph(\"\")\n",
    "doccc1.add_paragraph(\"\")\n",
    "doccc1.add_paragraph(\"\")\n",
    "doccc1.add_paragraph(\"\")\n",
    "doccc1.add_paragraph(\"\")\n",
    "doccc1.add_paragraph(\"\")\n",
    "doccc1.add_paragraph(\"\")\n",
    "doccc1.add_paragraph(\"\")\n",
    "p=doccc1.add_paragraph()\n",
    "p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "r=p.add_run(Topic)\n",
    "r.font.size = Pt(24)\n",
    "r.bold = True\n",
    "r.underline = True\n",
    "r.font.color.rgb = RGBColor(0, 0, 153)\n",
    "doccc1.add_page_break()\n",
    "\n",
    "##################### To add table of contents - 3rd Document ###############################################\n",
    "\n",
    "\n",
    "\n",
    "paragraph = doccc1.add_paragraph()\n",
    "toc = paragraph.add_run(\"\\t\\t\\t\\t Table of Contents \\t\")\n",
    "toc.bold = True\n",
    "toc.font.size = Pt(15)\n",
    "toc.font.color.rgb = RGBColor(0, 0, 153)\n",
    "run = paragraph.add_run()\n",
    "fldChar = OxmlElement('w:fldChar')  # creates a new element\n",
    "fldChar.set(qn('w:fldCharType'), 'begin')  # sets attribute on element\n",
    "instrText = OxmlElement('w:instrText')\n",
    "instrText.set(qn('xml:space'), 'preserve')  # sets attribute on element\n",
    "instrText.text = 'TOC \\\\o \"1-2\" \\\\h \\\\z \\\\u'   # change 1-3 depending on heading levels you need\n",
    "\n",
    "fldChar2 = OxmlElement('w:fldChar')\n",
    "fldChar2.set(qn('w:fldCharType'), 'separate')\n",
    "fldChar3 = OxmlElement('w:updateFields')\n",
    "fldChar3.set(qn('w:val'), 'true')\n",
    "#fldChar3.text = \"Right-click to update field.\"\n",
    "fldChar2.append(fldChar3)\n",
    "\n",
    "fldChar4 = OxmlElement('w:fldChar')\n",
    "fldChar4.set(qn('w:fldCharType'), 'end')\n",
    "\n",
    "r_element = run._r\n",
    "r_element.append(fldChar)\n",
    "r_element.append(instrText)\n",
    "r_element.append(fldChar2)\n",
    "r_element.append(fldChar4)\n",
    "p_element = paragraph._p\n",
    "\n",
    "doccc1.add_page_break()\n",
    "\n",
    "##################### To add logo - 3rd Document ###############################################\n",
    "\n",
    "\n",
    "logo_path = 'C:\\\\Users\\\\ds_008\\\\Downloads\\\\DSLogo.png'    # Path of the image file\n",
    "section = doccc1.sections[0]   # Create a section\n",
    "sec_header = section.header   # Create header \n",
    "header_tp = sec_header.add_paragraph()  # Add a paragraph in the header, you can add any anything in the paragraph\n",
    "header_run = header_tp.add_run()   # Add a run in the paragraph. In the run you can set the values \n",
    "header_run.add_picture(logo_path, width=Inches(1.3))  # Add a picture and set width.\n",
    "#rml_header = \"\\t Applied Artificial Intelligence for Schools Content \\t Generation by Topic \\t\"\n",
    "headr = course_name+ \": \"+\"Content Generated by Topic as a Reference Material\"\n",
    "header_run.add_text(\"\\n                                                                                                        \")\n",
    "#    header_run.add_text(\"Applied Artificial Intelligence for Schools Content Generation by Topic\")\n",
    "header_run.add_text(headr)\n",
    "header_run.add_text(\"\\n_______________________________________________________________________________________\")\n",
    "header_run.font.size =  Pt(13)\n",
    "header_run.font.color.rgb = RGBColor(0, 0, 0)\n",
    "header_run.font.bold = True\n",
    "\n",
    "\n",
    "doccc1.add_paragraph('')\n",
    "\n",
    "##################### To add footer with page number - 3rd Document ###############################################\n",
    "\n",
    "\n",
    "section = doccc1.sections[0]\n",
    "footer = section.footer\n",
    "footer_para = footer.paragraphs[0]\n",
    "footer_para.text = \"_________________________________________________________________________________ \\t \\n\\n © DeepSphere.AI | Confidential and Proprietary |Not for Distribution \\t\"\n",
    "add_page_number(doccc1.sections[0].footer.paragraphs[0])\n",
    "\n",
    "##################### To add table with url and its ranking - 3rd Document ###############################################\n",
    "\n",
    "#Add heading for the table - 2nd Document\n",
    "\n",
    "\n",
    "topic1 = \"Extracted URls and its Ranking for the Given Topic: \" + Topic\n",
    "table_heading = doccc1.add_heading(topic1, 1)\n",
    "doccc1.add_paragraph('')\n",
    "table_heading.style.font.color.rgb = RGBColor(0, 0, 153)\n",
    "table_heading.style.font.size = Pt(16)\n",
    "table_heading.style.font.bold = True\n",
    "table_heading.style.font.all_caps = True\n",
    "\n",
    "table = doccc1.add_table(rows=1, cols=2)\n",
    "row = table.rows[0].cells\n",
    "row[0].text = 'URL Rank'\n",
    "row[1].text = 'URLs'\n",
    "for url_rank, urls in datat:\n",
    "    row = table.add_row().cells\n",
    "    row[0].text = str(url_rank) + \"                               \"\n",
    "    row[1].text = urls\n",
    "\n",
    "table.style = 'Colorful List'\n",
    "table.autofit = False\n",
    "table.allow_autofit = False\n",
    "cell = table.rows[0].cells[0]\n",
    "cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "cell.paragraphs[0].vertical_alignment = WD_ALIGN_VERTICAL.CENTER\n",
    "cell1 = table.rows[0].cells[1]\n",
    "cell1.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "cell1.paragraphs[0].vertical_alignment = WD_ALIGN_VERTICAL.CENTER\n",
    "table.columns[0].width = Inches(1.6)\n",
    "table.columns[1].width = Inches(5)\n",
    "bolding_columns = [1]\n",
    "for row in list(range(1, len(table.columns[1].cells))):\n",
    "    for column in bolding_columns:\n",
    "        table.rows[row].cells[column].paragraphs[0].runs[0].font.color.rgb = RGBColor(0, 0, 153)\n",
    "\n",
    "\n",
    "doccc1.add_page_break()\n",
    "\n",
    "def single_list(list,ignore_types=(str)): \n",
    "    for item in list:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, ignore_types):\n",
    "            yield from single_list(item,ignore_types=(str))\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "#Item_list = [10,20,[30,40],[50,'Null',70],100]\n",
    "items_single=single_list(list3)\n",
    "i=1\n",
    "df = pd.DataFrame()\n",
    "for item in items_single:\n",
    "    df = df.append({'col1' : item}, ignore_index=True)\n",
    "df = df.reset_index()\n",
    "df['index'] = df['index'] +1\n",
    "df_p = df[df['col1'].str.startswith('p -> ')]\n",
    "df_notp = df[~df['col1'].str.startswith('p -> ')]\n",
    "#    df_notp['col1'] = df_notp['col1'].str.replace(\"\"\"[EDIT]\"\"\", \"\", regex=False)\n",
    "\n",
    "cleaned = list(map(clean_string, df_p['col1']))\n",
    "#print(cleaned)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(cleaned)\n",
    "\n",
    "for x in range(0,X.shape[0]):\n",
    "    for y in range(x,X.shape[0]):\n",
    "        if(x!=y):\n",
    "            #if(cosine_similarity(X[x],X[y])>threshold):\n",
    "#                print(\"sent \" + str(x) + \": \\n\",sentences[x])\n",
    "#                print(\"sent \" + str(y) + \": \\n\",sentences[y])\n",
    "#                print(\"Cosine similarity between :\" + \"sent \" + str(x) + \" \" + \"sent \" + str(y), \" \\n\",cosine_similarity(X[x],X[y]))\n",
    "#                print()\n",
    "                if cosine_similarity(X[x],X[y]) >0.13:\n",
    "                    df_p['col1'][y] = \"\"\n",
    "dfnew1 = pd.concat([df_p, df_notp])\n",
    "df_new = dfnew1.sort_values(by = \"index\")\n",
    "\n",
    "df_p['col1'] = df_p['col1'].str.replace(\"p -> \", \"\")\n",
    "df_p1 = df_p[df_p['col1'].apply(lambda x: len(x.split(' ')) > 15)]\n",
    "df_p1 = df_p1[df_p1['col1'].apply(lambda x: len(x.split(' ')) > 50)]\n",
    "list1 = df_p1['col1'].tolist()\n",
    "text = \"\".join(list1)\n",
    "text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
    "    return sentences  \n",
    "\n",
    "if len(text)>=20:\n",
    "    text = text.replace('\\n',' ')\n",
    "\n",
    "    \n",
    "# sentences = summarize_text(text)\n",
    "# sentences = [i for n, i in enumerate(tokenize_sentences(sentences)) if i not in tokenize_sentences(sentences)[:n]]\n",
    "# sentences = fuzzy_dup_remove(sentences)\n",
    "# split_list = []\n",
    "# for i,sentence in enumerate(sentences):\n",
    "#     if i <5:\n",
    "#         pos = pos_tree_from_sentence(sentence)\n",
    "#         split_sentence = get_np_vp(pos,sentence)\n",
    "#         split_list.append(split_sentence)\n",
    "#print('split_sentence in app.py- ',split_list)\n",
    "\n",
    "\n",
    "alt_sent_list = []\n",
    "\n",
    "sentences = summarize_text(text)\n",
    "#print(\"#####################Summarize Text\", sentences, \"#####################\")\n",
    "sentences = [i for n, i in enumerate(tokenize_sentences(sentences)) if i not in tokenize_sentences(sentences)[:n]]\n",
    "#print(\"#####################tokenize_sentences\", sentences, \"#####################\")\n",
    "sentences = fuzzy_dup_remove(sentences)\n",
    "#print(\"#####################fuzzy_dup_remove\", sentences, \"#####################\")\n",
    "flat_list = alternate_sentences(sentences)\n",
    "\n",
    "true_sents = flat_list[1::2]\n",
    "false_sents = flat_list[0::2]\n",
    "# print(\"################ True Sentences ################### \\n\")\n",
    "# i=0\n",
    "# for x in true_sents:\n",
    "#     i+=1\n",
    "#     print(i, \" - \", x, \"\\n\")\n",
    "# print(\"\\n\\n\")\n",
    "# print(\"################ False Sentences ################### \\n\")\n",
    "# j=0\n",
    "# for y in false_sents:\n",
    "#     j+=1\n",
    "#     print(j, \" - \", y, \"\\n\")\n",
    "\n",
    "            # list_write = flat_list\n",
    "            # random.shuffle(list_write)\n",
    "            # for i,sentence in enumerate(sentences):\n",
    "            #     if i <5:\n",
    "            #         pos = pos_tree_from_sentence(sentence)\n",
    "            #         alt_sentence = alternate_sentences(pos,sentence)\n",
    "            #         alt_sent_list.append(alt_sentence)\n",
    "            #         flat_list = [item for sublist in alt_sent_list for item in sublist]\n",
    "#                 output_file(text,\"Input Text\")\n",
    "#                 output_file(flat_list,quest)\n",
    "#             li2 = []\n",
    "#             # for i in flat_list:\n",
    "#             #     for j in list_write:\n",
    "#             #         print('i value - ',i)\n",
    "#             #         print('j value - ',j)\n",
    "#             #         if i==j:\n",
    "#             #             li2.append('True')\n",
    "#             #         else:\n",
    "#             #             li2.append('False')\n",
    "#             #         break\n",
    "#             for i,itm in enumerate(flat_list):\n",
    "#                 if i%2==0:\n",
    "#                     li2.append('False')\n",
    "#                 else:\n",
    "#                     li2.append('True')\n",
    "#             doc = DocxTemplate(\"TrueFalse_Template.docx\")\n",
    "#             quest = list(zip(flat_list,li2))\n",
    "# #                 print('questttttttttttttttttt - ',quest)\n",
    "#             context = get_context_tf(quest,unit,grade,title)\n",
    "#             doc.render(context)\n",
    "# # #                 doc.save(\"complex_case_manageme   nt_true_or_false.docx\")\n",
    "#             doc.save(\"School-Questions/8th/\"+\"unit\"+context[\"unit\"]+\"_true_or_false.docx\")\n",
    "#     else:\n",
    "#         st.error(\"Please select input file!\")\n",
    "# vm_col1,vm_col2,vm_col3 = st.beta_columns((1,1,2))\n",
    "# vm_col1.success(\"Validate Model\")\n",
    "# vm_col2.success(\"Step 6\")\n",
    "# if vm_col3.button('View Model Outcome'):\n",
    "#     if text is not None:\n",
    "#         alt_sent_list = []\n",
    "#         sentences = summarize_text(text)\n",
    "#         sentences = [i for n, i in enumerate(tokenize_sentences(sentences)) if i not in tokenize_sentences(sentences)[:n]]\n",
    "#         sentences = fuzzy_dup_remove(sentences)\n",
    "#         flat_list = list(alternate_sentences(sentences))\n",
    "\n",
    "#    nname = Topic + \".csv\"\n",
    "#    df_new.to_csv(nname)\n",
    "\n",
    "#     i=1\n",
    "#     for x in range(len(df_new['col1'])):\n",
    "#     #    i=1\n",
    "#         try:\n",
    "#             if df_new['col1'][x].startswith('url -> '):\n",
    "#                 para1 = doccc1.add_paragraph().add_run(\"-----------------------------------------------------------------------------------------\")\n",
    "#                 para1.bold = True\n",
    "#                 para1.font.size = Pt(14)\n",
    "#                 para1.font.color.rgb = RGBColor(0, 0, 153)\n",
    "#             #    docu.add_paragraph(\"\")\n",
    "#                 txt1 = \"**Content Set #\" + str(i) + \"**\"\n",
    "#                 para2 = doccc1.add_paragraph().add_run(txt1)\n",
    "#                 para2.bold = True\n",
    "#                 para2.font.size = Pt(14)\n",
    "#                 para2.font.color.rgb = RGBColor(0, 0, 153)\n",
    "#             #    docu.add_paragraph(\"\")\n",
    "#             #    docu.add_paragraph(\"\")\n",
    "#                 txt2 = \"URL #\" + str(i) + \":    \" + df_new['col1'][x].replace(\"url -> \", \"\")\n",
    "#                 para3 = doccc1.add_paragraph().add_run(txt2)\n",
    "#                 para3.bold = True\n",
    "#                 para3.font.size = Pt(14)\n",
    "#                 para3.font.color.rgb = RGBColor(0, 0, 153)\n",
    "#             #    docu.add_paragraph(\"\")\n",
    "#                 para4 = doccc1.add_paragraph().add_run(\"-----------------------------------------------------------------------------------------\")\n",
    "#                 para4.bold=True\n",
    "#                 para4.font.size = Pt(14)\n",
    "#                 para4.font.color.rgb = RGBColor(0, 0, 153)\n",
    "#                 i+=1\n",
    "#             elif df_new['col1'][x] == \"\":\n",
    "#                 pass\n",
    "#             elif df_new['col1'][x].startswith(\"h\") and df_new['col1'][x+1].startswith('url -> '):\n",
    "#                 df_new['col1'][x] = \"\"\n",
    "#             elif df_new['col1'][x].startswith(\"h\") and df_new['col1'][x+1].startswith(\"h\"):\n",
    "#                 df_new['col1'][x] = \"\"\n",
    "#             elif df_new['col1'][x].startswith(\"h\") and df_new['col1'][x+1] == \"\":\n",
    "#                 df_new['col1'][x] = \"\"\n",
    "#             elif df_new['col1'][x].startswith(\"h\") and df_new['col1'][x+1].startswith(\"p\") and len(df_new['col1'][x+1].split()) < 15:\n",
    "#                 df_new['col1'][x] = \"\"\n",
    "#                 df_new['col1'][x+1] = \"\"\n",
    "#             elif df_new['col1'][x].startswith(\"p\") and len(df_new['col1'][x].split()) < 15:\n",
    "#                 df_new['col1'][x] = \"\"\n",
    "#             elif df_new['col1'][x].startswith(\"h\") and any(y in df_new['col1'][x].lower().split(\" \") for y in unwanted_words):\n",
    "#                 df_new['col1'][x] = \"\"\n",
    "#             elif df_new['col1'][x] == df_new['col1'][x+1]:\n",
    "#                 df_new['col1'][x] = \"\"\n",
    "#             elif df_new['col1'][x].startswith(\"h1\") and df_new['col1'][x+1].startswith(\"p\"):\n",
    "#                 h1 = doccc1.add_heading(df_new['col1'][x].replace(\"h1 -> \", \"\"), 1)\n",
    "#                 h1.style.font.color.rgb = RGBColor(0, 0, 139)\n",
    "#                 h1.style.font.size = Pt(14)\n",
    "#                 h1.style.font.bold = True\n",
    "#                 h1.style.font.all_caps = True\n",
    "#             elif df_new['col1'][x].startswith(\"h2\") and df_new['col1'][x+1].startswith(\"p\"):\n",
    "#                 h2 = doccc1.add_heading(df_new['col1'][x].replace(\"h2 -> \", \"\"), 2)\n",
    "#                 h2.style.font.color.rgb = RGBColor(0, 0, 139)\n",
    "#                 h2.style.font.size = Pt(14)\n",
    "#                 h2.style.font.bold = True\n",
    "#                 h2.style.font.all_caps = True\n",
    "#             elif df_new['col1'][x].startswith(\"h3\") and df_new['col1'][x+1].startswith(\"p\"):\n",
    "#                 h3 = doccc1.add_heading(df_new['col1'][x].replace(\"h3 -> \", \"\"), 3)\n",
    "#                 h3.style.font.color.rgb = RGBColor(0, 0, 139)\n",
    "#                 h3.style.font.size = Pt(14)\n",
    "#                 h3.style.font.bold = True\n",
    "#                 h3.style.font.all_caps = True\n",
    "#             elif df_new['col1'][x].startswith(\"h4\") and df_new['col1'][x+1].startswith(\"p\"):\n",
    "#                 h4 = doccc1.add_heading(df_new['col1'][x].replace(\"h4 -> \", \"\"), 4)\n",
    "#                 h4.style.font.color.rgb = RGBColor(0, 0, 139)\n",
    "#                 h4.style.font.size = Pt(14)\n",
    "#                 h4.style.font.bold = True\n",
    "#                 h4.style.font.all_caps = True\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     df_new['col1'][x].replace(\"p -> \", \"\")\n",
    "#                 except Exception as ee:\n",
    "#                     doccc1.add_paragraph(str(ee))\n",
    "#                 else:\n",
    "#                     if (df_new['col1'][x].startswith(\"p\")) and ((len(df_new['col1'][x].split(\". \"))) > 3):\n",
    "#                         doccc1.add_paragraph(summarizer(remove_control_characters(df_new['col1'][x].replace('\\x00','')).replace(\"p -> \", \"\"), (len(df_new['col1'][x].split(\". \")) +1)/2))\n",
    "#                     else:\n",
    "#                         doccc1.add_paragraph(remove_control_characters(df_new['col1'][x].replace('\\x00','')).replace(\"p -> \", \"\"))\n",
    "#         except:\n",
    "#             break\n",
    "#         else:\n",
    "#             pass\n",
    "#     doccc1.save(str(Topic)+\".docx\")\n",
    "#     convertFiletoPDF(str(Topic)+\".docx\")\n",
    "now = datetime.now()\n",
    "endtime = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "d1 = datetime.strptime(starttime, fmt)\n",
    "d2 = datetime.strptime(endtime, fmt)\n",
    "\n",
    "diff = d2 -d1\n",
    "diff_minutes = (diff.days * 24 * 60) + (diff.seconds/60)\n",
    "\n",
    "print(starttime)\n",
    "print(endtime)\n",
    "print(diff_minutes)\n",
    "print(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367a0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e992fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
