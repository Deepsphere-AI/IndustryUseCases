# -*- coding: utf-8 -*-
"""Automatic Answering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B5j8e-WyudC-VS3iN8zSBM2_3B5c9_wK
"""

import streamlit as st
import numpy as np
import pandas as pd
df = pd.read_excel(r'C:\Users\Sai\Documents\Deepsphere AI\train.xlsx')


import re
import gensim
from gensim.parsing.preprocessing import remove_stopwords

#from nltk.stem.lancaster import LancasterStemmer

def clean_sentence(sentence, stopwords=False):

  sentence = sentence.lower().strip()
  sentence = re.sub(r'[^a-z0-9\s]', '', sentence)

  if stopwords:
    sentence = remove_stopwords(sentence)
  return sentence

def get_cleaned_sentences(df,stopwords=False):
  sents=df[["Questions"]]
  cleaned_sentences=[]

  for index,row in df.iterrows():
    #print(index,row)
    cleaned=clean_sentence(row["Questions"],stopwords);
    cleaned_sentences.append(cleaned);
  return cleaned_sentences;

cleaned_sentences=get_cleaned_sentences(df,stopwords=True)
#print(cleaned_sentences);

#print("\n")

cleaned_sentences_with_stopwords=get_cleaned_sentences(df,stopwords=False)
#print(cleaned_sentences_with_stopwords);

import sklearn
from sklearn.metrics.pairwise import cosine_similarity;
def retriveAndPrintFAQAnswer(question_embedding,sentence_embeddings,FAQdf,sentences):
    max_sin=-1;
    index_sin=-1;
    for index,faq_embeddings in enumerate(sentence_embeddings):
      #print(cosine_similarity(faq_embeddings,question_embedding))
      sin=cosine_similarity(faq_embeddings,question_embedding)[0][0];
      #print("SIN: ", sin)
      #print(index, sin, sentences[index])
      if sin>max_sin:
          max_sin=sin;
          index_sin=index;
    return index_sin

   # print("\n")
   # print("Question: ",question)
   # print("\n");
   #print("Retrived: ",FAQdf.iloc[index_sin,1])
   # print(FAQdf.iloc[index_sin,2])


from gensim.models import Word2Vec
import gensim.downloader as api
from gensim import corpora
from gensim import models
import gensim.models.keyedvectors as word2vec
import pprint
import numpy
from PIL import Image
from gensim.models import Word2Vec
import gensim.downloader as api

def getWordVec(word,model):
        samp=model['computer'];
        vec=[0]*len(samp);
        try:
                vec=model[word];
        except:
                vec=[0]*len(samp);
        return (vec)

def getPhraseEmbedding(phrase,embeddingmodel):

        samp=getWordVec('computer', embeddingmodel);
        vec=numpy.array([0]*len(samp));
        den=0;
        for word in phrase.split():
          den=den+1;
          vec=vec+numpy.array(getWordVec(word,embeddingmodel));
        return vec.reshape(1, -1)
def local_css(file_name):
    with open(file_name) as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)
footer="""<style>
a:link , a:visited{
color: black;
background-color: transparent;
}

a:hover,  a:active {
color: red;
background-color: transparent;
text-decoration: underline;
}

.footer {
position: fixed;
left: 0;
bottom: 0;
width: 100%;
background-color: white;
color: black;
text-align: center;
}
</style>
<div class="footer">
<p>@DeepSphere.AI 2021 | Confidential & Proprietary <a style='display: block; text-align: center;</a></p>
</div>
"""

if __name__=='__main__':
    image = Image.open('DeepSphere_Logo_Final.png')
    st.image(image)
    local_css('Style.css')
    st.markdown('<h1 style="text-align: center; color: blue;">NLP Reinvents FAQs:Machine Driven Dynamic FAQs</h1>', unsafe_allow_html=True)
    with st.form(key='my_form'):
        text_input = st.text_input(label='How May I Help You?')
        submit_button = st.form_submit_button(label='Submit')
    st.markdown(footer,unsafe_allow_html=True)
    href = f'<a style="text-align: center; color: white;" href="http://localhost:8501/" class="button">Refresh</a>'
    st.sidebar.markdown(href, unsafe_allow_html=True)
    if len(text_input) > 0:
        sentences=cleaned_sentences_with_stopwords

        #split it by white space
        sentence_words = [[word for word in document.split() ]
                          for document in sentences]
        dictionary = corpora.Dictionary(sentence_words)
                 
        bow_corpus = [dictionary.doc2bow(text) for text in sentence_words]
              
        question_orig=text_input
        question=clean_sentence(question_orig,stopwords=False);
        question_embedding = dictionary.doc2bow(question.split())

        
        glove_model=None;
        glove_model = api.load(r'C:\Users\Sai\Downloads\glovemodel.mod')
        
        
        sent_embeddings=[];
        for sent in cleaned_sentences:
            sent_embeddings.append(getPhraseEmbedding(sent,glove_model));

        question_embedding=getPhraseEmbedding(question,glove_model);
        b = retriveAndPrintFAQAnswer(question_embedding,sent_embeddings,df, cleaned_sentences_with_stopwords);
        st.write("Retrived: ",df.iloc[b,1])
        st.write(df.iloc[b,2])
    
    